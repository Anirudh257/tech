<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-03-09T16:16:31+00:00</updated><id>/feed.xml</id><title type="html">Skit Tech</title><subtitle>Speech Technology from Skit</subtitle><entry><title type="html">Turn Taking Dynamics in Voice Bots</title><link href="/Turn_Taking_Dynamics_in_Voice_Bots/" rel="alternate" type="text/html" title="Turn Taking Dynamics in Voice Bots" /><published>2022-03-07T00:00:00+00:00</published><updated>2022-03-07T00:00:00+00:00</updated><id>/Turn_Taking_Dynamics_in_Voice_Bots</id><content type="html" xml:base="/Turn_Taking_Dynamics_in_Voice_Bots/">&lt;p&gt;One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette.  Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.&lt;/p&gt;

&lt;p&gt;Turn-taking is even more difficult in a multi-party setting. You might remember the last group call you had and just when you were about to take the turn, someone else jumped right in (because you waited for a tad bit too long) and you never got to speak. Turn-taking behaviour also differs culturally. In some cultures, interruptions and barge-ins are a lot more natural. There is also a difference in the inter-turn pause duration. These factors often lead to an unnatural conversation flow when speaking to a person from a different culture.&lt;/p&gt;

&lt;p&gt;Note : Bots with explicit turn-taking signalling like wake-words are out of scope for this blog.&lt;/p&gt;

&lt;h2 id=&quot;natural-turn-taking-dynamics&quot;&gt;Natural Turn Taking Dynamics&lt;/h2&gt;

&lt;p&gt;Irrespective of nuances, there are aspects of turn taking behaviour which are globally present in natural human-human conversation and one’s that we would want to imbibe in a human-bot interaction as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Barge-ins: These are situations when one agent interrupts the other. They occur very commonly. Examples of situations are : when one feels the other person is making a mistake or when ones feels the need to add some essential information, one naturally barges in.&lt;/li&gt;
  &lt;li&gt;Full Duplex Conversations : A half duplex conversation is one where turns are alternatively taken, like playing a tennis match, however in natural conversations, there are often instances when both people are saying something at the same time.
    &lt;ul&gt;
      &lt;li&gt;backchannels : words and fillers like “okay”, “alright” or “hmm” provide a lot of context about the state of the other person(for example attentiveness), especially when one is talking over the phone and visual cues are absent.&lt;/li&gt;
      &lt;li&gt;corrections : at times, when a person is saying something, one might want to make a small correction. For example, if there is an announcement being made “for the next meeting, you are supposed to finish submissions by 12th December, at so and so time….”. When the person is saying 12th December someone might correct by saying 13th December. This information is assimilated by the person and they often correct themselves. So, humans have the ability to hear and understand even while speaking and are active listeners.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/duplex-conversations.png&quot; /&gt;
    &lt;figcaption&gt;Fig 1: Full duplex vs half duplex conversations.&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Minimal inter-turn pauses : if you’ve ever spoken with a voice assistant, one of the first observations is that it takes too long to start speaking after you are done and the other way around. Human conversations have a much lower turn taking latency. If this latency is near optimal, it also lends to a feeling that the other person is understanding you and the left over impression is that of a conversation gone well. Human’s have an average pause duration of 200ms as shown below, while bots have a much higher latency.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/pause-duration.png&quot; /&gt;
    &lt;figcaption&gt;Fig 2: Turn Taking Pause duration as measured from the Switchboard corpus. Image is taken from [1].&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Turn taking cues : often in natural conversations, people produce small vocal cues like filler words “umm” or “uhhh” to convey that they want to say something and take the turn.&lt;/li&gt;
  &lt;li&gt;Turn yielding cues : there are markers is conversations when one knows that the person is done speaking. This is how we are able to separate pauses, which happen when a person is thinking in between his utterance vs one when he is done speaking.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;turn-taking-dynamics-in-voice-bots&quot;&gt;Turn-Taking Dynamics in Voice Bots&lt;/h2&gt;

&lt;p&gt;Below, we discuss different versions of turn-taking dynamics implemented in voice-bots each with more features and increasing levels of difficulty.&lt;/p&gt;

&lt;h3 id=&quot;version---10&quot;&gt;Version - 1.0&lt;/h3&gt;

&lt;p&gt;These are some characteristics of a bare bone turn-taking behaviour that one would need in a voice bot deployment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initial patience : the time that the bot waits for the person to starts speaking&lt;/li&gt;
  &lt;li&gt;Silence detection : if the bot detects silence for a certain duration after the person has started speaking, it assumes the person’s turn is over.&lt;/li&gt;
  &lt;li&gt;Max turn duration : it doesn’t make sense to just be listening (because of error compounding, loss of context, maybe one is hearing just noise), so usually voice bots have a maximum duration to which they listen to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---20&quot;&gt;Version - 2.0&lt;/h3&gt;

&lt;p&gt;This version add robustness for real life situations, make the bot more human-like and tries to reduce the latency between turns.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VAD instead of silence detection : Often existence of background noise, speech and other signals causes the bot to keep listening. Instead one could train a Voice-Activity Detection system rather than use silence detection, to have robustness to background events and to listen to the user only when they are speaking.&lt;/li&gt;
  &lt;li&gt;Variable thresholds for silence detection and max duration : In some states for example, when the bot is expecting a yes/no answer, it makes sense to use smaller thresholds. In general dynamic thresholding should be used.&lt;/li&gt;
  &lt;li&gt;For turn-switching, instead of a simple VAD, use an IPU based model discussed &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;here&lt;/a&gt;. This uses a smaller VAD threshold + cues to predict the turn is over. One could start with some verbal cues for example phrase completion.&lt;/li&gt;
  &lt;li&gt;Adding backchannels as bot responses : So far we’ve only discussed aspects of perception, but backchannels are a very useful response feature. It makes the user feel that the bot is more attentive and is actively listening.
    &lt;ul&gt;
      &lt;li&gt;One could also add filler words in the main channel, when the bot is taking too long to produce a response in cases of high latency. This would prevent the user from asking a question to verify if the bot is there or not. Without this, the user’s speech would lead to further increase in latency as it would be perceived as a case when the user wants to take the turn and say something useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---30&quot;&gt;Version - 3.0&lt;/h3&gt;

&lt;p&gt;There are no good baselines for these and working on improvements would constitute state of the art performance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-party situations : These are a lot more complex and require modelling multiple parties. An application could be when the bot is overseeing a human-machine interaction say, between a call centre agent and a human. Another common use is when during a typical 2 party interaction, someone interrupts the user. This requires the bot being aware that the user is speaking to someone else and then waiting.&lt;/li&gt;
  &lt;li&gt;Full - Duplex Conversations : Unlike human-human conversations a bots can attentively listen at the same time, while saying something. This offers a possibility of redesigning interactions which can leverage this feature.&lt;/li&gt;
  &lt;li&gt;Personalisation of Turn taking behaviour : This involves changing the parameters based on user characteristics. One could entrain one’s system to be more in line with the user’s behaviour. At times when the user is angry it might involve changing the durations to feel that they are being heard.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;Turn-taking in Conversational Systems and Human-Robot Interaction: A Review&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="Turn-taking" /><category term="barge-in" /><category term="duplex conversations" /><summary type="html">One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette. Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.</summary></entry><entry><title type="html">Speeding up Inference with the Lottery Ticket Hypothesis</title><link href="/sparse-models/" rel="alternate" type="text/html" title="Speeding up Inference with the Lottery Ticket Hypothesis" /><published>2022-02-23T00:00:00+00:00</published><updated>2022-02-23T00:00:00+00:00</updated><id>/sparse-models</id><content type="html" xml:base="/sparse-models/">&lt;script src=&quot;https://cdn.plot.ly/plotly-2.8.3.min.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The most fancy tool in the Modern Machine Learning toolbox are Neural Networks (NNs),
especially &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Deep&lt;/code&gt; ones. NNs usually have much more number of parameters than the
number of datapoints, and hence are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;overparameterized&lt;/code&gt; models. Are all of the parameters
needed and perform a useful function in the model?&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lottery Ticket Hypothesis&lt;/code&gt; (LTH) [1] states that for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reasonably-sized&lt;/code&gt; NN, there
exists at least one sub-network (i.e., an NN with some of the parameters/weights
removed) when trained from scratch that is at least as performant as the full network.&lt;/p&gt;

&lt;p&gt;The Lottery Ticket Hypothesis is named as such because of the reasoning that
there can exist millions of sub-networks in even a relatively small NN; finding
these well-performing sub-networks (or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;winning tickets&lt;/code&gt;) amongst all the possible
networks (lottery tickets) is akin to a winning a lottery.&lt;/p&gt;

&lt;h2 id=&quot;sparse-models-why&quot;&gt;Sparse Models? Why?&lt;/h2&gt;

&lt;p&gt;Having sparse models have direct consequences for deployment purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;If most of the weights are zeros, we can store the weights in CSR/CSC format, and
only store the non-zero indices; leading to a very small disk footprint.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Matrix Multiplications and other operations for zero-ed elements can be ignored,
leading to a sharp decline in the number of Floating Point Operations during inference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performance in terms of robustness to noise and accuracy can acutally &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increase&lt;/code&gt; when
performing sparsification, as shown by [4] for Automatic Speech Recognition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-search-of-winning-tickets&quot;&gt;The Search of Winning Tickets&lt;/h2&gt;

&lt;p&gt;Finding winning tickets reliably is called the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ticket Search&lt;/code&gt; problem. This is
usually done by using some sort of weight pruning (zero-ing weights) which are
less &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;important&lt;/code&gt;. Each pruning method has its own way of measuring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;importance&lt;/code&gt;.
This pruning is done in an iterative fashion, the procedure of Iterative Magnitude
Pruning (IMP) is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Initialize a network &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt; and store its initial weights.&lt;/li&gt;
  &lt;li&gt;Train the network &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt; on the dataset.&lt;/li&gt;
  &lt;li&gt;Prune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; number of weights from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Restore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt; to its initial weight values, except for the pruned weights.&lt;/li&gt;
  &lt;li&gt;Repeat steps 2-4 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; times.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With sufficiently small &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; and large &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;, we can reliably find winning tickets.
LTH and successive works have shown that IMP and variants (such as Layer-adaptive
Magnitude Pruning, LAMP [2]) can remove upto 90% of weights without degrading
performance for Vision and NLP tasks; however the same can be applied to almost any task.&lt;/p&gt;

&lt;h2 id=&quot;dismantling-the-lottery-in-the-lottery-ticket-hypothesis&quot;&gt;Dismantling The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lottery&lt;/code&gt; in the Lottery Ticket Hypothesis&lt;/h2&gt;

&lt;p&gt;In the heart of the hypothesis lies the assumption that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the network needs to be a large model
(i.e., a dense sampling of tickets)&lt;/code&gt; in order for it to contain at least one subnetwork which is
initialized in such a way that it trains to a high-performing model. A recent work [3] argues that this
assumption is incorrect and the size of the network is not the only reason for the emergence of LTH.&lt;/p&gt;

&lt;p&gt;The authors of [3] experiment with Ticket Search on a wide range of CNNs with various number of parameters, depth, and structures. They are listed below:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;style type=&quot;text/css&quot;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class=&quot;tg&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-amwm&quot;&gt;Name&lt;/th&gt;
    &lt;th class=&quot;tg-amwm&quot;&gt;# Params&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;LeNet-5&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;61K&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;PNASNet-A&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;0.13M&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;PNASNet-B&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;0.45M&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;ResNet32&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;0.46M&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;MobileNetV1&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;3.22M&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;EfficientNet&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;3.59M&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;ResNet18&lt;/td&gt;
    &lt;td class=&quot;tg-baqh&quot;&gt;11.17M&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;

&lt;p&gt;The authors perform ticket search 50 times for each architecture on the CIFAR-10 dataset and compute
trajectory lengths, percentage of surviving weights, test accuracy etc. after each stage of pruning.&lt;/p&gt;

&lt;h3 id=&quot;ticket-search-difficulty&quot;&gt;Ticket Search Difficulty&lt;/h3&gt;

&lt;p&gt;The success rate of finding winning tickets out of 50 trials is denoted as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ticket Search Difficulty&lt;/code&gt; of a particular architecture.
In the figures below, the architectures are ordered in increasing order of the number of parameters.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;html&gt;
&lt;head&gt;&lt;meta charset=&quot;utf-8&quot; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;                            &lt;div id=&quot;3b018782-3058-48a3-9714-33efbd587675&quot; class=&quot;plotly-graph-div&quot; style=&quot;height:500px; width:900px;&quot;&gt;&lt;/div&gt;            &lt;script type=&quot;text/javascript&quot;&gt;                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(&quot;3b018782-3058-48a3-9714-33efbd587675&quot;)) {                    Plotly.newPlot(                        &quot;3b018782-3058-48a3-9714-33efbd587675&quot;,                        [{&quot;marker&quot;:{&quot;color&quot;:&quot;rgba(124, 209, 184, 0.5)&quot;,&quot;line&quot;:{&quot;color&quot;:&quot;rgb(124, 209, 184)&quot;,&quot;width&quot;:1.5}},&quot;name&quot;:&quot;Success Rate&quot;,&quot;x&quot;:[&quot;LeNet-5&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-B&quot;,&quot;ResNet32&quot;,&quot;MobileNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;ResNet18&quot;],&quot;y&quot;:[67.9245283018868,86.53846153846155,90.0,83.01886792452831,94.0,88.0,66.66666666666666],&quot;type&quot;:&quot;bar&quot;},{&quot;marker&quot;:{&quot;color&quot;:&quot;rgba(250, 187, 81, 0.5)&quot;,&quot;line&quot;:{&quot;color&quot;:&quot;rgb(250, 187, 81)&quot;,&quot;width&quot;:1.5}},&quot;name&quot;:&quot;Test Accuracy&quot;,&quot;x&quot;:[&quot;LeNet-5&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-B&quot;,&quot;ResNet32&quot;,&quot;MobileNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;ResNet18&quot;],&quot;y&quot;:[67.92,80.1,85.29,89.9,85.29,87.54,93.47],&quot;type&quot;:&quot;bar&quot;}],                        {&quot;template&quot;:{&quot;data&quot;:{&quot;bar&quot;:[{&quot;error_x&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;error_y&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;marker&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#E5ECF6&quot;,&quot;width&quot;:0.5},&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;bar&quot;}],&quot;barpolar&quot;:[{&quot;marker&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#E5ECF6&quot;,&quot;width&quot;:0.5},&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;barpolar&quot;}],&quot;carpet&quot;:[{&quot;aaxis&quot;:{&quot;endlinecolor&quot;:&quot;#2a3f5f&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;minorgridcolor&quot;:&quot;white&quot;,&quot;startlinecolor&quot;:&quot;#2a3f5f&quot;},&quot;baxis&quot;:{&quot;endlinecolor&quot;:&quot;#2a3f5f&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;minorgridcolor&quot;:&quot;white&quot;,&quot;startlinecolor&quot;:&quot;#2a3f5f&quot;},&quot;type&quot;:&quot;carpet&quot;}],&quot;choropleth&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;choropleth&quot;}],&quot;contour&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;contour&quot;}],&quot;contourcarpet&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;contourcarpet&quot;}],&quot;heatmap&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;heatmap&quot;}],&quot;heatmapgl&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;heatmapgl&quot;}],&quot;histogram&quot;:[{&quot;marker&quot;:{&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;histogram&quot;}],&quot;histogram2d&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;histogram2d&quot;}],&quot;histogram2dcontour&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;histogram2dcontour&quot;}],&quot;mesh3d&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;mesh3d&quot;}],&quot;parcoords&quot;:[{&quot;line&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;parcoords&quot;}],&quot;pie&quot;:[{&quot;automargin&quot;:true,&quot;type&quot;:&quot;pie&quot;}],&quot;scatter&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatter&quot;}],&quot;scatter3d&quot;:[{&quot;line&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatter3d&quot;}],&quot;scattercarpet&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattercarpet&quot;}],&quot;scattergeo&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattergeo&quot;}],&quot;scattergl&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattergl&quot;}],&quot;scattermapbox&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattermapbox&quot;}],&quot;scatterpolar&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterpolar&quot;}],&quot;scatterpolargl&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterpolargl&quot;}],&quot;scatterternary&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterternary&quot;}],&quot;surface&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;surface&quot;}],&quot;table&quot;:[{&quot;cells&quot;:{&quot;fill&quot;:{&quot;color&quot;:&quot;#EBF0F8&quot;},&quot;line&quot;:{&quot;color&quot;:&quot;white&quot;}},&quot;header&quot;:{&quot;fill&quot;:{&quot;color&quot;:&quot;#C8D4E3&quot;},&quot;line&quot;:{&quot;color&quot;:&quot;white&quot;}},&quot;type&quot;:&quot;table&quot;}]},&quot;layout&quot;:{&quot;annotationdefaults&quot;:{&quot;arrowcolor&quot;:&quot;#2a3f5f&quot;,&quot;arrowhead&quot;:0,&quot;arrowwidth&quot;:1},&quot;autotypenumbers&quot;:&quot;strict&quot;,&quot;coloraxis&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;colorscale&quot;:{&quot;diverging&quot;:[[0,&quot;#8e0152&quot;],[0.1,&quot;#c51b7d&quot;],[0.2,&quot;#de77ae&quot;],[0.3,&quot;#f1b6da&quot;],[0.4,&quot;#fde0ef&quot;],[0.5,&quot;#f7f7f7&quot;],[0.6,&quot;#e6f5d0&quot;],[0.7,&quot;#b8e186&quot;],[0.8,&quot;#7fbc41&quot;],[0.9,&quot;#4d9221&quot;],[1,&quot;#276419&quot;]],&quot;sequential&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;sequentialminus&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]]},&quot;colorway&quot;:[&quot;#636efa&quot;,&quot;#EF553B&quot;,&quot;#00cc96&quot;,&quot;#ab63fa&quot;,&quot;#FFA15A&quot;,&quot;#19d3f3&quot;,&quot;#FF6692&quot;,&quot;#B6E880&quot;,&quot;#FF97FF&quot;,&quot;#FECB52&quot;],&quot;font&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;geo&quot;:{&quot;bgcolor&quot;:&quot;white&quot;,&quot;lakecolor&quot;:&quot;white&quot;,&quot;landcolor&quot;:&quot;#E5ECF6&quot;,&quot;showlakes&quot;:true,&quot;showland&quot;:true,&quot;subunitcolor&quot;:&quot;white&quot;},&quot;hoverlabel&quot;:{&quot;align&quot;:&quot;left&quot;},&quot;hovermode&quot;:&quot;closest&quot;,&quot;mapbox&quot;:{&quot;style&quot;:&quot;light&quot;},&quot;paper_bgcolor&quot;:&quot;white&quot;,&quot;plot_bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;polar&quot;:{&quot;angularaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;radialaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;}},&quot;scene&quot;:{&quot;xaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;},&quot;yaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;},&quot;zaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;}},&quot;shapedefaults&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;}},&quot;ternary&quot;:{&quot;aaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;baxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;caxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;}},&quot;title&quot;:{&quot;x&quot;:0.05},&quot;xaxis&quot;:{&quot;automargin&quot;:true,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;,&quot;title&quot;:{&quot;standoff&quot;:15},&quot;zerolinecolor&quot;:&quot;white&quot;,&quot;zerolinewidth&quot;:2},&quot;yaxis&quot;:{&quot;automargin&quot;:true,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;,&quot;title&quot;:{&quot;standoff&quot;:15},&quot;zerolinecolor&quot;:&quot;white&quot;,&quot;zerolinewidth&quot;:2}}},&quot;barmode&quot;:&quot;group&quot;,&quot;legend&quot;:{&quot;title&quot;:{&quot;text&quot;:&quot;&quot;},&quot;yanchor&quot;:&quot;bottom&quot;,&quot;y&quot;:0,&quot;xanchor&quot;:&quot;right&quot;,&quot;x&quot;:1,&quot;bordercolor&quot;:&quot;Black&quot;,&quot;borderwidth&quot;:1},&quot;width&quot;:900,&quot;height&quot;:500,&quot;title&quot;:{&quot;text&quot;:&quot;&quot;},&quot;paper_bgcolor&quot;:&quot;rgba(255,255,255,1)&quot;,&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255,1)&quot;,&quot;xaxis&quot;:{&quot;showline&quot;:true,&quot;linewidth&quot;:1,&quot;linecolor&quot;:&quot;black&quot;,&quot;mirror&quot;:true,&quot;ticks&quot;:&quot;outside&quot;,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;LightGray&quot;,&quot;title&quot;:{&quot;text&quot;:&quot;Model&quot;}},&quot;yaxis&quot;:{&quot;showline&quot;:true,&quot;linewidth&quot;:1,&quot;linecolor&quot;:&quot;black&quot;,&quot;mirror&quot;:true,&quot;ticks&quot;:&quot;outside&quot;,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;LightGray&quot;,&quot;range&quot;:[40,100],&quot;title&quot;:{&quot;text&quot;:&quot;Percentage(%)&quot;}},&quot;font&quot;:{&quot;family&quot;:&quot;Times New Roman&quot;,&quot;size&quot;:14,&quot;color&quot;:&quot;Black&quot;}},                        {&quot;responsive&quot;: true}                    )                };                            &lt;/script&gt;        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/div&gt;

&lt;p&gt;For the architectures they tested, they conclude that, in general, it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easier&lt;/code&gt; to find winning tickets for smaller
architectures; which is a contradictory to the assumption of LTH.&lt;/p&gt;

&lt;h3 id=&quot;quality-of-winning-tickets&quot;&gt;Quality of Winning Tickets&lt;/h3&gt;

&lt;p&gt;Since larger architectures have more number of sub-networks, one would except to see the highest accuracy
gain as compared to smaller architectures, as there is a higher chances of better-quality
sub-networks to exist in larger architectures.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;html&gt;
&lt;head&gt;&lt;meta charset=&quot;utf-8&quot; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;                            &lt;div id=&quot;f57f2643-da21-4637-a8f2-3bd514cc4898&quot; class=&quot;plotly-graph-div&quot; style=&quot;height:500px; width:900px;&quot;&gt;&lt;/div&gt;            &lt;script type=&quot;text/javascript&quot;&gt;                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(&quot;f57f2643-da21-4637-a8f2-3bd514cc4898&quot;)) {                    Plotly.newPlot(                        &quot;f57f2643-da21-4637-a8f2-3bd514cc4898&quot;,                        [{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;LeNet-5&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(62, 142, 126)&quot;},&quot;name&quot;:&quot;LeNet-5&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;LeNet-5&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;,&quot;LeNet-5&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[1.904906988143921,1.185653567314148,0.31115585565567017,0.918765127658844,0.4581131935119629,3.0877082347869873,0.01486614067107439,0.2592170238494873,0.7795246243476868,0.2681369185447693,1.5106699466705322,2.275078773498535,1.6907098293304443,1.6891382932662964,0.20161201059818268,2.2212250232696533,0.7639181613922119,2.006239414215088,1.0940240621566772,1.3209880590438843,1.2307639122009277,2.979546546936035,2.5618128776550293,0.3922659158706665,0.9279676675796509,1.0539932250976562,0.8206157684326172,0.6183719038963318,0.3687315583229065,0.499565064907074,0.08828363567590714,1.1712963581085205,0.6325509548187256,0.3815144896507263,0.6114400029182434,0.7411799430847168],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;PNASNet-A&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(124, 209, 184)&quot;},&quot;name&quot;:&quot;PNASNet-A&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;PNASNet-A&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-A&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.37312862277030945,0.7777158617973328,0.7569185495376587,0.7470100522041321,0.6361508369445801,0.5876388549804688,0.5087525248527527,0.7585185766220093,0.3203180134296417,0.7118767499923706,0.5092485547065735,0.9042367935180664,1.132690668106079,0.050088800489902496,0.7672712802886963,1.161194086074829,0.9396141171455383,0.13635876774787903,0.7845544815063477,0.974764347076416,0.7243684530258179,0.5844332575798035,1.2492191791534424,1.072721242904663,0.5359594821929932,0.7078983187675476,0.9835668802261353,0.22349180281162262,0.5997793078422546,1.4964420795440674,0.6469324231147766,0.47499656677246094,0.8693549633026123,0.1367146521806717,0.6373433470726013,0.199421688914299,0.5104627013206482,0.9246506094932556,1.5810776948928833,1.1372202634811401,1.7715840339660645,2.2419307231903076,1.1877931356430054,1.1895993947982788],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;PNASNet-B&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(250, 187, 81)&quot;},&quot;name&quot;:&quot;PNASNet-B&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;PNASNet-B&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;,&quot;PNASNet-B&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.7194252014160156,0.07123123109340668,0.4829164147377014,1.2866263389587402,0.3063895106315613,2.1590445041656494,0.2948460876941681,0.4946974813938141,1.8773694038391113,0.4590388238430023,0.19976283609867096,0.719925582408905,1.6085163354873657,0.6129235029220581,0.21139201521873474,0.22405047714710236,0.9193852543830872,1.4580318927764893,0.17705562710762024,1.3745747804641724,0.6387519836425781,1.9715543985366821,0.31795135140419006,1.1238667964935303,0.7917727828025818,0.8242045640945435,0.8541952967643738,1.0403178930282593,0.35473933815956116,0.3548561930656433,1.1016350984573364,1.2160643339157104,0.8608440160751343,1.309522032737732,0.6849336624145508,0.6353699564933777,0.39887842535972595,0.7312215566635132,0.569457471370697,0.2356509268283844,1.3872231245040894,0.540349006652832,0.8878892064094543,1.243784785270691,1.290016531944275],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;ResNet32&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(237, 216, 109)&quot;},&quot;name&quot;:&quot;ResNet32&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;ResNet32&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;,&quot;ResNet32&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.724388599395752,0.4115171730518341,0.27848947048187256,0.13357385993003845,0.3112480938434601,0.0,1.435297966003418,0.9365551471710205,0.4339594841003418,0.4097937047481537,0.12203307449817657,0.022212162613868713,0.891657829284668,0.011140718124806881,1.0246108770370483,0.28769999742507935,0.5563591718673706,0.8996974229812622,0.7459343075752258,0.5886912941932678,0.7462666034698486,0.5460279583930969,0.710557758808136,0.7679492831230164,0.6359474658966064,0.6019406318664551,0.2669312059879303,0.10007376223802567,0.033254384994506836,0.8024979829788208,1.17449152469635,0.49928390979766846,0.16664984822273254,0.8912605047225952,0.37785765528678894,0.2009156346321106,0.5001077055931091,0.9483500123023987,0.5229208469390869,0.05528253689408302,0.3878529369831085],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;MobileNetV1&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(195, 242, 119)&quot;},&quot;name&quot;:&quot;MobileNetV1&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;MobileNetV1&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;,&quot;MobileNetV1&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.058364953845739365,0.4949890077114105,0.11686160415410995,0.27002036571502686,0.035115040838718414,0.11652117967605591,0.4917435348033905,0.8603457808494568,0.5385144948959351,0.4809427261352539,1.1619783639907837,0.29359951615333557,0.24697057902812958,0.43213629722595215,0.24584299325942993,0.7538272738456726,0.8109090328216553,0.37545427680015564,0.5306032299995422,0.21018251776695251,0.25782400369644165,0.1288365125656128,0.4314308762550354,0.9018555879592896,0.4458058476448059,0.36333516240119934,0.197879359126091,0.4677286446094513,0.04692189395427704,0.17532892525196075,0.7077122926712036,1.1467090845108032,0.41002631187438965,0.8001886010169983,0.41142410039901733,0.29212433099746704,0.7028211951255798,0.9508129358291626,0.7495015859603882,0.42313188314437866,0.8464627861976624,0.292705774307251,0.7406594157218933,0.29076528549194336,0.869154155254364,0.32752227783203125,1.2722368240356445],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;EfficientNetV1&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(236, 237, 140)&quot;},&quot;name&quot;:&quot;EfficientNetV1&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;EfficientNetV1&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;EfficientNetV1&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.26240241527557373,0.37796568870544434,0.2054094672203064,0.22875094413757324,0.3645889461040497,0.11424480378627777,0.12562884390354156,0.21655167639255524,0.3872041702270508,0.2971453070640564,0.28581228852272034,0.29653528332710266,0.17137156426906586,0.5380050539970398,0.21674059331417084,0.3324936032295227,0.20505845546722412,0.1936064213514328,0.19403968751430511,0.4935724139213562,0.11436238884925842,0.6630020141601562,0.367140531539917,0.6200492978096008,0.148330956697464,0.4344315230846405,0.3205481171607971,1.0566190481185913,0.5916535258293152,0.7203261852264404,0.21672458946704865,0.353434681892395,0.49221646785736084,0.14878106117248535,0.26205235719680786,0.5007994771003723,0.41011691093444824,0.7219032049179077,0.2394789308309555,0.3780002295970917,0.25157374143600464,0.4228602945804596,0.022734710946679115,0.23896118998527527],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;},{&quot;alignmentgroup&quot;:&quot;True&quot;,&quot;hovertemplate&quot;:&quot;keys=%{x}&lt;br&gt;values=%{y}&lt;extra&gt;&lt;/extra&gt;&quot;,&quot;legendgroup&quot;:&quot;ResNet18&quot;,&quot;marker&quot;:{&quot;color&quot;:&quot;rgb(232, 150, 111)&quot;},&quot;name&quot;:&quot;ResNet18&quot;,&quot;notched&quot;:false,&quot;offsetgroup&quot;:&quot;ResNet18&quot;,&quot;orientation&quot;:&quot;v&quot;,&quot;showlegend&quot;:true,&quot;x&quot;:[&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;,&quot;ResNet18&quot;],&quot;x0&quot;:&quot; &quot;,&quot;xaxis&quot;:&quot;x&quot;,&quot;y&quot;:[0.28839632868766785,0.20309439301490784,0.9115265011787415,0.5134742259979248,1.4744153022766113,0.7400995492935181,0.26726534962654114,0.6636013388633728,0.8357428312301636,0.8035140633583069,0.19163239002227783,0.1386866718530655,0.10666503757238388,0.3000415563583374,0.6323647499084473,0.6963058710098267,0.08531709015369415,0.5877275466918945,0.7925434708595276,0.10652867704629898,0.23464293777942657,0.43757063150405884,0.08540816605091095,0.021318381652235985,0.9904168248176575,0.717035710811615,0.4825187027454376,0.5029440522193909,0.2562710642814636,0.7407351732254028,0.19239024817943573,0.16057583689689636,0.04260402172803879],&quot;y0&quot;:&quot; &quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;type&quot;:&quot;box&quot;}],                        {&quot;template&quot;:{&quot;data&quot;:{&quot;bar&quot;:[{&quot;error_x&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;error_y&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;marker&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#E5ECF6&quot;,&quot;width&quot;:0.5},&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;bar&quot;}],&quot;barpolar&quot;:[{&quot;marker&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#E5ECF6&quot;,&quot;width&quot;:0.5},&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;barpolar&quot;}],&quot;carpet&quot;:[{&quot;aaxis&quot;:{&quot;endlinecolor&quot;:&quot;#2a3f5f&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;minorgridcolor&quot;:&quot;white&quot;,&quot;startlinecolor&quot;:&quot;#2a3f5f&quot;},&quot;baxis&quot;:{&quot;endlinecolor&quot;:&quot;#2a3f5f&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;minorgridcolor&quot;:&quot;white&quot;,&quot;startlinecolor&quot;:&quot;#2a3f5f&quot;},&quot;type&quot;:&quot;carpet&quot;}],&quot;choropleth&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;choropleth&quot;}],&quot;contour&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;contour&quot;}],&quot;contourcarpet&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;contourcarpet&quot;}],&quot;heatmap&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;heatmap&quot;}],&quot;heatmapgl&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;heatmapgl&quot;}],&quot;histogram&quot;:[{&quot;marker&quot;:{&quot;pattern&quot;:{&quot;fillmode&quot;:&quot;overlay&quot;,&quot;size&quot;:10,&quot;solidity&quot;:0.2}},&quot;type&quot;:&quot;histogram&quot;}],&quot;histogram2d&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;histogram2d&quot;}],&quot;histogram2dcontour&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;histogram2dcontour&quot;}],&quot;mesh3d&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;type&quot;:&quot;mesh3d&quot;}],&quot;parcoords&quot;:[{&quot;line&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;parcoords&quot;}],&quot;pie&quot;:[{&quot;automargin&quot;:true,&quot;type&quot;:&quot;pie&quot;}],&quot;scatter&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatter&quot;}],&quot;scatter3d&quot;:[{&quot;line&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatter3d&quot;}],&quot;scattercarpet&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattercarpet&quot;}],&quot;scattergeo&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattergeo&quot;}],&quot;scattergl&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattergl&quot;}],&quot;scattermapbox&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scattermapbox&quot;}],&quot;scatterpolar&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterpolar&quot;}],&quot;scatterpolargl&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterpolargl&quot;}],&quot;scatterternary&quot;:[{&quot;marker&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;type&quot;:&quot;scatterternary&quot;}],&quot;surface&quot;:[{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;},&quot;colorscale&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;type&quot;:&quot;surface&quot;}],&quot;table&quot;:[{&quot;cells&quot;:{&quot;fill&quot;:{&quot;color&quot;:&quot;#EBF0F8&quot;},&quot;line&quot;:{&quot;color&quot;:&quot;white&quot;}},&quot;header&quot;:{&quot;fill&quot;:{&quot;color&quot;:&quot;#C8D4E3&quot;},&quot;line&quot;:{&quot;color&quot;:&quot;white&quot;}},&quot;type&quot;:&quot;table&quot;}]},&quot;layout&quot;:{&quot;annotationdefaults&quot;:{&quot;arrowcolor&quot;:&quot;#2a3f5f&quot;,&quot;arrowhead&quot;:0,&quot;arrowwidth&quot;:1},&quot;autotypenumbers&quot;:&quot;strict&quot;,&quot;coloraxis&quot;:{&quot;colorbar&quot;:{&quot;outlinewidth&quot;:0,&quot;ticks&quot;:&quot;&quot;}},&quot;colorscale&quot;:{&quot;diverging&quot;:[[0,&quot;#8e0152&quot;],[0.1,&quot;#c51b7d&quot;],[0.2,&quot;#de77ae&quot;],[0.3,&quot;#f1b6da&quot;],[0.4,&quot;#fde0ef&quot;],[0.5,&quot;#f7f7f7&quot;],[0.6,&quot;#e6f5d0&quot;],[0.7,&quot;#b8e186&quot;],[0.8,&quot;#7fbc41&quot;],[0.9,&quot;#4d9221&quot;],[1,&quot;#276419&quot;]],&quot;sequential&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]],&quot;sequentialminus&quot;:[[0.0,&quot;#0d0887&quot;],[0.1111111111111111,&quot;#46039f&quot;],[0.2222222222222222,&quot;#7201a8&quot;],[0.3333333333333333,&quot;#9c179e&quot;],[0.4444444444444444,&quot;#bd3786&quot;],[0.5555555555555556,&quot;#d8576b&quot;],[0.6666666666666666,&quot;#ed7953&quot;],[0.7777777777777778,&quot;#fb9f3a&quot;],[0.8888888888888888,&quot;#fdca26&quot;],[1.0,&quot;#f0f921&quot;]]},&quot;colorway&quot;:[&quot;#636efa&quot;,&quot;#EF553B&quot;,&quot;#00cc96&quot;,&quot;#ab63fa&quot;,&quot;#FFA15A&quot;,&quot;#19d3f3&quot;,&quot;#FF6692&quot;,&quot;#B6E880&quot;,&quot;#FF97FF&quot;,&quot;#FECB52&quot;],&quot;font&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;},&quot;geo&quot;:{&quot;bgcolor&quot;:&quot;white&quot;,&quot;lakecolor&quot;:&quot;white&quot;,&quot;landcolor&quot;:&quot;#E5ECF6&quot;,&quot;showlakes&quot;:true,&quot;showland&quot;:true,&quot;subunitcolor&quot;:&quot;white&quot;},&quot;hoverlabel&quot;:{&quot;align&quot;:&quot;left&quot;},&quot;hovermode&quot;:&quot;closest&quot;,&quot;mapbox&quot;:{&quot;style&quot;:&quot;light&quot;},&quot;paper_bgcolor&quot;:&quot;white&quot;,&quot;plot_bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;polar&quot;:{&quot;angularaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;radialaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;}},&quot;scene&quot;:{&quot;xaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;},&quot;yaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;},&quot;zaxis&quot;:{&quot;backgroundcolor&quot;:&quot;#E5ECF6&quot;,&quot;gridcolor&quot;:&quot;white&quot;,&quot;gridwidth&quot;:2,&quot;linecolor&quot;:&quot;white&quot;,&quot;showbackground&quot;:true,&quot;ticks&quot;:&quot;&quot;,&quot;zerolinecolor&quot;:&quot;white&quot;}},&quot;shapedefaults&quot;:{&quot;line&quot;:{&quot;color&quot;:&quot;#2a3f5f&quot;}},&quot;ternary&quot;:{&quot;aaxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;baxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;},&quot;bgcolor&quot;:&quot;#E5ECF6&quot;,&quot;caxis&quot;:{&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;}},&quot;title&quot;:{&quot;x&quot;:0.05},&quot;xaxis&quot;:{&quot;automargin&quot;:true,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;,&quot;title&quot;:{&quot;standoff&quot;:15},&quot;zerolinecolor&quot;:&quot;white&quot;,&quot;zerolinewidth&quot;:2},&quot;yaxis&quot;:{&quot;automargin&quot;:true,&quot;gridcolor&quot;:&quot;white&quot;,&quot;linecolor&quot;:&quot;white&quot;,&quot;ticks&quot;:&quot;&quot;,&quot;title&quot;:{&quot;standoff&quot;:15},&quot;zerolinecolor&quot;:&quot;white&quot;,&quot;zerolinewidth&quot;:2}}},&quot;xaxis&quot;:{&quot;anchor&quot;:&quot;y&quot;,&quot;domain&quot;:[0.0,1.0],&quot;title&quot;:{&quot;text&quot;:&quot;Architecture&quot;},&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;LeNet-5&quot;,&quot;PNASNet-A&quot;,&quot;PNASNet-B&quot;,&quot;ResNet32&quot;,&quot;MobileNetV1&quot;,&quot;EfficientNetV1&quot;,&quot;ResNet18&quot;],&quot;showline&quot;:true,&quot;linewidth&quot;:1,&quot;linecolor&quot;:&quot;black&quot;,&quot;mirror&quot;:true,&quot;ticks&quot;:&quot;outside&quot;,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;LightGray&quot;},&quot;yaxis&quot;:{&quot;anchor&quot;:&quot;x&quot;,&quot;domain&quot;:[0.0,1.0],&quot;title&quot;:{&quot;text&quot;:&quot;Best Sparse - Accuracy Gain (%)&quot;},&quot;showline&quot;:true,&quot;linewidth&quot;:1,&quot;linecolor&quot;:&quot;black&quot;,&quot;mirror&quot;:true,&quot;ticks&quot;:&quot;outside&quot;,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;LightGray&quot;},&quot;legend&quot;:{&quot;title&quot;:{&quot;text&quot;:&quot;keys&quot;},&quot;tracegroupgap&quot;:0},&quot;margin&quot;:{&quot;t&quot;:60},&quot;boxmode&quot;:&quot;overlay&quot;,&quot;showlegend&quot;:false,&quot;width&quot;:900,&quot;height&quot;:500,&quot;title&quot;:{&quot;text&quot;:&quot;&quot;},&quot;paper_bgcolor&quot;:&quot;rgba(255,255,255,1)&quot;,&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255,1)&quot;,&quot;font&quot;:{&quot;family&quot;:&quot;Times New Roman&quot;,&quot;size&quot;:14,&quot;color&quot;:&quot;Black&quot;}},                        {&quot;responsive&quot;: true}                    )                };                            &lt;/script&gt;        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/div&gt;

&lt;p&gt;Again, it is observed that ResNet18, which is the larget architecture has the lowest accuracy gain when
compared to its dense counterpart.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As a last-mile effort in your Machine Learning project, you should consider Iterative Pruning methods to improve generalizability, latency and memory requirements, even if your architecture is already very small/lightweight!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frankle, J. &amp;amp; Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR. 2019&lt;/p&gt;

&lt;p&gt;[2] Lee, J., Park, S., Mo, S., Ahn, S., &amp;amp; Shin, J. (2021). Layer-adaptive Sparsity for the Magnitude-based Pruning. ICLR. 2021&lt;/p&gt;

&lt;p&gt;[3] Anonymous. (2022). Not All Lotteries Are Made Equal. Submitted to Blog Track at ICLR 2022. https://openreview.net/forum?id=ugXMIHeasoz&lt;/p&gt;

&lt;p&gt;[4] Ding, S., Chen, T., &amp;amp; Wang, Z. (2022). Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable. International Conference on Learning Representations. https://openreview.net/forum?id=9Nk6AJkVYB&lt;/p&gt;</content><author><name>Surya Kant Sahu</name></author><category term="Machine Learning" /><summary type="html"></summary></entry><entry><title type="html">Feature Disentanglement - I</title><link href="/feature-disentanglement1/" rel="alternate" type="text/html" title="Feature Disentanglement - I" /><published>2022-02-22T00:00:00+00:00</published><updated>2022-02-22T00:00:00+00:00</updated><id>/feature-disentanglement1</id><content type="html" xml:base="/feature-disentanglement1/">&lt;p&gt;The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.&lt;/p&gt;

&lt;h1 id=&quot;data-representation-and-latent-code&quot;&gt;Data Representation and Latent Code&lt;/h1&gt;
&lt;p&gt;An image \((x)\) from the MNIST dataset has 28x28 = 784 dimensions which is a sparse representation of the image that can be visualized. But all these dimensions are not required to represent the image. The content of the images can be represented in a condensed form using lesser dimensions called latent code. Although the actual image has 784 dimensions \(x \in R^{784}\), one way of representing MNIST image can be with just an integer ie: \(z \in \{0, 1, 2, …, 9\}\). This representation \(z\) reduces the dimension of representing the image \(x\) to 1 which captures the content of which number is present in the image and the variability in the dataset. This is one example of discrete latent code for the MNIST dataset, a continuous latent code will contain more information about the image such as the style of the image, position of the number, size of the number in the image, etc.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;600&quot; height=&quot;300&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://www.mdpi.com/applsci/applsci-09-03169/article_deploy/html/images/applsci-09-03169-g001.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2:  Sample Images of MNIST from [1]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;autoencoder&quot;&gt;AutoEncoder&lt;/h2&gt;

&lt;p&gt;Autoencoder[2] models are popularly used to learn such latent code in an unsupervised manner by compressing the image to a fixed dimension code \(z\) and generating the image back using this latent code with an encoder-decoder model.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/08b0b21725c236fb1860285677a00248f77c7587/2-Figure1-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Autoencoder architecture from Autoencoders[2]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The encoder \(q_{\phi}(z \mid x)\) of the autoencoder compresses the image to a fixed dimension\((d)\) latent code\((z)\), and the decoder \(p_{\theta}(x \mid z)\) is a conditional image generator. The dimension of z has to be such that, the image can be completely reconstructed by the decoder with the latent code. Choosing the dimension of the latent code is a problem on its own[3].&lt;/p&gt;

&lt;p&gt;The autoencoder models trained will successfully encode the images into a latent code \(z\), but there is no guarantee that the latent code can be easily inferred, ie: we do not know where in the d-dimensional space the model encoded the image into, and thus difficult to choose a latent code to generate image during inference. So the conclusion is we have no idea how and where the encoder encodes the images, so we do not have control over synthesis during inference. The following figure shows the latent code learned by the AutoEncoder model with different training, as we can observe the latent space keep changing the range and quadrant and thus difficult to infer.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Latent code of MNIST images learned by an Auto Encoder [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational AutoEncoder(VAE)&lt;/h2&gt;

&lt;p&gt;Variational autoencoders(VAE) [6] solve this problem by forcing the latent code (z) to be close to a known prior distribution(Gaussian), this gives us control over the latent space. During inference, the latent space can be sampled from this known distribution for image generation. The following figure shows the latent code learned by VAE with different training, and the latent space across training is centered to the mean 0 across dimensions.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig2.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Latent code of MNIST images learned by a VAE [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;VAE allows us to have control over the latent space and sample from the known prior distribution. But this again does not give us control over the generation of the image. Say if you want to generate an image of the number ‘3’ or ‘7’, you cannot do that(at least not directly). This is where the term “disentanglement” comes into play.&lt;/p&gt;

&lt;h1 id=&quot;disentanglement&quot;&gt;Disentanglement&lt;/h1&gt;
&lt;p&gt;Feature disentanglement is isolating the source of variation in observation data. There is a lot more factors/feature of an MNIST image other than the number itself, such as the location of the number in the image, size of the image, angle of the number, etc. These factors are independent of each other.&lt;/p&gt;

&lt;p&gt;Feature disentanglement involves separating underlying concepts of “Big one in the left”: ie: size(big), number(one), location(left).
Our interest here is to see if we can isolate these factors in the latent code so that we can have control over the generation of the images. So we want the encoder to disentangle the representation into different factors and then we generate the image with desired factors say “small seven in the top rotated 30 degrees”.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; height=&quot;400&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/35da0a2001eea88486a5de677ab97868c93d0824/6-Figure2-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5: Generated MNIST images by InfoGAN [5] varied digit, thickness and roatation.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-vae&quot;&gt;Beta-VAE&lt;/h2&gt;
&lt;p&gt;Beta-VAE is a variant of VAE which allows disentanglement of the learned latent code. Beta-VAE adds hyperparameter to the loss function which modulates the learning constraint of VAE.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://miro.medium.com/max/1400/1*Z6tj5bVoArekVgv65gfkfg.png&quot; /&gt;
  &lt;figcaption&gt;Fig 6: Loss function of beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The first part of the loss function takes care of the reconstruction of the image, it is the second term that learns the latent code of VAE. Different dimensions that span across Gaussians are independent, so by making the prior distribution gaussian, we force the dimensions of the latent code to be independent of each other. So increasing the weight of the second part of the loss, makes the latent code to be disentangled and independent. But this also brings a tradeoff between disentanglement and the reconstruction capability of the VAE. Although Beta-VAE models are good in disentangling the features, the reconstruction ability of this model is not the best.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.00.13_PM.png&quot; /&gt;
  &lt;figcaption&gt;Fig 7: Samples generated by beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-tcvae&quot;&gt;Beta-TCVAE&lt;/h2&gt;
&lt;p&gt;beta-TCVAE decomposes the KL divergence[10] term of the loss function of VAE into reconstruction loss, Index-code mutual information[8] between data and latent variable, Total Correlation[9] of z, and Dimension wise KL divergence[10] of \(z\)(respectively in the following formula). This helps to break the overall KL Divergence of \(z\) into dimension-wise quantities, which will focus on each dimension of the latent code \(z\). In this formulation, the beta hyperparameter is only on the Total Correlation term which is more important for disentanglement without affecting the reconstruction. So, Beta-TCVAE has better reconstruction ability than Beta-VAE with similar disentanglement property.&lt;/p&gt;

\[\mathcal{L}_{\beta-\mathrm{TC}}:=\mathbb{E}_{q(z \mid n) p(n)}[\log p(n \mid z)]-\alpha I_{q}(z ; n)-\beta \operatorname{KL}\left(q(z) \| \prod_{j} q\left(z_{j}\right)\right)-\gamma \sum_{j} \operatorname{KL}\left(q\left(z_{j}\right) \| p\left(z_{j}\right)\right)\]

&lt;p&gt;where \(\alpha = \gamma = 1\) and only \(\beta\) is varies as the hyperparameter.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://vitalab.github.io/article/images/IsolatingSourcesOfDisentanglementInVAEs/figure1.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 8: Samples generated by beta-TCVAE [8].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;In future posts, we will examine many new methods for feature disentanglement and how these methods can be applied to speech signals.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://www.mdpi.com/2076-3417/9/15/3169/htm&quot;&gt;A Survey of Handwritten Character Recognition with MNIST and EMNIST&lt;/a&gt; (2019)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://arxiv.org/abs/2003.05991&quot;&gt;Autoencoders&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0925231215015994&quot;&gt;Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://www.youtube.com/watch?v=itOlzH9FHkI&quot;&gt;Disentangled Representations - How to do Interpretable Compression with Neural Models&lt;/a&gt; (2020)&lt;/p&gt;

&lt;p&gt;[5] : &lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot;&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-Encoding Variational Bayes&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://openreview.net/forum?id=Sy2fzU9gl&quot;&gt;beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework &lt;/a&gt; (2017)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://arxiv.org/abs/1802.04942&quot;&gt;Isolating Sources of Disentanglement in Variational Autoencoders&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_correlation&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;</content><author><name>Shangeth Rajaa</name></author><category term="Machine Learning" /><summary type="html">The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.</summary></entry><entry><title type="html">Google Summer of Code, 2022</title><link href="/gsoc-2022/" rel="alternate" type="text/html" title="Google Summer of Code, 2022" /><published>2022-02-18T00:00:00+00:00</published><updated>2022-02-18T00:00:00+00:00</updated><id>/gsoc-2022</id><content type="html" xml:base="/gsoc-2022/">&lt;h1 id=&quot;google-summer-of-code---2022&quot;&gt;Google Summer of Code - 2022&lt;/h1&gt;

&lt;p&gt;This page contains ideas which we’d like to get help from GSoC Contributors. But before all that, if you haven’t heard about Skit.&lt;/p&gt;

&lt;h2 id=&quot;what-is-skit&quot;&gt;What is Skit?&lt;/h2&gt;

&lt;p&gt;We are a series B funded, AI-first SaaS voice automation company specializing in delivering multilingual voice bots for contact center automation. We have our Speech Bots deployed in major banks and large enterprises in several verticals. We have a foothold in India and are expanding in the US and South-East Asia.&lt;/p&gt;

&lt;p&gt;We have been listed in &lt;a href=&quot;https://www.forbes.com/sites/johnkang/2021/04/19/the-forbes-30-under-30-asia-startups-unshackling-businesses-using-ai/?sh=9268fa85f9aa&quot;&gt;Forbes 30 Under 30 Asia 2021&lt;/a&gt; and have been named by Gartner as a &lt;a href=&quot;https://www.businesswire.com/news/home/20211221005315/en/Skit-Named-as-a-Cool-Vendor-in-Gartner-Cool-Vendors-in-Conversational-and-NLT-Widen-Use-Cases-Domain-Knowledge-and-Dialect-Support?utm_medium=email&amp;amp;_hsmi=202791220&amp;amp;_hsenc=p2ANqtz--yQEAO9Q810nr71J9I8MPppXkOWpWg51LqIrOdv_Wc2X_Hj-ydmia5ruRLbQEEat7EPQ6fn_GHMMVWu4tUV8beoU2BQA&amp;amp;utm_content=202791220&amp;amp;utm_source=hs_email&quot;&gt;Cool vendor in Conversational and NLT Widen Use Cases&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our goal is to build the most natural and robust multi lingual voice bot with state of the art human-machine interaction capabilities.&lt;/p&gt;

&lt;p&gt;We build voicebots, so that agents don’t have to sit and answer user queries 24/7 for 365 days.&lt;/p&gt;

&lt;p&gt;You can get to know more about us over &lt;a href=&quot;https://skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our tech blog is present &lt;a href=&quot;https://tech.skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;p&gt;You can reach out to us at our skit-gsoc community discord, link to join here : &lt;a href=&quot;https://discord.gg/Y9sJwz5Sw8&quot;&gt;https://discord.gg/Y9sJwz5Sw8&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gsoc-2022-ideas&quot;&gt;GSoC 2022 Ideas&lt;/h2&gt;

&lt;h1 id=&quot;idea-1-enhancements-to-dialogy-via-core-code-or-plugins&quot;&gt;Idea 1: Enhancements to dialogy via core code or plugins.&lt;/h1&gt;

&lt;h3 id=&quot;project-description&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://skit-ai.github.io/dialogy/&quot;&gt;Dialogy&lt;/a&gt; is a framework to build machine-learning solutions for speech applications speech dialogue systems.&lt;/p&gt;

&lt;p&gt;The main principles which form the backbone of dialogy are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plugin-based: Makes it easy to import/export components to projects.&lt;/li&gt;
  &lt;li&gt;Stack-agnostic: No assumptions made on ML stack; your choice of machine learning library will not be affected by using Dialogy.&lt;/li&gt;
  &lt;li&gt;Progressive: Minimal boilerplate writing to let you focus on your machine learning problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At current shape, dialogy allows one to train models, test performance on metrics and deploy applications. We want to add more features which are desirable in an complete SLU framework. These features could include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyperparameter Tuning- There are multiple hyperparameters involved in using a transformers. A hyperparameter tuning integration would allow developer with a seamless way to experiment with hyperparams and train optimal models.&lt;/li&gt;
  &lt;li&gt;Model interpretability via Captum, LIT integration- Interpretability is necessary from both business and development perspectives. Such tools would allow developers to explain why a certain prediction was made, as well as discover biases and faults in the model.&lt;/li&gt;
  &lt;li&gt;Integration with experiment tracking platforms- It gets difficult to keep track of models being trained by developers- some are meant for production releases, some are meant for experimentation. An experiment tracking integration would enable developers to manage their models and results easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;github-links&quot;&gt;GitHub Link(s):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy&quot;&gt;dialogy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy-template-simple-transformers&quot;&gt;dialogy-template-simple-transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes&quot;&gt;Expected Outcomes&lt;/h3&gt;

&lt;p&gt;Dialogy as a platform would have following features after the project&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Integrated with experiment tracking for better project management&lt;/li&gt;
  &lt;li&gt;Integrated with model explainability and interpretability frameworks allowing users to use these tools&lt;/li&gt;
  &lt;li&gt;Integrated with an hyperparameter tuning service/library&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Understanding of Machine Learning frameworks like PyTorch, Huggingface, etc.
And of course Python.&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Himansu - &lt;a href=&quot;https://www.linkedin.com/in/himansu-didwania/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/hdidwania&quot;&gt;github&lt;/a&gt;, email - himansu@skit.ai&lt;/li&gt;
  &lt;li&gt;Jaivarsan - &lt;a href=&quot;https://www.linkedin.com/in/jaivarsan-b-50264b148/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/greed2411&quot;&gt;github&lt;/a&gt;, email - jaivarsan@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-2-speaker-anonymization&quot;&gt;Idea 2: Speaker Anonymization&lt;/h1&gt;

&lt;h3 id=&quot;project-description-1&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;The goal of this project is to explore and implement methods to anonymize speech to remove the speaker information from the signal by distorting the speaker prosodic features or with techniques like voice conversion. The speaker information in the signal can be used to attack the speaker verification systems which leads to privacy and security concerns. The idea is to explore simple signal/speech processing techniques which are faster in both implementation and deployment, as well as neural methods which use deep learning models and architectures to resynthesis the speech signal with the original speaker’s information removed. Removing the speaker’s information can help us to use speech datasets without worrying about privacy attacks on the speakers in the dataset. This project involves the implementation of various research papers in the field and improving upon them and creating a python package for speaker anonymization.&lt;/p&gt;

&lt;h3 id=&quot;github&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/aunom&quot;&gt;aunom&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-1&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Literature Review of research papers in the field&lt;/li&gt;
  &lt;li&gt;Implementation of various research papers which are interesting to the project in python.&lt;/li&gt;
  &lt;li&gt;Github repository for experiments and final python package for speaker anonymization with proper documentation.&lt;/li&gt;
  &lt;li&gt;Demonstration of the methods implemented and final package.&lt;/li&gt;
  &lt;li&gt;Talk/presentation on the work done during the GSoC period.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required-1&quot;&gt;Skills Required&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Experience in Signal/Speech processing (Preferred)&lt;/li&gt;
  &lt;li&gt;Experience in Deep learning for speech tasks like Automatic Speech Recognition, Speech synthesis, speaker-related tasks  etc.&lt;/li&gt;
  &lt;li&gt;Python and deep learning frameworks like PyTorch/TensorFlow.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;possible-mentors-1&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Shangeth Rajaa - &lt;a href=&quot;https://www.linkedin.com/in/shangeth/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/shangeth&quot;&gt;github&lt;/a&gt;, email - shangeth.rajaa@skit.ai&lt;/li&gt;
  &lt;li&gt;Swaraj - &lt;a href=&quot;https://www.linkedin.com/in/swaraj-dalmia/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/swarajdalmia&quot;&gt;github&lt;/a&gt;, email - swaraj@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size-1&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 Hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-1&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-3-improving-kaldi-serve-performance&quot;&gt;Idea 3: Improving kaldi-serve performance&lt;/h1&gt;

&lt;h3 id=&quot;project-description-2&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;​​Kaldi Serve is a plug-and-play abstraction over the &lt;a href=&quot;https://kaldi-asr.org/&quot;&gt;Kaldi ASR&lt;/a&gt; toolkit, designed for ease of deployment and optimal runtime performance. It currently has the following key features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real-time streaming (uni &amp;amp; bi-directional) audio recognition.&lt;/li&gt;
  &lt;li&gt;Thread-safe concurrent Decoder queue for server environments.&lt;/li&gt;
  &lt;li&gt;RNNLM lattice rescoring.&lt;/li&gt;
  &lt;li&gt;N-best alternatives with AM/LM costs, word-level timings, and confidence scores.&lt;/li&gt;
  &lt;li&gt;Easy extensibility for custom applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are mainly looking at improving the runtime performance of the ASR pipeline by offloading Decoder computation to the GPU to be performed in mini-batches, and implementing a request queueing mechanism within the gRPC server to be able to utilize the parallel computing capability in order to boost latencies under high concurrent loads.&lt;/p&gt;

&lt;h3 id=&quot;github-1&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/kaldi-serve&quot;&gt;kaldi-serve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-2&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;p&gt;Integration with Kaldi Batched Threaded NNet3 CUDA pipeline for enabling batched computation in kaldi-serve gRPC application.&lt;/p&gt;

&lt;h3 id=&quot;skills-required-2&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Having some combination of these:
Languages: C++, CUDA, Python
Frameworks: Kaldi ASR, Pytorch, gRPC, Pybind11
Basics: Speech Recognition, Language Modeling, Deep Learning&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors-2&quot;&gt;Possible Mentors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Prabhsimran - &lt;a href=&quot;https://www.linkedin.com/in/pskrunner14/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/pskrunner14&quot;&gt;github&lt;/a&gt;, email - prabhsimran@skit.ai&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-size-2&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-2&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;</content><author><name>Jaivarsan B</name></author><category term="Machine Learning" /><category term="work" /><summary type="html">Google Summer of Code - 2022</summary></entry><entry><title type="html">Speech-First Conversational AI</title><link href="/speech-first-conversational-ai/" rel="alternate" type="text/html" title="Speech-First Conversational AI" /><published>2022-02-06T00:00:00+00:00</published><updated>2022-02-06T00:00:00+00:00</updated><id>/speech-first-conversational-ai</id><content type="html" xml:base="/speech-first-conversational-ai/">&lt;p&gt;We often get asked about the differences between voice and chat bots. The most
common perception is that the voice bot problem can be reduced to chat bot after
plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS)
system. We believe that’s an overly naive assumption about spoken conversations,
even in restricted goal-oriented dialog systems. This post is an attempt to
describe the differences involved and define what &lt;em&gt;Speech-First&lt;/em&gt; Conversational
AI means.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Speech is the most sophisticated behavior of the most complex organism in the
known universe. - &lt;a href=&quot;https://youtu.be/Zy3Ny-WjyGE?t=251&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Conversational AI systems solve problems of conversations, either using text or
voice. Since &lt;em&gt;conversations&lt;/em&gt; are specific to humans, there are many
anthropomorphic expectations from these systems. These expectations, while still
strong, are less restraining in text conversations as compared to speech. Speech
is deeply ingrained in human communication and minor misses could lead to
violation of user expectations. Contrast this with text messaging which is a,
relatively new, human-constructed channel&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; where expectations are different
and more lenient.&lt;/p&gt;

&lt;p&gt;There are multiple academic sources on differences between speech and text, here
we will describing a few key differences that we have noticed while building
speech-first conversation systems in more practical settings.&lt;/p&gt;

&lt;h2 id=&quot;signal&quot;&gt;Signal&lt;/h2&gt;

&lt;p&gt;In addition to the textual content, speech signals contain information about the
user’s state, trait, and the environment. Speech isn’t merely a redundant
modality, but adds valuable extra information. Different style of uttering the
same utterance can drastically change the meaning, something that’s used a lot
in human-human conversations.&lt;/p&gt;

&lt;p&gt;Environmental factors like recording quality, background ambience, and audio
events impact signals’ reception and semantics. Even beyond the immediate
environment, a lot of socio-cultural factors are embedded in speech beyond the
level they are in text chats. Because the signals are rich, the difficulty of a
few common problems across text and speech, like low-resource languages, is
higher.&lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;Once you go on transcribing audios utterances using ASRs, transcription errors
will add on to your burden. While ASR systems are improving day-on-day, there
still is error potential in handling acoustically similar utterances. Overall,
an entirely new set of problems like far-field ASR, signal enhancement, etc.
exist in spoken conversations.&lt;/p&gt;

&lt;p&gt;Additionally many &lt;em&gt;noisy&lt;/em&gt; deviations from fluent speech are not mere errors but
develop their own pragmatic sense and convey strong meaning. Speech
&lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_disfluency&quot;&gt;disfluencies&lt;/a&gt; are commonly
assumed behaviors of natural conversations and lack of them could even cause
discomfort.&lt;/p&gt;

&lt;h2 id=&quot;interaction-behavior&quot;&gt;Interaction Behavior&lt;/h2&gt;

&lt;p&gt;We don’t take turns in a half-duplex manner while talking. Even then, most
dialog management systems are designed like sequential turn-taking state
machines where party A says something, then hands over control to party B, then
takes back after B is done. The way we take turns in true spoken conversations
is more &lt;em&gt;full-duplex&lt;/em&gt; and that’s where a lot of interesting conversational
phenomenon happen.&lt;/p&gt;

&lt;p&gt;While conversing, we freely barge-in, attempt corrections, and show other
backchannel behaviors. When the other party also start doing the same and
utilizing these both parties can have much more effective and grounded
conversations.&lt;/p&gt;

&lt;p&gt;Additionally, because of lack of a visual interface to keep the context, user
recall around dialog history is different and that leads to different flow
designs.&lt;/p&gt;

&lt;h2 id=&quot;personalization-and-adaptations&quot;&gt;Personalization and adaptations&lt;/h2&gt;

&lt;p&gt;With all the extra added richness in the signals, the potential of
personalization and adaptations goes up. A human talking to another human does
many micro-adaptations including the choice-of-words (common with text
conversations) and the acoustics of their voices based on the ongoing
conversation.&lt;/p&gt;

&lt;p&gt;Sometimes these adaptations get ossified and form &lt;em&gt;sub-languages&lt;/em&gt; that need
different approaches of designing conversations. In our experience, people
talking to voice bots talks in a different sub-language, a relatively
understudied phenomenon.&lt;/p&gt;

&lt;h2 id=&quot;response-generation&quot;&gt;Response Generation&lt;/h2&gt;

&lt;p&gt;Similar to the section on input &lt;em&gt;signals&lt;/em&gt;, the output &lt;em&gt;signal&lt;/em&gt; from the voice
bot is also (should also be) extremely rich. This puts a lot of stake in
response production for natural conversation. The timing and content of sounds,
along with their tones impart strong semantic and pragmatic sense to the
utterance. Clever use of these also drive the conversations in a more fruitful
direction for both parties.&lt;/p&gt;

&lt;p&gt;Possibilities concerning this area of work is &lt;em&gt;extremely&lt;/em&gt; limited in text
messaging.&lt;/p&gt;

&lt;h2 id=&quot;development&quot;&gt;Development&lt;/h2&gt;

&lt;p&gt;Finally, working with audios is more difficult than text because of additional
and storage processing capabilities needed. Here is an audio utterance for the
text “1 2 3”:&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws3 = WaveSurfer.create({
           container: '#waveform-3',
           backend: 'MediaElement'
         });
         ws3.load('/assets/audios/posts/speech-first-conversational-ai/counts.wav');

         ws3.on('audioprocess', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('ready', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('finish', function () {
           let button = $('#controls-3 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-3').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws3.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws3.skipBackward()
                 break
               case 'forward':
                 ws3.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-3&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-3&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-3&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;❯ file counts.wav  # 48.0 kB (48,044 bytes)
counts.wav: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 Hz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compare this with just 6 bytes needed for the string itself (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;1 2 3&quot; | wc
--bytes&lt;/code&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;These differences lead to gaps that are difficult to bridge and that’s what
keeps us busy at Skit. If these problems interest you, you should reach out to
us on &lt;a href=&quot;mailto:join@skit.ai&quot;&gt;join@skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Epistolary communication aside. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">We often get asked about the differences between voice and chat bots. The most common perception is that the voice bot problem can be reduced to chat bot after plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS) system. We believe that’s an overly naive assumption about spoken conversations, even in restricted goal-oriented dialog systems. This post is an attempt to describe the differences involved and define what Speech-First Conversational AI means.</summary></entry><entry><title type="html">Speaker Entrainment</title><link href="/speaker-entrainment/" rel="alternate" type="text/html" title="Speaker Entrainment" /><published>2022-02-04T00:00:00+00:00</published><updated>2022-02-04T00:00:00+00:00</updated><id>/speaker-entrainment</id><content type="html" xml:base="/speaker-entrainment/">&lt;p&gt;In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Speaker Entrainment (also known as accomodation or alignment) is a psycho-social phenomenon that has been observed in human-human conversations in which interlocutors tend to match each other’s speech features. Believed to be crucial to the success and naturalness of human-human conversations, this can look like matching style related aspects such as pitch, rate of articulation or intensity, or content related factors, such as lexical patterns.&lt;/p&gt;

&lt;p&gt;This phenomenon essentially helps one manage the social “distance” between the two speakers, and hence serves to build trust. This trust has the potential to increase call resolution rates and improve customer satisfaction in task-oriented dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/user-study.jpg&quot; /&gt;
  &lt;figcaption&gt;User study for speaker entrainment.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Much research has gone into what features are relevant for speaker entrainment, such as phonetic features [1], linguistic features such as word choice [2], structure/syntax [3], style [4] and acoustic-prosodic features (pitch, intensity, rate of articulation, NHR, jitter, shimmer) [5, 6, 7].&lt;/p&gt;

&lt;p&gt;Based on this, our research work in this project aims at building a baseline bot using low-level understanding of the above features to establish the statistical significance of speaker entrainment as well as understand its potential to improve customer experience. We also publish a &lt;a href=&quot;https://tech.skit.ai/explore/speaker-entrainment&quot;&gt;demo&lt;/a&gt; to show what this looks like in real-world conversations.&lt;/p&gt;

&lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt;

&lt;p&gt;There have been a few implementations of speaker entrainment modules in research, such as Lubold &lt;em&gt;et al.&lt;/em&gt; (2016) [8], which discusses a system of entrainment based only on modelling pitch and Levitan &lt;em&gt;et al.&lt;/em&gt; (2016) [9] which details a system based on \(f_0\), intensity and rate of articulation. Subsequently Hoegen &lt;em&gt;et al.&lt;/em&gt; (2019) [10] discusses a system of modelling acoustic and content (lexical) variables separately, and Entrainment2Vec (2020) [11] details a graph based model of entrainment with vector representations in multi-party dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/entrainer.png&quot; /&gt;
  &lt;figcaption&gt;Basic structure of a Speaker Entrainment dialog system (reproduced from [14]).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;For our baseline model, we choose mean \(f_0\), intensity and rate of articulation (calculated by utterances/sec). We average over these over the past three utterances (two speaker utterances and one bot utterance) to calculate the value for next bot utterance. This is inspired from [10] and prevents drastic jumps in the bot’s voice profile, which might lead to unnaturalness. For the value of any feature \(F\) at turn \(i\) we have,&lt;/p&gt;

\[F_{bot, i} = \frac{F_{user, i-1} + F_{bot, i-1} + F_{user, i-2}}{3}\]

&lt;p&gt;as the value of the feature at \(i\)-th turn in the bot’s speech.&lt;/p&gt;
&lt;h1 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h1&gt;

&lt;p&gt;We record 11 scripts with varying measures in each of the three features (e.g. pitch rising, intensity low/high, rate of articulation low) and one with an angry user using the entraining bot and a control bot which does not entrain. We involve 30 participants in this experiment who are asked to rate the bots on factors such as likeability and naturalness. We then conduct a paired right taled t-test to determine the statistical significance of speaker entrainment over the three features and combinations thereof.&lt;/p&gt;

&lt;p&gt;The questions have been inspired from Shamekhi &lt;em&gt;et al.&lt;/em&gt; [12], which are posed comparatively, e.g. “Does Bot A sound more natural than Bot B?”. These are answered on a comparative scale as well (from Strongly Disagree to Strongly Agree) to encourage decisiveness in differentiation among participants. Note that if one assumes the comparative scale comes from a difference of scores that the participant evaluates internally, the paired t-test can still be conducted. This is because if we assume \(X_C\) to be the comparative score arising from the difference of \(X_A\) and \(X_B\) (score for Bot A and Bot B respectively), we have,&lt;/p&gt;

\[X_C = X_B - X_A\]

&lt;p&gt;Note that for performing a t-test we only need the difference in mean and the sum of squares of standard deviation.&lt;/p&gt;

\[t=\frac{E[X_B]-E[X_A]}{\sqrt{\frac{Var(X_B)}{n}+\frac{Var(X_B)}{n}}}\]

&lt;p&gt;This can be easily rearranged to&lt;/p&gt;

\[t=\frac{E[X_c]}{\sqrt{\frac{Var(X_c)}{n}}}\]

&lt;p&gt;With this t-value, knowing what side of the tail our data lies on, we use the right tailed cumulative distribution and calculate the p-values accordingly.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We find that the entrained bot performs better than the non-entrained bot in most cases. Keeping our \(\alpha=0.01\), we reject the null hypothesis for entrainment in multiple feature-sets, namely: pitch, intensity, pitch and rate of articulation combined and loudness and rate of articulation combined. In most other cases, including that of the angry user, we find that the entrained bot performs better as well, however the p-values aren’t as signficant. There are cases when the non-entrained bot performs better than the the entrained bot, but they have a large overlap with the cases in which our perception module performs inaccurately, e.g. rate of articulation is higher in shorter utterances due to lesser number of silent periods.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;

&lt;p&gt;There are a few issues with our investigations in terms of the insights we can draw. Firstly, in cases of the entrained bot performing better, it is difficult to disentangle whether this is a result of the changed voice sounding better in absolute (e.g. perhaps the participants have a preference to higher pitched/faster speaking voices as a result of shared socio-cultural factors).&lt;/p&gt;

&lt;p&gt;Secondly, in the cases of entrained bot performing worse, it is again difficult to disentangle if this poor performance arises from entraining poorly or whether speaker entrainment over this feature leads to bad performance in general.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future Work&lt;/h1&gt;

&lt;p&gt;Levitan (2020) [13] is an inspiration of future directions in speaker entrainment research. So far, there is a significant lacunae in speaker entrainment research as far as incorporation of deep learning is concerned, be it in the perception module (i.e. rich representation spaces for audio) or control module (TTS with a natural control over features and emotions).&lt;/p&gt;

&lt;p&gt;Classifying user speech should also be incorporated to understand the degree of entrainment that is necessary, rather than just entraining for every user, which can look like discriminating based on the style of conversation such as High Involvement and High Consideration (described in Hoegen &lt;em&gt;et al.&lt;/em&gt; [10]). This layer of classification serves to decide the quality/degree of entrainment, which has the potential to improve customer experience. Furthermore, this layer can help detect angry/dissatisfied users as well to plan appropriate course of action by detecting high intensity speech, hyper-articulation, etc.&lt;/p&gt;

&lt;p&gt;One can stand to improve the quality of experimentation as well. Apart from having more granular options for providing opinions (like a 7-point scale), we can have more speakers as users and more voice profiles for the bot to disentangle the experiment’s results from inherent voice qualities. Moreover, any improvement in the entrainment module will improve the quality of results as well.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Many psycho-sociologists deem speaker entrainment to be crucial to naturalness and trust building in human-human conversations. Recent advancements discussed in Levitan (2020) [13] imply that speaker entrainment is more nuanced than previously thought, but this means if implemented and modelled well, speaker entrainment has the potential of significantly changing the way voicebots interact with users.&lt;/p&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;p&gt;[1] J. S. Pardo, “On phonetic convergence during conversational interaction,” J. Acoust. Soc. Am., vol. 119, no. 4, pp. 2382–2393, 2006.&lt;/p&gt;

&lt;p&gt;[2]	K. G. Niederhoffer and J. W. Pennebaker, “Linguistic style matching in social interaction,” J. Lang. Soc. Psychol., vol. 21, no. 4, pp. 337–360, 2002.&lt;/p&gt;

&lt;p&gt;[3]	D. Reitter, J. D. Moore, and F. Keller, “Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,” 2010.&lt;/p&gt;

&lt;p&gt;[4]	C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs,” ArXiv Prepr. ArXiv11063077, 2011.&lt;/p&gt;

&lt;p&gt;[5]	R. Levitan and J. Hirschberg, “Measuring Acoustic-Prosodic Entrainment with Respect to Multiple Levels and Dimensions,” 2011.&lt;/p&gt;

&lt;p&gt;[6]	R. Levitan, A. Gravano, L. Willson, Š. Beňuš, J. Hirschberg, and A. Nenkova, “Acoustic-prosodic entrainment and social behavior,” in Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, 2012, pp. 11–19.&lt;/p&gt;

&lt;p&gt;[7]	N. Lubold and H. Pon-Barry, “Acoustic-prosodic entrainment and rapport in collaborative learning dialogues,” in Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, 2014, pp. 5–12.&lt;/p&gt;

&lt;p&gt;[8] N. Lubold, H. Pon-Barry, and E. Walker, “Naturalness and rapport in a pitch adaptive learning companion,” Dec. 2015, pp. 103–110. doi: 10.1109/ASRU.2015.7404781.&lt;/p&gt;

&lt;p&gt;[9]	R. Levitan et al., “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar,” in Proc. Interspeech 2016, 2016, pp. 1166–1170. doi: 10.21437/Interspeech.2016-985.&lt;/p&gt;

&lt;p&gt;[10]	R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski, “An End-to-End Conversational Style Matching Agent,” Proc. 19th ACM Int. Conf. Intell. Virtual Agents, pp. 111–118, Jul. 2019, doi: 10.1145/3308532.3329473.&lt;/p&gt;

&lt;p&gt;[11]	Z. Rahimi and D. Litman, “Entrainment2vec: Embedding entrainment for multi-party dialogues,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, no. 05, pp. 8681–8688.&lt;/p&gt;

&lt;p&gt;[12] A. Shamekhi, M. Czerwinski, G. Mark, M. Novotny, and G. Bennett, “An Exploratory Study Toward the Preferred Conversational Style for Compatible Virtual Agents,” Oct. 2017, Accessed: May 28, 2021. &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/exploratory-study-toward-preferred-conversational-style-compatible-virtual-agents-2/&quot;&gt;Online Available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[13] R. Levitan, “Developing an Integrated Model of Speech Entrainment,” in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Japan, Jul. 2020, pp. 5159–5163. doi: 10.24963/ijcai.2020/727.&lt;/p&gt;

&lt;p&gt;[14] Levitan, Rivka, et al. “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar.” Interspeech. Vol. 16. 2016.&lt;/p&gt;</content><author><name>Shikhar Mohan</name></author><category term="Machine Learning" /><category term="ASR" /><summary type="html">In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/entrain.png" /><media:content medium="image" url="/assets/images/entrain.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Evaluating an ASR in a Spoken Dialogue System</title><link href="/evaluating-an-asr-in-a-spoken-dialogue-system/" rel="alternate" type="text/html" title="Evaluating an ASR in a Spoken Dialogue System" /><published>2022-01-21T00:00:00+00:00</published><updated>2022-01-21T00:00:00+00:00</updated><id>/evaluating-an-asr-in-a-spoken-dialogue-system</id><content type="html" xml:base="/evaluating-an-asr-in-a-spoken-dialogue-system/">&lt;p&gt;An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.&lt;/p&gt;

&lt;p&gt;The ASR module takes as input a spoken utterance and outputs the most likely transcription. Most ASR’s output multiple alternatives with certain confidence scores. Some ASR’s including Kaldi’s implementations output N-Best alternatives in an order not necessarily reflective of the confidence. Some ASR systems output likelihood information of word sequences in the form of word-lattices or confusion networks [1] with probability information.&lt;/p&gt;

&lt;h2 id=&quot;what-is-wer-&quot;&gt;What is WER ?&lt;/h2&gt;

&lt;p&gt;Word Error Rate measures the transcription errors, treating words as the smallest unit. It takes 2 inputs, the actual transcript and a hypothesis transcript.&lt;/p&gt;

&lt;p&gt;There are two types of WER:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Isolated word recognition (IWR-WER)&lt;/li&gt;
  &lt;li&gt;Connected Speech Recognition (CSR-WER)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Isolated Word Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This considers the words in isolation and is not based on any alignment. It simply measures the number of non-hits.&lt;/p&gt;

\[IWR-WER = 1-\frac{H}{N}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits&lt;br /&gt;
N = total matched I/O words&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Connected Speech Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is calculated on the basis of alignment and uses the Levenshtein distance for words which measures the minimum edit distance. This is efficiently calculated using dynamic programming. It calculates the best alignment which minimises the number of substitutions, insertions and deletions necessary to map the actual transcript to the hypothesis transcript, giving equal weight to all the operations. WER is not input/output symmetric, as N is the total number of words in the actual transcripts.&lt;/p&gt;

\[CSR-WER = (I+S+D)/(N)\]

&lt;p&gt;where,
I = insertions&lt;br /&gt;
S = substitutions&lt;br /&gt;
D = deletions&lt;br /&gt;
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;An example calculation of the best alignment to calculate the CSR WER is shown below.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/wer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1: Taken from Speech and Language Processing by Jurafsky and Martin [2].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The CSR WER for the above hypothesis is = (6+3+1)/13 = 0.77&lt;/p&gt;

&lt;p&gt;The upper bound of CSR WER is not 1, but \(\frac{max(N1, N2)}{N1}\) where N1 is length of true transcripts and N2 is length of hypothesis transcript.&lt;/p&gt;

&lt;h2 id=&quot;statistical-significance-of-wer-&quot;&gt;Statistical Significance of WER ?&lt;/h2&gt;

&lt;p&gt;Improvements in WER’s over a test set is one of the standard ways of evaluating upgrades in an ASR. Often, what is missed out is in this evaluation is whether the gain is statistically significant or not. One of the standard statistical tests is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989) [2]. For an example on how to calculate the statistic, consult the book by Jurafsky.&lt;/p&gt;

&lt;h2 id=&quot;issues-with-wer&quot;&gt;Issues with WER&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It gives the same importance to words like “a”, “the” compared to verbs and nouns that carry more semantic value.&lt;/li&gt;
  &lt;li&gt;WER is a purely 1 : 1 transcript based metric, and doesn’t take into account the rich output of ASR systems like alternatives or word lattices.&lt;/li&gt;
  &lt;li&gt;The concept of edit-distance that WER is based on is appropriate for a dictation machine where the additional cost is that of correcting the transcripts rather than applications where communicating the meaning is of primary importance&lt;/li&gt;
  &lt;li&gt;It doesn’t take into account the performance of an ASR in the context of the dialogue pipeline. For example if a higher WER makes no difference in the information retrieval for the downstream SLU then is the improvement worth it ?
    &lt;ul&gt;
      &lt;li&gt;An example of a context dependent metric is discussed in [4].&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;It is not a true % based metric, because it has no upper bound therefore it doesn’t tell you how good a system is, but only that one is better than another. Even for the later, it provides only a heuristic for ranking of performance.
    &lt;ul&gt;
      &lt;li&gt;Consider two ASR systems, ASR-1 that replaces 2 wrong words for every word it listens too, and another ASR system that replaces 1 wrong word for every word it listens to. Both communicate zero information, but the WER for ASR-1 is 2 and the WER for ASR-1 is 1. This 50% difference in WER is not reflective of performance, since both the systems communicate no correct information whatsoever.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;debunking-conventional-wisdom&quot;&gt;Debunking Conventional Wisdom&lt;/h2&gt;

&lt;p&gt;One might assume that better ASR’s improve the performance of all downstream SLU systems. A paper in 2003 [7] found this was not always the case.  Their model had a 17% better slot accuracy despite a 46% worse WER performance. They have this gain by using a SLU model as the language model for speech recognition. Therefore it is not necessary that an oracular ASR will solve downstream inaccuracies. It is important to be aware that there might be non-linear correlations in metrics for the ASR vs the downstream task.&lt;/p&gt;

&lt;h2 id=&quot;wer-variants&quot;&gt;WER Variants:&lt;/h2&gt;

&lt;p&gt;A few WER variants are discussed below.&lt;/p&gt;

&lt;p&gt;Metrics that look at a different granularity than that of words :
&lt;em&gt;**&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sentence Error Rate (SER)&lt;/strong&gt; : The percentage of sentences with at least one word error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Character Error Rate (CER) :&lt;/strong&gt; Similar as WER with the smallest units as characters and not words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next two error rates are variations of WER, are are bounded between [0,1].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Match Error Rate (MER)&lt;/strong&gt; [3] : Measures the probability of a given match being incorrect.&lt;/li&gt;
&lt;/ul&gt;

\[MER = \frac{I+S+D}{I+S+D+H} = 1 - \frac{H}{I+S+D+H}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;MER is always &amp;lt;= WER.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Word Information Lost&lt;/strong&gt; &lt;strong&gt;(WIL)&lt;/strong&gt; [3] : Information theoretic measure based on entropy. For more details look at the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example values, comparing WER, MER and WIL are shown below:&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/mer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Taken from [3].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note : For implementations of WER, WIL, and MER, have a look at &lt;a href=&quot;https://pypi.org/project/jiwer/&quot;&gt;Jiwer&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;additional-error-analysis&quot;&gt;Additional Error Analysis&lt;/h2&gt;

&lt;p&gt;Apart from looking at just single metrics for evaluating an ASR there are few additional metrics that one should use to evaluate a goal oriented ASR deployed for a given application:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which speaker demographic is most often misrecognised ?&lt;/li&gt;
  &lt;li&gt;What (context-dependent) phones are least well recognised ?&lt;/li&gt;
  &lt;li&gt;Which words are most confused ? Generate a confusion matrix of confused words.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These often help prioritise improvements in terms of what might benefit the goal the most.&lt;/p&gt;

&lt;h1 id=&quot;semantics-based-metrics&quot;&gt;Semantics based Metrics&lt;/h1&gt;

&lt;p&gt;WER doesn’t look at the meaning of what has been transcribed despite the fact that is the semantics that are most relevant in an ASR that is a part of a spoken dialogue system.&lt;/p&gt;

&lt;h2 id=&quot;concept-accuracy&quot;&gt;Concept accuracy&lt;/h2&gt;

&lt;p&gt;It is a simple metric that looks at the accuracy of the concepts that are of relevance in the transcripts.&lt;/p&gt;

&lt;p&gt;Example:&lt;br /&gt;
Reference - I want to go from Boston to Baltimore on September 29
Hypothesis - Go from Boston to Baltimore on December 29&lt;/p&gt;

&lt;p&gt;The WER is \(45\%\). However if one looks at concept accuracy, it is 2/3. Out of the 3 concepts : “Boston”, “Baltimore” and “September 29” it gets 2 of them right.&lt;/p&gt;

&lt;h2 id=&quot;wer-with-embeddings&quot;&gt;WER with embeddings&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;this paper&lt;/a&gt; [9] they look at augmenting the WER metric for an ASR that is used for a Spoken Language Translation (SLT) task.&lt;/p&gt;

&lt;p&gt;They argue that some morphological operations, like adding a plural doesn’t impact the translation task and that such substitution errors should be penalised differently. To decide which ones to penalise they use word embeddings. They call their new metric, WER-E i.e. WER with embeddings. The only change in this metric is that the substitution cost in WER is replaced by the cosine distance between the two words, so near identical words get assigned a very low cost.&lt;/p&gt;

&lt;h2 id=&quot;other-metrics&quot;&gt;Other Metrics&lt;/h2&gt;

&lt;p&gt;There are lots of different papers that augment the WER metric or introduce a new metric specific to a downstream task. Mentioning a few of them below :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In [10] a new measure called Automatic Transcription Evaluation for Named Entity (ATENE) is introduced for the NER downstream task&lt;/li&gt;
  &lt;li&gt;3 new evaluation metrics are introduced in [12], where the downstream application is Information Retrieval&lt;/li&gt;
  &lt;li&gt;SemDist metric is introduced in [13] for downstream SLU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Downstream applications aside, it is quite pertinent to evaluate and benchmarks ASR across different social and demographic groups to evaluate bias and fairness. Systems don’t exist in isolation from the society in which they are deployed in and it is important that ML Engineers pay head to such metrics while deploying ASRs.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://arxiv.org/abs/2106.06519&quot;&gt;N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing (3rd ed. draft)&lt;/a&gt; by &lt;a href=&quot;http://web.stanford.edu/people/jurafsky/&quot;&gt;Dan Jurafsky&lt;/a&gt; and &lt;a href=&quot;http://www.cs.colorado.edu/~martin/&quot;&gt;James H. Martin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.researchgate.net/profile/Phil-Green-4/publication/221478089_From_WER_and_RIL_to_MER_and_WIL_improved_evaluation_measures_for_connected_speech_recognition/links/00b4951f95799284d9000000/From-WER-and-RIL-to-MER-and-WIL-improved-evaluation-measures-for-connected-speech-recognition.pdf&quot;&gt;From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition&lt;/a&gt; (2004)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.4023&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Automatic Human Utility Evaluation of ASR Systems: Does WER Really Predict Performance&lt;/a&gt;? (2013)&lt;/p&gt;

&lt;p&gt;[5] : http://www.cs.columbia.edu/~julia/courses/CS4706/asreval.pdf&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3447548.3467372&quot;&gt;Meaning Error Rate: ASR domain-specific metric framework&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.424&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Is word error rate a good indicator for spoken language understanding accuracy&lt;/a&gt; (2003)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1877050918302187&quot;&gt;Automatic speech recognition errors detection and correction: A review&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;Better Evaluation of ASR in Speech Translation Context Using Word Embeddings&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1289.pdf&quot;&gt;How to Evaluate ASR Output for Named Entity Recognition ?&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[11] : &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/5947637&quot;&gt;Why word error rate is not a good metric for speech recognizer training for the speech translation task?&lt;/a&gt; (2011)&lt;/p&gt;

&lt;p&gt;[12] : &lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/44281782/Evaluating_ASR_Output_for_Information_Re20160331-31804-1u2wdv8.pdf?1459484689=&amp;amp;response-content-disposition=inline%3B+filename%3DEvaluating_ASR_Output_for_Information_Re.pdf&amp;amp;Expires=1641991659&amp;amp;Signature=DMkXBKRcISYtsJw2M6l9P4Lth-0ZEF6plQyHD08TWaIhRZSaZdFCmtTagKLx7YMLGX~ZxvQtgiQe4EHJcZ-aGL5DiUh3Vfztn7feWDMZF~bEgFMluedYI6Jq39t0BBd2mVJjuUVCVtx1-S--pH89PQ9aFcfpbSH4W88uytgZHwXyZyeR9tIVzM45lblIVeFtvVNwREc6jmm1ijW4ir0lGbGlfI2DoLXwyYFM6pltQHngtoVtAfBrBF3XOMB2AwXA0hQkSDlO0v~iwletUK1o3xBdmb6MrK47A7nt8TlO9xFB33RA8hrN-KnafvJGdhI-Or8Ic2HN4cWlXvr~2uetNg__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=19&quot;&gt;Evaluating ASR Output for Information Retrieval&lt;/a&gt; (2007)&lt;/p&gt;

&lt;p&gt;[13] : &lt;a href=&quot;https://arxiv.org/pdf/2104.02138.pdf&quot;&gt;Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[14] : &lt;a href=&quot;https://aclanthology.org/W13-4064.pdf&quot;&gt;Which ASR should I choose for my dialogue system?&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[15] : &lt;a href=&quot;https://arxiv.org/pdf/2010.11745.pdf&quot;&gt;Rethinking evaluation in ASR : Are out models robust enough ?&lt;/a&gt; (2021)&lt;/p&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="WER" /><category term="sticky" /><summary type="html">An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/evaluating-asr.jpg" /><media:content medium="image" url="/assets/images/evaluating-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Complexity of Conversations - I</title><link href="/complexity-of-conversations/" rel="alternate" type="text/html" title="Complexity of Conversations - I" /><published>2022-01-18T00:00:00+00:00</published><updated>2022-01-18T00:00:00+00:00</updated><id>/complexity-of-conversations</id><content type="html" xml:base="/complexity-of-conversations/">&lt;p&gt;Consider a restaurant booking voice bot built using a frames and slots approach.
While this can easily &lt;em&gt;solve the problem&lt;/em&gt; of booking with high automation
accuracy, such slot-filling framework can’t carry on a meaningful conversation
in a debate unless you over-engineer the frames and slots to monstrous
complexity. Booking a restaurant is a form of conversation that’s innately
simpler than arguing with someone in a debate competition. We can roughly say
that these two conversations lie in different complexity classes. In this first
post of a series, we will lay down a few factors that will help us define a map
of conversations arranged according to their complexities.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;At Skit we build many kinds of task-oriented dialog systems for call center
automation. A very crude categorization of such systems, for us, is based on the
interaction with a sibling call handling system&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and the direction of
intention, user or agent initiation.&lt;/p&gt;

&lt;p&gt;While we have used multiple approaches to measure difficulty of conversations
for our product delivery purposes, it’s interesting to see if a purer framework
could be built around this. Similar to computational complexity, this can tell
us which problems are tractable under an algorithm. It can also help in
identifying the path towards the next generation of human machine conversational
systems.&lt;/p&gt;

&lt;p&gt;We will cover a few thoughts around a few core constructs of the framework next.
First is the definition of success in a conversation, second around the
difficulty of doing so, and third about the algorithms and their complexities.&lt;/p&gt;

&lt;h2 id=&quot;success&quot;&gt;Success&lt;/h2&gt;

&lt;p&gt;The definition of success of a conversation depends on alignment between goals
of the involved parties.&lt;/p&gt;

&lt;p&gt;A regular goal oriented conversation with user initiation has a simple success
definition. For example, a call with user asking for temperature of a place can
be called successful if the temperature is provided. The metric here could be
something like the following:&lt;/p&gt;

\[\text{Resolution%} = \frac{\text{Calls where user goals were met}}{\text{Total calls}}\]

&lt;p&gt;This simple formulation becomes tricky as the alignment between user and bot
goals becomes inexact. For example when the &lt;em&gt;bot is calling&lt;/em&gt; the user for
payment reminders, it might not just want to remind and collect next reminder
time, but also want to persuade the users to pay as early as possible. In such
cases, you might want to use another rate for &lt;em&gt;favorable outcomes&lt;/em&gt;:&lt;/p&gt;

\[\text{Favorable%} = \frac{\text{Calls with favorable outcomes}}{\text{Resolved calls}}\]

&lt;p&gt;Another example where this works is in &lt;em&gt;argumentative&lt;/em&gt; conversations where
holding a reasonable conversation and reaching conclusion is important
(&lt;em&gt;resolution&lt;/em&gt;), but winning the argument (the favorable outcome) is what defines
success.&lt;/p&gt;

&lt;h2 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h2&gt;

&lt;p&gt;We can look at difficulty of conversations from multiple levels. For the
smallest unit of dialog, a turn&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, parsing and generating every utterance in a
conversation can be rated for difficulty. Here are a few factors that drive
difficulty for a turn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Knowledge needed for understanding an entity. This could be general or
specific to a situation, involving connection with a dynamic or static
knowledge source.&lt;/li&gt;
  &lt;li&gt;Speech Acts. Simpler acts like &lt;em&gt;greeting&lt;/em&gt; are easier to handle, while
something like &lt;em&gt;pleading&lt;/em&gt; is hard.&lt;/li&gt;
  &lt;li&gt;Expression complexity, intentional or unintentional. For speech systems, this
is even more varied because of the richness of acoustic signals that adds to
the underlying text. For example sarcasm could be expressed by changing the
tone of speech and not just via textual constructs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But these are not sufficient since higher order behaviors across multiple turns
also make conversations difficult. As an example, consider negotiation for the
price in a market. In this situation, you need to use the conversational context
across turns to decide your next steps in a way that’s harder than situations
where context dependency is lesser.&lt;/p&gt;

&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;

&lt;p&gt;The frameworks of developing, and running, voice bots are the last pieces that
will help us to map out the tractability of problems. A common method in the
industry is the frame-filling model that roughly needs learning &lt;em&gt;intents&lt;/em&gt; and
&lt;em&gt;entities&lt;/em&gt; for each utterance.&lt;/p&gt;

&lt;p&gt;These frameworks, or algorithms, can be measured on their resource consumption.
We can start with sample complexity of conversations as the resource and create
statements like the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Under framework \(f\), you need an order of \(N\) data points to supervise a
voice bot of class \(k\) to achieve a success rate of \(R(N)\)&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be mapped to the statistical learning problem and abstractions can be
translated from there.&lt;/p&gt;

&lt;p&gt;Having set the groundwork here, we will tackle the more interesting problem of
complexity class definitions in a later post.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The sibling system could be non-existent, or backend human agents who can
take over the more co plex conversations, or complex parts of running
conversations. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We can cover backchannel events also in a kind of &lt;em&gt;background&lt;/em&gt; turn. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Including the other factors around PAC learning. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">Consider a restaurant booking voice bot built using a frames and slots approach. While this can easily solve the problem of booking with high automation accuracy, such slot-filling framework can’t carry on a meaningful conversation in a debate unless you over-engineer the frames and slots to monstrous complexity. Booking a restaurant is a form of conversation that’s innately simpler than arguing with someone in a debate competition. We can roughly say that these two conversations lie in different complexity classes. In this first post of a series, we will lay down a few factors that will help us define a map of conversations arranged according to their complexities.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On using ASR Alternatives for a Better SLU</title><link href="/on-using-asr-alternatives-for-a-better-slu/" rel="alternate" type="text/html" title="On using ASR Alternatives for a Better SLU" /><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>/on-using-asr-alternatives-for-a-better-slu</id><content type="html" xml:base="/on-using-asr-alternatives-for-a-better-slu/">&lt;p&gt;This blog discusses some concepts from the recently published &lt;a href=&quot;https://arxiv.org/pdf/2106.06519.pdf&quot;&gt;paper&lt;/a&gt; by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in &lt;a href=&quot;https://2021.aclweb.org/&quot;&gt;ACL-IJCNLP’21&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Voice bots in the industry heavily rely on the use of Automatic Speech Recognition (ASR) transcripts to understand the user and capture intents &amp;amp; entities which are then used to resolve the customer’s problem. ASR’s however are far from perfect, especially on noisy real world data and on instances with acoustic confusion. The downstream Spoken Language Understanding (SLU) components would benefit greatly if they take the ASR’s confusion into account.&lt;/p&gt;

&lt;p&gt;Example use-case with confounding ASR transcripts (over voice):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Bot: When would you like to make the reservation ?
User (actually said): right now
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ASR alternatives for given user’s speech:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- like, now
- right now
- write no
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Often the downstream SLU services which act on ASR transcripts use only the most probable alternative (also called 1-best alternative), thereby leaving out a lot of other information that exists in the form of alternative probabilities. This paper presents a simple way of using the information that exists in the alternatives to get SOTA performance on a standard benchmark for a SLU system.&lt;/p&gt;

&lt;h1 id=&quot;types-of-asr-outputs&quot;&gt;Types of ASR Outputs&lt;/h1&gt;

&lt;p&gt;Before we get into how to best use ASR’s confusion to increase the performance of the SLU, we discuss the different ways an ASR outputs the probable word sequence probabilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N-best alternatives : this is a list of the top N alternative sentences for the given spoken utterance. These are usually ranked based on probability of occurrence.&lt;/li&gt;
  &lt;li&gt;Word lattices : As shown in the below figure, they have no a-priori structure per se. Every path from the start node to the end node represents a possible hypothesis transcript. Every transition adds a word to the hypothesis.&lt;/li&gt;
  &lt;li&gt;Word confusion networks : They are a more compact and normalised topology for word lattices. They enforce certain constaints such that competing words should be in the same group and there by also ensure aligment of words that occur at the same time interval. Though these graphs capture lesser number of possibilities, this topology can be used to get as high recognition accuracy as using word lattices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The transitions probabilities of both, word lattices and word confusion networks are weighted by the acoustic and language model probabilities.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/word-lattices.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 1: The structure of a word lattice as contrasted with a word confusion network.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;dataset&quot;&gt;Dataset&lt;/h1&gt;

&lt;p&gt;The task that is used to compare the modelling approaches is the &lt;a href=&quot;https://aclanthology.org/W14-4337.pdf&quot;&gt;DSTC - 2 challenge&lt;/a&gt; where one is required to predict intent act-slot-value triplets. Pairs of sentences i.e. a sentence whose intent is required to be predicted along with its context is given. For each sentence the top 10 best ASR alternatives are also provided.&lt;/p&gt;

&lt;h1 id=&quot;modelling-approach&quot;&gt;Modelling Approach&lt;/h1&gt;

&lt;p&gt;The central idea of the paper is to leverage pre-trained transformer models BERT and XLMRoBERTa and fine-tune them with a simple input representation shown below. The input consists of the ASR alternatives seperated by a seperator token concatenated with the context. A segment id (not shown below) is also used, which is used to contrast the context (in green) with the alternatives (in purple).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/example-input.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Input representation used to fine-tune transformer models.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;On top of the transformer model a semantic tuple classifier (STC) is applied to predict the act-slot-value triplets. Using this approach, we achieve a performance equivalent to the prior state-of-the-art model on DSTC-2 dataset. We get comparable F1 and SOTA accuracy. The previous SOTA model, WCN-BERT uses word confusion networks.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/model-architecture.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Model Architecture, The input representation is encoded by a transformer model which forms an
input for a Semantic Tuple Classifier (STC). STC uses binary classifiers to predict the presence of act-slot pairs,
followed by a multi-class classifier that predicts the value for each act-slot pair.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Here, using a simple ASR output such an N-best alternatives we get a comparable performance to the SOTA model that uses a much more informative probability graph such as a word confusion network.&lt;/p&gt;

&lt;h1 id=&quot;ablation-experiments&quot;&gt;Ablation Experiments&lt;/h1&gt;

&lt;p&gt;Two ablation experiments are performed. One on low data regimes and another to check the impact of the context on performance.&lt;/p&gt;

&lt;h2 id=&quot;low-data-regime&quot;&gt;Low Data Regime&lt;/h2&gt;

&lt;p&gt;Here, the baseline models are compared with our approach using 5%, 10%, 20% and 50% of the training data respectively. In all these situations our approach beats SOTA by a considerable margin, proving that our training approach effectively transfer learns. We hypothesise, that this is due to the structural similarity between the input representations of the initial training of these open sourced models and the fine-turning that is done on DSTC-2 dataset. It also demonstrates that n-best alternatives are a more natural representation to fine-tune transformer models compared to word lattices or word confusion networks.&lt;/p&gt;

&lt;h2 id=&quot;context-dependency&quot;&gt;Context Dependency&lt;/h2&gt;

&lt;p&gt;In this experiment we wanted to test the impact of adding context (last turn) to performance and to check if it is relevant at all. An improvement of around ~1.5% F1 score is obtained using context. An example situation where context is relevance is shown below. Dialog context can help in resolving ambiguities in parses and reducing the impact of ASR noise.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/context-dependence.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Example to demonstrate context dependence.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Lastly, this methodology can be used by users of third-party ASR APIs which do not provide word-lattice information and thereby is more accessible.&lt;/p&gt;

&lt;h1 id=&quot;implementation-and-citation&quot;&gt;Implementation and Citation&lt;/h1&gt;

&lt;p&gt;The code for the project can be found &lt;a href=&quot;https://github.com/skit-ai/N-Best-ASR-Transformer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you use our work, please cite using the following BibTex Citation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{ganesan-etal-2021-n,
    title = &quot;N-Best {ASR} Transformer: Enhancing {SLU} Performance using Multiple {ASR} Hypotheses&quot;,
    author = &quot;Ganesan, Karthik  and
      Bamdev, Pakhi  and
      B, Jaivarsan  and
      Venugopal, Amresh  and
      Tushar, Abhinav&quot;,
    booktitle = &quot;Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)&quot;,
    month = aug,
    year = &quot;2021&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2021.acl-short.14&quot;,
    doi = &quot;10.18653/v1/2021.acl-short.14&quot;,
    pages = &quot;93--98&quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Tür, G., Deoras, A., &amp;amp; Hakkani-Tür, D. (2013, September). Semantic parsing using word confusion networks with conditional random fields. In &lt;em&gt;INTERSPEECH&lt;/em&gt; (pp. 2579-2583).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="SLU" /><summary type="html">This blog discusses some concepts from the recently published paper by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in ACL-IJCNLP’21.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/n-best-asr.jpg" /><media:content medium="image" url="/assets/images/n-best-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Authentication in gRPC</title><link href="/authentication-in-grpc/" rel="alternate" type="text/html" title="Authentication in gRPC" /><published>2021-10-31T00:00:00+00:00</published><updated>2021-10-31T00:00:00+00:00</updated><id>/authentication-in-grpc</id><content type="html" xml:base="/authentication-in-grpc/">&lt;p&gt;In gRPC, there are a number of ways you can add authentication between client
and server. It is handled via Credentials Objects.&lt;/p&gt;

&lt;p&gt;There are two types of credential objects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Channel Credentials&lt;/strong&gt;: These are handled on the channel level, i.e when
the connection is established and a channel is created.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Call Credentials&lt;/strong&gt;: These are handled on per request level, i.e for every
RPC call that is made. These Credential objects can also be combined to
create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CompositeChannelCredentials&lt;/code&gt; with one Channel Credential and one
Call Credential object.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let us see how we can use these credential objects.&lt;/p&gt;

&lt;h2 id=&quot;client-side-tlsssl-authentication&quot;&gt;Client-Side TLS/SSL Authentication&lt;/h2&gt;
&lt;p&gt;gRPC provides a way to establish a connection without any secure connection i.e just like HTTP.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithInsecure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For secure communication, we will create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransportCredentials&lt;/code&gt; which is a type
of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ChannelCredential&lt;/code&gt; object.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewClientTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;localhost:50051&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServerTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about generating own ssl certificates &lt;a href=&quot;https://www.linuxjournal.com/content/understanding-public-key-infrastructure-and-x509-certificates&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the case where you don’t own the client, it means you are creating a gRPC
API for public use, you cannot give your certificate to everyone using your
client. In that case, we rely on well known &lt;a href=&quot;https://en.wikipedia.org/wiki/Certificate_authority&quot;&gt;Certificate
Authority&lt;/a&gt; like LetsEncrypt, Amazon, etc. to generate a
certificate. So let us change our client code a little.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;InsecureSkipVerify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewTLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;// server code remains the same&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case what happens is that grpc loads the certificates of well-known
Certificate Authorities from the OS and sends it to the server, hence no need
to manually provide a certificate.&lt;/p&gt;

&lt;h2 id=&quot;token-based-authentication--oauth2&quot;&gt;Token-Based Authentication / OAuth2&lt;/h2&gt;

&lt;p&gt;Many a time we want to differentiate a client by issuing them different tokens.
TLS Authentication is a good way to secure your connection but it does not tell
us from which client the request is coming from. We will send the token in
request metadata just like HTTP Headers.&lt;/p&gt;

&lt;p&gt;gRPC has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google.golang.org/grpc/metadata&lt;/code&gt; package to send and receive metadata
with a RPC request.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// add metadata to request context&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Bearer xxx-xxx-xxx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// use this context to call rpc&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CallRPC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requestObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnaryInterceptor&lt;/code&gt; on the server which acts as middleware and
checks for the token for all the requests.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// create a middleware&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryServerInfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FromContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;missing context metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  
    &lt;span class=&quot;c&quot;&gt;// Take care: grpc internally reduce key values to lowercase&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xxx-xxx-xxx&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// pass this when creating server&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But above code only works for non-streaming RPCs. For Streaming RPCs you can
implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StreamInterceptor&lt;/code&gt;. Instead of implementing it again you can use
this package. I hope this article helps you with authentication in gRPC.&lt;/p&gt;

&lt;p&gt;If you are interested in working with cutting-edge technologies, come work with
&lt;a href=&quot;https://skit.ai&quot;&gt;Skit&lt;/a&gt;. Apply &lt;a href=&quot;https://skit.recruiterbox.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Deepankar Agrawal</name></author><category term="Engineering" /><category term="authentication" /><category term="gRPC" /><summary type="html">In gRPC, there are a number of ways you can add authentication between client and server. It is handled via Credentials Objects.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/thumbnail-authentication-in-grpc.svg" /><media:content medium="image" url="/assets/images/thumbnail-authentication-in-grpc.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>