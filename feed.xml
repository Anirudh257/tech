<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-02-06T10:31:42+00:00</updated><id>/feed.xml</id><title type="html">Skit Tech</title><subtitle>Speech Technology from Skit</subtitle><entry><title type="html">Speech-First Conversational AI</title><link href="/speech-first-conversational-ai/" rel="alternate" type="text/html" title="Speech-First Conversational AI" /><published>2022-02-06T00:00:00+00:00</published><updated>2022-02-06T00:00:00+00:00</updated><id>/speech-first-conversational-ai</id><content type="html" xml:base="/speech-first-conversational-ai/">&lt;p&gt;We often get asked about the differences between voice and chat bots. The most
common perception is that the voice bot problem can be reduced to chat bot after
plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS)
system. We believe that’s an overly naive assumption about spoken conversations,
even in restricted goal-oriented dialog systems. This post is an attempt to
describe the differences involved and define what &lt;em&gt;Speech-First&lt;/em&gt; Conversational
AI means.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Speech is the most sophisticated behavior of the most complex organism in the
known universe. - &lt;a href=&quot;https://youtu.be/Zy3Ny-WjyGE?t=251&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Conversational AI systems solve problems of conversations, either using text or
voice. Since &lt;em&gt;conversations&lt;/em&gt; are specific to humans, there are many
anthropomorphic expectations from these systems. These expectations, while still
strong, are less restraining in text conversations as compared to speech. Speech
is deeply ingrained in human communication and minor misses could lead to
violation of user expectations. Contrast this with text messaging which is a,
relatively new, human-constructed channel&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; where expectations are different
and more lenient.&lt;/p&gt;

&lt;p&gt;There are multiple academic sources on differences between speech and text, here
we will describing a few key differences that we have noticed while building
speech-first conversation systems in more practical settings.&lt;/p&gt;

&lt;h2 id=&quot;signal&quot;&gt;Signal&lt;/h2&gt;

&lt;p&gt;In addition to the textual content, speech signals contain information about the
user’s state, trait, and the environment. Speech isn’t merely a redundant
modality, but adds valuable extra information. Different style of uttering the
same utterance can drastically change the meaning, something that’s used a lot
in human-human conversations.&lt;/p&gt;

&lt;p&gt;Environmental factors like recording quality, background ambience, and audio
events impact signals’ reception and semantics. Even beyond the immediate
environment, a lot of socio-cultural factors are embedded in speech beyond the
level they are in text chats. Because the signals are rich, the difficulty of a
few common problems across text and speech, like low-resource languages, is
higher.&lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;Once you go on transcribing audios utterances using ASRs, transcription errors
will add on to your burden. While ASR systems are improving day-on-day, there
still is error potential in handling acoustically similar utterances. Overall,
an entirely new set of problems like far-field ASR, signal enhancement, etc.
exist in spoken conversations.&lt;/p&gt;

&lt;p&gt;Additionally many &lt;em&gt;noisy&lt;/em&gt; deviations from fluent speech are not mere errors but
develop their own pragmatic sense and convey strong meaning. Speech
&lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_disfluency&quot;&gt;disfluencies&lt;/a&gt; are commonly
assumed behaviors of natural conversations and lack of them could even cause
discomfort.&lt;/p&gt;

&lt;h2 id=&quot;interaction-behavior&quot;&gt;Interaction Behavior&lt;/h2&gt;

&lt;p&gt;We don’t take turns in a half-duplex manner while talking. Even then, most
dialog management systems are designed like sequential turn-taking state
machines where party A says something, then hands over control to party B, then
takes back after B is done. The way we take turns in true spoken conversations
is more &lt;em&gt;full-duplex&lt;/em&gt; and that’s where a lot of interesting conversational
phenomenon happen.&lt;/p&gt;

&lt;p&gt;While conversing, we freely barge-in, attempt corrections, and show other
backchannel behaviors. When the other party also start doing the same and
utilizing these both parties can have much more effective and grounded
conversations.&lt;/p&gt;

&lt;p&gt;Additionally, because of lack of a visual interface to keep the context, user
recall around dialog history is different and that leads to different flow
designs.&lt;/p&gt;

&lt;h2 id=&quot;personalization-and-adaptations&quot;&gt;Personalization and adaptations&lt;/h2&gt;

&lt;p&gt;With all the extra added richness in the signals, the potential of
personalization and adaptations goes up. A human talking to another human does
many micro-adaptations including the choice-of-words (common with text
conversations) and the acoustics of their voices based on the ongoing
conversation.&lt;/p&gt;

&lt;p&gt;Sometimes these adaptations get ossified and form &lt;em&gt;sub-languages&lt;/em&gt; that need
different approaches of designing conversations. In our experience, people
talking to voice bots talks in a different sub-language, a relatively
understudied phenomenon.&lt;/p&gt;

&lt;h2 id=&quot;response-generation&quot;&gt;Response Generation&lt;/h2&gt;

&lt;p&gt;Similar to the section on input &lt;em&gt;signals&lt;/em&gt;, the output &lt;em&gt;signal&lt;/em&gt; from the voice
bot is also (should also be) extremely rich. This puts a lot of stake in
response production for natural conversation. The timing and content of sounds,
along with their tones impart strong semantic and pragmatic sense to the
utterance. Clever use of these also drive the conversations in a more fruitful
direction for both parties.&lt;/p&gt;

&lt;p&gt;Possibilities concerning this area of work is &lt;em&gt;extremely&lt;/em&gt; limited in text
messaging.&lt;/p&gt;

&lt;h2 id=&quot;development&quot;&gt;Development&lt;/h2&gt;

&lt;p&gt;Finally, working with audios is more difficult than text because of additional
and storage processing capabilities needed. Here is an audio utterance for the
text “1 2 3”:&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws3 = WaveSurfer.create({
           container: '#waveform-3',
           backend: 'MediaElement'
         });
         ws3.load('/assets/audios/posts/speech-first-conversational-ai/counts.wav');

         ws3.on('audioprocess', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('ready', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('finish', function () {
           let button = $('#controls-3 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-3').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws3.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws3.skipBackward()
                 break
               case 'forward':
                 ws3.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-3&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-3&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-3&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;❯ file counts.wav  # 48.0 kB (48,044 bytes)
counts.wav: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 Hz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compare this with just 6 bytes needed for the string itself (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;1 2 3&quot; | wc
--bytes&lt;/code&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;These differences lead to gaps that are difficult to bridge and that’s what
keeps us busy at Skit. If these problems interest you, you should reach out to
us on &lt;a href=&quot;mailto:join@skit.ai&quot;&gt;join@skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Epistolary communication aside. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">We often get asked about the differences between voice and chat bots. The most common perception is that the voice bot problem can be reduced to chat bot after plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS) system. We believe that’s an overly naive assumption about spoken conversations, even in restricted goal-oriented dialog systems. This post is an attempt to describe the differences involved and define what Speech-First Conversational AI means.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Evaluating an ASR in a Spoken Dialogue System</title><link href="/evaluating-an-asr-in-a-spoken-dialogue-system/" rel="alternate" type="text/html" title="Evaluating an ASR in a Spoken Dialogue System" /><published>2022-01-21T00:00:00+00:00</published><updated>2022-01-21T00:00:00+00:00</updated><id>/evaluating-an-asr-in-a-spoken-dialogue-system</id><content type="html" xml:base="/evaluating-an-asr-in-a-spoken-dialogue-system/">&lt;p&gt;An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.&lt;/p&gt;

&lt;p&gt;The ASR module takes as input a spoken utterance and outputs the most likely transcription. Most ASR’s output multiple alternatives with certain confidence scores. Some ASR’s including Kaldi’s implementations output N-Best alternatives in an order not necessarily reflective of the confidence. Some ASR systems output likelihood information of word sequences in the form of word-lattices or confusion networks [1] with probability information.&lt;/p&gt;

&lt;h2 id=&quot;what-is-wer-&quot;&gt;What is WER ?&lt;/h2&gt;

&lt;p&gt;Word Error Rate measures the transcription errors, treating words as the smallest unit. It takes 2 inputs, the actual transcript and a hypothesis transcript.&lt;/p&gt;

&lt;p&gt;There are two types of WER:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Isolated word recognition (IWR-WER)&lt;/li&gt;
  &lt;li&gt;Connected Speech Recognition (CSR-WER)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Isolated Word Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This considers the words in isolation and is not based on any alignment. It simply measures the number of non-hits.&lt;/p&gt;

\[IWR-WER = 1-\frac{H}{N}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits&lt;br /&gt;
N = total matched I/O words&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Connected Speech Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is calculated on the basis of alignment and uses the Levenshtein distance for words which measures the minimum edit distance. This is efficiently calculated using dynamic programming. It calculates the best alignment which minimises the number of substitutions, insertions and deletions necessary to map the actual transcript to the hypothesis transcript, giving equal weight to all the operations. WER is not input/output symmetric, as N is the total number of words in the actual transcripts.&lt;/p&gt;

\[CSR-WER = (I+S+D)/(N)\]

&lt;p&gt;where,
I = insertions&lt;br /&gt;
S = substitutions&lt;br /&gt;
D = deletions&lt;br /&gt;
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;An example calculation of the best alignment to calculate the CSR WER is shown below.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/wer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1: Taken from Speech and Language Processing by Jurafsky and Martin [2].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The CSR WER for the above hypothesis is = (6+3+1)/13 = 0.77&lt;/p&gt;

&lt;p&gt;The upper bound of CSR WER is not 1, but \(\frac{max(N1, N2)}{N1}\) where N1 is length of true transcripts and N2 is length of hypothesis transcript.&lt;/p&gt;

&lt;h2 id=&quot;statistical-significance-of-wer-&quot;&gt;Statistical Significance of WER ?&lt;/h2&gt;

&lt;p&gt;Improvements in WER’s over a test set is one of the standard ways of evaluating upgrades in an ASR. Often, what is missed out is in this evaluation is whether the gain is statistically significant or not. One of the standard statistical tests is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989) [2]. For an example on how to calculate the statistic, consult the book by Jurafsky.&lt;/p&gt;

&lt;h2 id=&quot;issues-with-wer&quot;&gt;Issues with WER&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It gives the same importance to words like “a”, “the” compared to verbs and nouns that carry more semantic value.&lt;/li&gt;
  &lt;li&gt;WER is a purely 1 : 1 transcript based metric, and doesn’t take into account the rich output of ASR systems like alternatives or word lattices.&lt;/li&gt;
  &lt;li&gt;The concept of edit-distance that WER is based on is appropriate for a dictation machine where the additional cost is that of correcting the transcripts rather than applications where communicating the meaning is of primary importance&lt;/li&gt;
  &lt;li&gt;It doesn’t take into account the performance of an ASR in the context of the dialogue pipeline. For example if a higher WER makes no difference in the information retrieval for the downstream SLU then is the improvement worth it ?
    &lt;ul&gt;
      &lt;li&gt;An example of a context dependent metric is discussed in [4].&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;It is not a true % based metric, because it has no upper bound therefore it doesn’t tell you how good a system is, but only that one is better than another. Even for the later, it provides only a heuristic for ranking of performance.
    &lt;ul&gt;
      &lt;li&gt;Consider two ASR systems, ASR-1 that replaces 2 wrong words for every word it listens too, and another ASR system that replaces 1 wrong word for every word it listens to. Both communicate zero information, but the WER for ASR-1 is 2 and the WER for ASR-1 is 1. This 50% difference in WER is not reflective of performance, since both the systems communicate no correct information whatsoever.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;debunking-conventional-wisdom&quot;&gt;Debunking Conventional Wisdom&lt;/h2&gt;

&lt;p&gt;One might assume that better ASR’s improve the performance of all downstream SLU systems. A paper in 2003 [7] found this was not always the case.  Their model had a 17% better slot accuracy despite a 46% worse WER performance. They have this gain by using a SLU model as the language model for speech recognition. Therefore it is not necessary that an oracular ASR will solve downstream inaccuracies. It is important to be aware that there might be non-linear correlations in metrics for the ASR vs the downstream task.&lt;/p&gt;

&lt;h2 id=&quot;wer-variants&quot;&gt;WER Variants:&lt;/h2&gt;

&lt;p&gt;A few WER variants are discussed below.&lt;/p&gt;

&lt;p&gt;Metrics that look at a different granularity than that of words :
&lt;em&gt;**&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sentence Error Rate (SER)&lt;/strong&gt; : The percentage of sentences with at least one word error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Character Error Rate (CER) :&lt;/strong&gt; Similar as WER with the smallest units as characters and not words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next two error rates are variations of WER, are are bounded between [0,1].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Match Error Rate (MER)&lt;/strong&gt; [3] : Measures the probability of a given match being incorrect.&lt;/li&gt;
&lt;/ul&gt;

\[MER = \frac{I+S+D}{I+S+D+H} = 1 - \frac{H}{I+S+D+H}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;MER is always &amp;lt;= WER.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Word Information Lost&lt;/strong&gt; &lt;strong&gt;(WIL)&lt;/strong&gt; [3] : Information theoretic measure based on entropy. For more details look at the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example values, comparing WER, MER and WIL are shown below:&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/mer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Taken from [3].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note : For implementations of WER, WIL, and MER, have a look at &lt;a href=&quot;https://pypi.org/project/jiwer/&quot;&gt;Jiwer&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;additional-error-analysis&quot;&gt;Additional Error Analysis&lt;/h2&gt;

&lt;p&gt;Apart from looking at just single metrics for evaluating an ASR there are few additional metrics that one should use to evaluate a goal oriented ASR deployed for a given application:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which speaker demographic is most often misrecognised ?&lt;/li&gt;
  &lt;li&gt;What (context-dependent) phones are least well recognised ?&lt;/li&gt;
  &lt;li&gt;Which words are most confused ? Generate a confusion matrix of confused words.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These often help prioritise improvements in terms of what might benefit the goal the most.&lt;/p&gt;

&lt;h1 id=&quot;semantics-based-metrics&quot;&gt;Semantics based Metrics&lt;/h1&gt;

&lt;p&gt;WER doesn’t look at the meaning of what has been transcribed despite the fact that is the semantics that are most relevant in an ASR that is a part of a spoken dialogue system.&lt;/p&gt;

&lt;h2 id=&quot;concept-accuracy&quot;&gt;Concept accuracy&lt;/h2&gt;

&lt;p&gt;It is a simple metric that looks at the accuracy of the concepts that are of relevance in the transcripts.&lt;/p&gt;

&lt;p&gt;Example:&lt;br /&gt;
Reference - I want to go from Boston to Baltimore on September 29
Hypothesis - Go from Boston to Baltimore on December 29&lt;/p&gt;

&lt;p&gt;The WER is \(45\%\). However if one looks at concept accuracy, it is 2/3. Out of the 3 concepts : “Boston”, “Baltimore” and “September 29” it gets 2 of them right.&lt;/p&gt;

&lt;h2 id=&quot;wer-with-embeddings&quot;&gt;WER with embeddings&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;this paper&lt;/a&gt; [9] they look at augmenting the WER metric for an ASR that is used for a Spoken Language Translation (SLT) task.&lt;/p&gt;

&lt;p&gt;They argue that some morphological operations, like adding a plural doesn’t impact the translation task and that such substitution errors should be penalised differently. To decide which ones to penalise they use word embeddings. They call their new metric, WER-E i.e. WER with embeddings. The only change in this metric is that the substitution cost in WER is replaced by the cosine distance between the two words, so near identical words get assigned a very low cost.&lt;/p&gt;

&lt;h2 id=&quot;other-metrics&quot;&gt;Other Metrics&lt;/h2&gt;

&lt;p&gt;There are lots of different papers that augment the WER metric or introduce a new metric specific to a downstream task. Mentioning a few of them below :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In [10] a new measure called Automatic Transcription Evaluation for Named Entity (ATENE) is introduced for the NER downstream task&lt;/li&gt;
  &lt;li&gt;3 new evaluation metrics are introduced in [12], where the downstream application is Information Retrieval&lt;/li&gt;
  &lt;li&gt;SemDist metric is introduced in [13] for downstream SLU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Downstream applications aside, it is quite pertinent to evaluate and benchmarks ASR across different social and demographic groups to evaluate bias and fairness. Systems don’t exist in isolation from the society in which they are deployed in and it is important that ML Engineers pay head to such metrics while deploying ASRs.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://arxiv.org/abs/2106.06519&quot;&gt;N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing (3rd ed. draft)&lt;/a&gt; by &lt;a href=&quot;http://web.stanford.edu/people/jurafsky/&quot;&gt;Dan Jurafsky&lt;/a&gt; and &lt;a href=&quot;http://www.cs.colorado.edu/~martin/&quot;&gt;James H. Martin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.researchgate.net/profile/Phil-Green-4/publication/221478089_From_WER_and_RIL_to_MER_and_WIL_improved_evaluation_measures_for_connected_speech_recognition/links/00b4951f95799284d9000000/From-WER-and-RIL-to-MER-and-WIL-improved-evaluation-measures-for-connected-speech-recognition.pdf&quot;&gt;From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition&lt;/a&gt; (2004)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.4023&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Automatic Human Utility Evaluation of ASR Systems: Does WER Really Predict Performance&lt;/a&gt;? (2013)&lt;/p&gt;

&lt;p&gt;[5] : http://www.cs.columbia.edu/~julia/courses/CS4706/asreval.pdf&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3447548.3467372&quot;&gt;Meaning Error Rate: ASR domain-specific metric framework&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.424&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Is word error rate a good indicator for spoken language understanding accuracy&lt;/a&gt; (2003)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1877050918302187&quot;&gt;Automatic speech recognition errors detection and correction: A review&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;Better Evaluation of ASR in Speech Translation Context Using Word Embeddings&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1289.pdf&quot;&gt;How to Evaluate ASR Output for Named Entity Recognition ?&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[11] : &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/5947637&quot;&gt;Why word error rate is not a good metric for speech recognizer training for the speech translation task?&lt;/a&gt; (2011)&lt;/p&gt;

&lt;p&gt;[12] : &lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/44281782/Evaluating_ASR_Output_for_Information_Re20160331-31804-1u2wdv8.pdf?1459484689=&amp;amp;response-content-disposition=inline%3B+filename%3DEvaluating_ASR_Output_for_Information_Re.pdf&amp;amp;Expires=1641991659&amp;amp;Signature=DMkXBKRcISYtsJw2M6l9P4Lth-0ZEF6plQyHD08TWaIhRZSaZdFCmtTagKLx7YMLGX~ZxvQtgiQe4EHJcZ-aGL5DiUh3Vfztn7feWDMZF~bEgFMluedYI6Jq39t0BBd2mVJjuUVCVtx1-S--pH89PQ9aFcfpbSH4W88uytgZHwXyZyeR9tIVzM45lblIVeFtvVNwREc6jmm1ijW4ir0lGbGlfI2DoLXwyYFM6pltQHngtoVtAfBrBF3XOMB2AwXA0hQkSDlO0v~iwletUK1o3xBdmb6MrK47A7nt8TlO9xFB33RA8hrN-KnafvJGdhI-Or8Ic2HN4cWlXvr~2uetNg__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=19&quot;&gt;Evaluating ASR Output for Information Retrieval&lt;/a&gt; (2007)&lt;/p&gt;

&lt;p&gt;[13] : &lt;a href=&quot;https://arxiv.org/pdf/2104.02138.pdf&quot;&gt;Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[14] : &lt;a href=&quot;https://aclanthology.org/W13-4064.pdf&quot;&gt;Which ASR should I choose for my dialogue system?&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[15] : &lt;a href=&quot;https://arxiv.org/pdf/2010.11745.pdf&quot;&gt;Rethinking evaluation in ASR : Are out models robust enough ?&lt;/a&gt; (2021)&lt;/p&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="WER" /><summary type="html">An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/evaluating-asr.jpg" /><media:content medium="image" url="/assets/images/evaluating-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Complexity of Conversations - I</title><link href="/complexity-of-conversations/" rel="alternate" type="text/html" title="Complexity of Conversations - I" /><published>2022-01-18T00:00:00+00:00</published><updated>2022-01-18T00:00:00+00:00</updated><id>/complexity-of-conversations</id><content type="html" xml:base="/complexity-of-conversations/">&lt;p&gt;Consider a restaurant booking voice bot built using a frames and slots approach.
While this can easily &lt;em&gt;solve the problem&lt;/em&gt; of booking with high automation
accuracy, such slot-filling framework can’t carry on a meaningful conversation
in a debate unless you over-engineer the frames and slots to monstrous
complexity. Booking a restaurant is a form of conversation that’s innately
simpler than arguing with someone in a debate competition. We can roughly say
that these two conversations lie in different complexity classes. In this first
post of a series, we will lay down a few factors that will help us define a map
of conversations arranged according to their complexities.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;At Skit we build many kinds of task-oriented dialog systems for call center
automation. A very crude categorization of such systems, for us, is based on the
interaction with a sibling call handling system&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and the direction of
intention, user or agent initiation.&lt;/p&gt;

&lt;p&gt;While we have used multiple approaches to measure difficulty of conversations
for our product delivery purposes, it’s interesting to see if a purer framework
could be built around this. Similar to computational complexity, this can tell
us which problems are tractable under an algorithm. It can also help in
identifying the path towards the next generation of human machine conversational
systems.&lt;/p&gt;

&lt;p&gt;We will cover a few thoughts around a few core constructs of the framework next.
First is the definition of success in a conversation, second around the
difficulty of doing so, and third about the algorithms and their complexities.&lt;/p&gt;

&lt;h2 id=&quot;success&quot;&gt;Success&lt;/h2&gt;

&lt;p&gt;The definition of success of a conversation depends on alignment between goals
of the involved parties.&lt;/p&gt;

&lt;p&gt;A regular goal oriented conversation with user initiation has a simple success
definition. For example, a call with user asking for temperature of a place can
be called successful if the temperature is provided. The metric here could be
something like the following:&lt;/p&gt;

\[\text{Resolution%} = \frac{\text{Calls where user goals were met}}{\text{Total calls}}\]

&lt;p&gt;This simple formulation becomes tricky as the alignment between user and bot
goals becomes inexact. For example when the &lt;em&gt;bot is calling&lt;/em&gt; the user for
payment reminders, it might not just want to remind and collect next reminder
time, but also want to persuade the users to pay as early as possible. In such
cases, you might want to use another rate for &lt;em&gt;favorable outcomes&lt;/em&gt;:&lt;/p&gt;

\[\text{Favorable%} = \frac{\text{Calls with favorable outcomes}}{\text{Resolved calls}}\]

&lt;p&gt;Another example where this works is in &lt;em&gt;argumentative&lt;/em&gt; conversations where
holding a reasonable conversation and reaching conclusion is important
(&lt;em&gt;resolution&lt;/em&gt;), but winning the argument (the favorable outcome) is what defines
success.&lt;/p&gt;

&lt;h2 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h2&gt;

&lt;p&gt;We can look at difficulty of conversations from multiple levels. For the
smallest unit of dialog, a turn&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, parsing and generating every utterance in a
conversation can be rated for difficulty. Here are a few factors that drive
difficulty for a turn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Knowledge needed for understanding an entity. This could be general or
specific to a situation, involving connection with a dynamic or static
knowledge source.&lt;/li&gt;
  &lt;li&gt;Speech Acts. Simpler acts like &lt;em&gt;greeting&lt;/em&gt; are easier to handle, while
something like &lt;em&gt;pleading&lt;/em&gt; is hard.&lt;/li&gt;
  &lt;li&gt;Expression complexity, intentional or unintentional. For speech systems, this
is even more varied because of the richness of acoustic signals that adds to
the underlying text. For example sarcasm could be expressed by changing the
tone of speech and not just via textual constructs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But these are not sufficient since higher order behaviors across multiple turns
also make conversations difficult. As an example, consider negotiation for the
price in a market. In this situation, you need to use the conversational context
across turns to decide your next steps in a way that’s harder than situations
where context dependency is lesser.&lt;/p&gt;

&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;

&lt;p&gt;The frameworks of developing, and running, voice bots are the last pieces that
will help us to map out the tractability of problems. A common method in the
industry is the frame-filling model that roughly needs learning &lt;em&gt;intents&lt;/em&gt; and
&lt;em&gt;entities&lt;/em&gt; for each utterance.&lt;/p&gt;

&lt;p&gt;These frameworks, or algorithms, can be measured on their resource consumption.
We can start with sample complexity of conversations as the resource and create
statements like the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Under framework \(f\), you need an order of \(N\) data points to supervise a
voice bot of class \(k\) to achieve a success rate of \(R(N)\)&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be mapped to the statistical learning problem and abstractions can be
translated from there.&lt;/p&gt;

&lt;p&gt;Having set the groundwork here, we will tackle the more interesting problem of
complexity class definitions in a later post.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The sibling system could be non-existent, or backend human agents who can
take over the more co plex conversations, or complex parts of running
conversations. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We can cover backchannel events also in a kind of &lt;em&gt;background&lt;/em&gt; turn. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Including the other factors around PAC learning. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">Consider a restaurant booking voice bot built using a frames and slots approach. While this can easily solve the problem of booking with high automation accuracy, such slot-filling framework can’t carry on a meaningful conversation in a debate unless you over-engineer the frames and slots to monstrous complexity. Booking a restaurant is a form of conversation that’s innately simpler than arguing with someone in a debate competition. We can roughly say that these two conversations lie in different complexity classes. In this first post of a series, we will lay down a few factors that will help us define a map of conversations arranged according to their complexities.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On using ASR Alternatives for a Better SLU</title><link href="/on-using-asr-alternatives-for-a-better-slu/" rel="alternate" type="text/html" title="On using ASR Alternatives for a Better SLU" /><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>/on-using-asr-alternatives-for-a-better-slu</id><content type="html" xml:base="/on-using-asr-alternatives-for-a-better-slu/">&lt;p&gt;This blog discusses some concepts from the recently published &lt;a href=&quot;https://arxiv.org/pdf/2106.06519.pdf&quot;&gt;paper&lt;/a&gt; by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in &lt;a href=&quot;https://2021.aclweb.org/&quot;&gt;ACL-IJCNLP’21&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Voice bots in the industry heavily rely on the use of Automatic Speech Recognition (ASR) transcripts to understand the user and capture intents &amp;amp; entities which are then used to resolve the customer’s problem. ASR’s however are far from perfect, especially on noisy real world data and on instances with acoustic confusion. The downstream Spoken Language Understanding (SLU) components would benefit greatly if they take the ASR’s confusion into account.&lt;/p&gt;

&lt;p&gt;Example use-case with confounding ASR transcripts (over voice):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Bot: When would you like to make the reservation ?
User (actually said): right now
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ASR alternatives for given user’s speech:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- like, now
- right now
- write no
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Often the downstream SLU services which act on ASR transcripts use only the most probable alternative (also called 1-best alternative), thereby leaving out a lot of other information that exists in the form of alternative probabilities. This paper presents a simple way of using the information that exists in the alternatives to get SOTA performance on a standard benchmark for a SLU system.&lt;/p&gt;

&lt;h1 id=&quot;types-of-asr-outputs&quot;&gt;Types of ASR Outputs&lt;/h1&gt;

&lt;p&gt;Before we get into how to best use ASR’s confusion to increase the performance of the SLU, we discuss the different ways an ASR outputs the probable word sequence probabilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N-best alternatives : this is a list of the top N alternative sentences for the given spoken utterance. These are usually ranked based on probability of occurrence.&lt;/li&gt;
  &lt;li&gt;Word lattices : As shown in the below figure, they have no a-priori structure per se. Every path from the start node to the end node represents a possible hypothesis transcript. Every transition adds a word to the hypothesis.&lt;/li&gt;
  &lt;li&gt;Word confusion networks : They are a more compact and normalised topology for word lattices. They enforce certain constaints such that competing words should be in the same group and there by also ensure aligment of words that occur at the same time interval. Though these graphs capture lesser number of possibilities, this topology can be used to get as high recognition accuracy as using word lattices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The transitions probabilities of both, word lattices and word confusion networks are weighted by the acoustic and language model probabilities.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/word-lattices.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 1: The structure of a word lattice as contrasted with a word confusion network.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;dataset&quot;&gt;Dataset&lt;/h1&gt;

&lt;p&gt;The task that is used to compare the modelling approaches is the &lt;a href=&quot;https://aclanthology.org/W14-4337.pdf&quot;&gt;DSTC - 2 challenge&lt;/a&gt; where one is required to predict intent act-slot-value triplets. Pairs of sentences i.e. a sentence whose intent is required to be predicted along with its context is given. For each sentence the top 10 best ASR alternatives are also provided.&lt;/p&gt;

&lt;h1 id=&quot;modelling-approach&quot;&gt;Modelling Approach&lt;/h1&gt;

&lt;p&gt;The central idea of the paper is to leverage pre-trained transformer models BERT and XLMRoBERTa and fine-tune them with a simple input representation shown below. The input consists of the ASR alternatives seperated by a seperator token concatenated with the context. A segment id (not shown below) is also used, which is used to contrast the context (in green) with the alternatives (in purple).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/example-input.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Input representation used to fine-tune transformer models.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;On top of the transformer model a semantic tuple classifier (STC) is applied to predict the act-slot-value triplets. Using this approach, we achieve a performance equivalent to the prior state-of-the-art model on DSTC-2 dataset. We get comparable F1 and SOTA accuracy. The previous SOTA model, WCN-BERT uses word confusion networks.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/model-architecture.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Model Architecture, The input representation is encoded by a transformer model which forms an
input for a Semantic Tuple Classifier (STC). STC uses binary classifiers to predict the presence of act-slot pairs,
followed by a multi-class classifier that predicts the value for each act-slot pair.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Here, using a simple ASR output such an N-best alternatives we get a comparable performance to the SOTA model that uses a much more informative probability graph such as a word confusion network.&lt;/p&gt;

&lt;h1 id=&quot;ablation-experiments&quot;&gt;Ablation Experiments&lt;/h1&gt;

&lt;p&gt;Two ablation experiments are performed. One on low data regimes and another to check the impact of the context on performance.&lt;/p&gt;

&lt;h2 id=&quot;low-data-regime&quot;&gt;Low Data Regime&lt;/h2&gt;

&lt;p&gt;Here, the baseline models are compared with our approach using 5%, 10%, 20% and 50% of the training data respectively. In all these situations our approach beats SOTA by a considerable margin, proving that our training approach effectively transfer learns. We hypothesise, that this is due to the structural similarity between the input representations of the initial training of these open sourced models and the fine-turning that is done on DSTC-2 dataset. It also demonstrates that n-best alternatives are a more natural representation to fine-tune transformer models compared to word lattices or word confusion networks.&lt;/p&gt;

&lt;h2 id=&quot;context-dependency&quot;&gt;Context Dependency&lt;/h2&gt;

&lt;p&gt;In this experiment we wanted to test the impact of adding context (last turn) to performance and to check if it is relevant at all. An improvement of around ~1.5% F1 score is obtained using context. An example situation where context is relevance is shown below. Dialog context can help in resolving ambiguities in parses and reducing the impact of ASR noise.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/context-dependence.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Example to demonstrate context dependence.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Lastly, this methodology can be used by users of third-party ASR APIs which do not provide word-lattice information and thereby is more accessible.&lt;/p&gt;

&lt;h1 id=&quot;implementation-and-citation&quot;&gt;Implementation and Citation&lt;/h1&gt;

&lt;p&gt;The code for the project can be found &lt;a href=&quot;https://github.com/skit-ai/N-Best-ASR-Transformer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you use our work, please cite using the following BibTex Citation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{ganesan-etal-2021-n,
    title = &quot;N-Best {ASR} Transformer: Enhancing {SLU} Performance using Multiple {ASR} Hypotheses&quot;,
    author = &quot;Ganesan, Karthik  and
      Bamdev, Pakhi  and
      B, Jaivarsan  and
      Venugopal, Amresh  and
      Tushar, Abhinav&quot;,
    booktitle = &quot;Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)&quot;,
    month = aug,
    year = &quot;2021&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2021.acl-short.14&quot;,
    doi = &quot;10.18653/v1/2021.acl-short.14&quot;,
    pages = &quot;93--98&quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Tür, G., Deoras, A., &amp;amp; Hakkani-Tür, D. (2013, September). Semantic parsing using word confusion networks with conditional random fields. In &lt;em&gt;INTERSPEECH&lt;/em&gt; (pp. 2579-2583).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="SLU" /><summary type="html">This blog discusses some concepts from the recently published paper by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in ACL-IJCNLP’21.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/n-best-asr.jpg" /><media:content medium="image" url="/assets/images/n-best-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Authentication in gRPC</title><link href="/authentication-in-grpc/" rel="alternate" type="text/html" title="Authentication in gRPC" /><published>2021-10-31T00:00:00+00:00</published><updated>2021-10-31T00:00:00+00:00</updated><id>/authentication-in-grpc</id><content type="html" xml:base="/authentication-in-grpc/">&lt;p&gt;In gRPC, there are a number of ways you can add authentication between client
and server. It is handled via Credentials Objects.&lt;/p&gt;

&lt;p&gt;There are two types of credential objects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Channel Credentials&lt;/strong&gt;: These are handled on the channel level, i.e when
the connection is established and a channel is created.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Call Credentials&lt;/strong&gt;: These are handled on per request level, i.e for every
RPC call that is made. These Credential objects can also be combined to
create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CompositeChannelCredentials&lt;/code&gt; with one Channel Credential and one
Call Credential object.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let us see how we can use these credential objects.&lt;/p&gt;

&lt;h2 id=&quot;client-side-tlsssl-authentication&quot;&gt;Client-Side TLS/SSL Authentication&lt;/h2&gt;
&lt;p&gt;gRPC provides a way to establish a connection without any secure connection i.e just like HTTP.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithInsecure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For secure communication, we will create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransportCredentials&lt;/code&gt; which is a type
of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ChannelCredential&lt;/code&gt; object.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewClientTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;localhost:50051&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServerTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about generating own ssl certificates &lt;a href=&quot;https://www.linuxjournal.com/content/understanding-public-key-infrastructure-and-x509-certificates&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the case where you don’t own the client, it means you are creating a gRPC
API for public use, you cannot give your certificate to everyone using your
client. In that case, we rely on well known &lt;a href=&quot;https://en.wikipedia.org/wiki/Certificate_authority&quot;&gt;Certificate
Authority&lt;/a&gt; like LetsEncrypt, Amazon, etc. to generate a
certificate. So let us change our client code a little.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;InsecureSkipVerify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewTLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;// server code remains the same&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case what happens is that grpc loads the certificates of well-known
Certificate Authorities from the OS and sends it to the server, hence no need
to manually provide a certificate.&lt;/p&gt;

&lt;h2 id=&quot;token-based-authentication--oauth2&quot;&gt;Token-Based Authentication / OAuth2&lt;/h2&gt;

&lt;p&gt;Many a time we want to differentiate a client by issuing them different tokens.
TLS Authentication is a good way to secure your connection but it does not tell
us from which client the request is coming from. We will send the token in
request metadata just like HTTP Headers.&lt;/p&gt;

&lt;p&gt;gRPC has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google.golang.org/grpc/metadata&lt;/code&gt; package to send and receive metadata
with a RPC request.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// add metadata to request context&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Bearer xxx-xxx-xxx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// use this context to call rpc&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CallRPC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requestObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnaryInterceptor&lt;/code&gt; on the server which acts as middleware and
checks for the token for all the requests.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// create a middleware&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryServerInfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FromContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;missing context metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  
    &lt;span class=&quot;c&quot;&gt;// Take care: grpc internally reduce key values to lowercase&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xxx-xxx-xxx&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// pass this when creating server&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But above code only works for non-streaming RPCs. For Streaming RPCs you can
implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StreamInterceptor&lt;/code&gt;. Instead of implementing it again you can use
this package. I hope this article helps you with authentication in gRPC.&lt;/p&gt;

&lt;p&gt;If you are interested in working with cutting-edge technologies, come work with
&lt;a href=&quot;https://skit.ai&quot;&gt;Skit&lt;/a&gt;. Apply &lt;a href=&quot;https://skit.recruiterbox.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Deepankar Agrawal</name></author><category term="Engineering" /><category term="authentication" /><category term="gRPC" /><summary type="html">In gRPC, there are a number of ways you can add authentication between client and server. It is handled via Credentials Objects.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/thumbnail-authentication-in-grpc.svg" /><media:content medium="image" url="/assets/images/thumbnail-authentication-in-grpc.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Seminar - Code Mixing in NLP and Speech</title><link href="/Code-Mixing-Seminar/" rel="alternate" type="text/html" title="Seminar - Code Mixing in NLP and Speech" /><published>2021-08-24T00:00:00+00:00</published><updated>2021-08-24T00:00:00+00:00</updated><id>/Code-Mixing-Seminar</id><content type="html" xml:base="/Code-Mixing-Seminar/">&lt;p&gt;Below are some pointers and insights from the papers that we covered in the recently concluded &lt;a href=&quot;https://skit-ai.github.io/seminars/topics/&quot;&gt;seminar on Code-mixing in NLP and Speech&lt;/a&gt;. During the seminar we covered 6 papers, two each from NLP, Speech Synthesis and Speech Recognition.&lt;/p&gt;

&lt;h1 id=&quot;session-1-nlp&quot;&gt;Session-1 (NLP)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2004.12376.pdf&quot;&gt;GLUECoS : An Evaluation Benchmark for Code-Switched NLP&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shashank Shailabh&lt;/p&gt;

&lt;p&gt;The main idea of GLUECoS is to introduce a benchmark evaluation task set for code-switched NLP. It is inspired from GLUE (Generalized Language Evaluation Benchmark) and covers tasks such as Language Identification (LID),  Part-Of-Speech (POS) Tagging, Named Entity Recognition (NER), Question &amp;amp; Answering (QA), Natural Language Inference (NLI) and sentiment analysis (SENT).  and also introduces benchmark code-mixed datasets. The language pairs that this benchmark covers in English-Spanish and English-Hindi code-mixed datasets. The paper also introduces several Metrics, most of which are covered in the previous blog on Metrics for Code-switching.&lt;/p&gt;

&lt;p&gt;To arrive at a baseline performance, several cross-lingual word embeddings models such as MUSE (supervised and unsupervised embeddings), BiCVM, Biskip and GCM embeddings are used. For supervised cross-lingual embeddings one requires a parallel mono-lingual corpus of the two languages such that each sentence in one language maps as close as possible to the parallel sentence in the other language. In the unsupervised setting an adversarial loss is used to train cross-lingual embeddings. The other model that is used for the baseline is multilingual models (mBERT). This model is simply trained on monolingual data from each language, and has no explicit loss to map the embedding of the same words from both languages to the same point. Another fine-tuned version of mBert is used which is fine-tuned on synthetically generated code-switched data. Interestingly the mBert model performs better as compared the cross-lingual embeddings and fine-tuning on synthetic data further improves performance.&lt;/p&gt;

&lt;h1 id=&quot;session-2-speech-synthesis&quot;&gt;Session-2 (Speech Synthesis)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2008.00768v1.pdf&quot;&gt;Building Multilingual End-to-End Speech Synthesisers for Indian Languages&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Swaraj&lt;/p&gt;

&lt;p&gt;The goal of the paper is to build a Multilingual TTS for Indic languages in a low data resource setting. The paper discusses ways of assimilating inputs from different indian scripts both at the level of characters and phonemes, into a common input set. This leverages the similarities in phonemes and the structure of Indian languages which fall in the Abjad class of languages where vowels are diacritics.&lt;/p&gt;

&lt;p&gt;For phonemes, it looks at 2 approaches : a transliteration approach using the unified parser and the common label set; and another approach that considers a 1:1 phoneme map between the different languages. At character level they present the MLCM character set that is 68 characters in total and combines 8 indic scripts. Since vowels are a major component in indic languages, they consider approaches where the vowel and vowel modifier are mapped to the same and different input token. For modelling they use tacotron-2 and compare the impact of various input tokens on performance for monolingual and multilingual settings. The primary take away from the discussion were the different ways of combining and varying the input representations for training an Indic language TTS.&lt;/p&gt;

&lt;h1 id=&quot;session---3-nlp&quot;&gt;Session - 3 (NLP)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://aclanthology.org/2021.adaptnlp-1.12.pdf&quot;&gt;BERTologiCoMix, How does Code-Mixing interact with Multilingual BERT?&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Jaivarsan B&lt;/p&gt;

&lt;p&gt;This paper addresses the impact (on code-mixed tasks) of fine-tuning with different types of code-mixed data and outlines the changes to mBERT’s attention heads during such fine-tuning. Code-mixing can be treated as a domain adaptation problem, but in this particular context it is to adapt at the grammatical level, not purely at the vocabulary level or style level. For sake of empirical study they evaluate two different language pairs, English-Spanish (enes), English-Hindi (enhi) with the extra three different varieties on them. Those three varieties are namely: randomly-ordered CM (l-CM), grammatically appropriate CM (g-cm) and real-world CM (r-CM), so in total we have 2 lang pairs \(\times\) 3 varieties = 6 possible fine-tuning datasets on mBERT against the plain mBERT.&lt;/p&gt;

&lt;p&gt;They evaluate these 6 combinations of CM datasets on GLUECoS benchmark. They empirically find out that naturally occurring code-mixed data (r-CM) brings in the best performance improvement after fine-tuning, against synthetic fine-tunings (l-CM, g-CM) and even plain mBERT. Other task specific observations include: fine-tuning with r-CM data helps with SENT, NER, LID and QA CM tasks. General trend observed was easier tasks like LID, POS are solved in earlier layers and as complexity of tasks increases, the effective layer moves deeper. This paper’s other significant contribution is to visualize the differences to these mBERT attention heads post-CM-fine-tuning. They use three methods (including one of their own contribution), which try to answer questions related to: Has anything changed within the models due to pre-training with CM datasets? Attention patterns of which heads have changed? How do attention heads respond to code-mixed probes?. There are changes to these models, which show these attention heads are receptive to CM specific data at different layers of mBERT for different tasks and different language pairs.&lt;/p&gt;

&lt;p&gt;The most important finding from these probing experiments is that there are discernible changes introduced in the models due to exposure to CM data, of which a particularly interesting observation is that this exposure increases the overall responsivity of the attention heads to CM.&lt;/p&gt;

&lt;h1 id=&quot;session---4-speech-recognition&quot;&gt;Session - 4 (Speech Recognition)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://www.researchgate.net/profile/Karan-Taneja/publication/335829565_Exploiting_Monolingual_Speech_Corpora_for_Code-Mixed_Speech_Recognition/links/602571e3299bf1cc26bcbce9/Exploiting-Monolingual-Speech-Corpora-for-Code-Mixed-Speech-Recognition.pdf&quot;&gt;Exploiting Monolingual Speech Corpora for Code-mixed Speech Recognition&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kriti Anandan&lt;/p&gt;

&lt;p&gt;This paper aims to mitigate the problem of scarcity in code-mixed datasets for training ASR models robust to code-mixed data. It introduces two algorithms to synthetically generate code mixed data by using annotated monolingual data sets that are available in large quantities and a small amount of annotated real code-mixed data. Both these algorithms make use of probability distributions derived from real code-mixed data to imitate their characteristics. The code-mixed sentences are framed from sentence fragments extracted from the monolingual corpus. In the paper, first algorithm is a naïve approach that uses the code-mixed language span distributions to weave fragments of two different languages in an alternate fashion where fragment lengths are picked by sampling the distribution. The second algorithm uses two distributions that try to model phone transitions across fragments and within fragments.&lt;/p&gt;

&lt;p&gt;The authors also demonstrate the usefulness of these synthetically generated datasets by adding them to the existing monolingual dataset for training the acoustic model of an ASR. Results from the best acoustic model show a ~ 6.85 point drop in WER score. They also conduct some language modelling experiments by using code-mixed transcripts to train the language model of the ASR. The best system uses the best acoustic model trained from the previous experiment and the language model trained on the synthetic text generated by all the algorithms presented, as well as the real code-mixed text. The results via this method show a ~ 8.17 point drop in the WER score.&lt;/p&gt;

&lt;h1 id=&quot;session---5-speech-recognition&quot;&gt;Session - 5 (Speech Recognition)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2006.00782.pdf&quot;&gt;Learning to recognize code switched speech without forgetting monolingual speech recognition&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shahid Nagra&lt;/p&gt;

&lt;p&gt;The goal of the paper is to fine tune ASR models on code switched speech without affecting the performance on the same models on monolingual speech. They propose the use of a learning without forgetting algorithm. This approach can also be used when one doesn’t have access to the monolingual data that the model was trained on, which is often the case for open-source ASR models.&lt;/p&gt;

&lt;p&gt;They conducted 5 experiments where training and fine-tuning is varied across the 3 datasets, monolingual and code-mixed and pooled. The best model performance is got when fine-turning a model on pooled data (code-mixed + monolingual) where only 25% of code-mixed data is used. An extra KL Diverge loss between the fine-tuned and the pre-trained model is added to prevent the increase in WER on the monolingual data. A metric poWER is also introduced which measures the Levenstein distance between the phonemic representation of utterances which takes care of measuring WER in the code-mixed setting.&lt;/p&gt;

&lt;h1 id=&quot;session---6-speech-synthesis&quot;&gt;Session - 6 (Speech Synthesis)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://www1.se.cuhk.edu.hk/~hccl/publications/pub/Icassp20_cstts_camera_ready.pdf&quot;&gt;Code-Switched Speech Synthesis Using Bilingual Phonetic Posteriorgram with Only Monolingual Corpora&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shangeth Rajaa&lt;/p&gt;

&lt;p&gt;This paper aims to synthesize fluent code-switched speech using only monolingual speech corpora and explores the usage of Bilingual Phonetic Posteriorgram (PPG) as the speech representation. PPGs are the posterior probabilities of each phonetic class for a specific frame of one utterance, stacking up the PPG for two languages forms the bilingual PPG. As obtaining a bilingual/code-switched speech corpus for TTS training can be expensive, this paper solves the problem, by training a tacotron2 based code-switched TTS system with only the monolingual corpus of both the languages (English and Chinese) where each language is spoken by a different speaker. The trained model is then evaluated by generating speech for a code-switched text corpus with Mean Opinion Score (MOS) score.&lt;/p&gt;

&lt;p&gt;The paper introduces the use of Bilingual PPG as a representation of speech as PPGs are speaker-independent and can capture the context of the speech better, PPG features are computed with a pretrained ASR model for each language. They replaced the traditional acoustic model which predicts the acoustic features(Mel Spectrogram) directly from text, to predict the PPG from the text, followed by predicting the Mel Spectrogram from the predicted PPG, speaker embedding and language embedding. The proposed method also includes a residual encoder to encode the speech into a latent space in a Variational AudioEncoder (VAE) setting which helps to model the prosodic features of the audio. The proposed method was compared with a tacotron2 based text to acoustic feature (Mel spectrogram) model baseline. As the proposed method uses a PPG feature, the model was able to disentangle the context information of the speech signal across different speakers and synthesize better code-mixed speech by combining the PPG feature, speaker embedding, language embedding, and prosody features.&lt;/p&gt;

&lt;p&gt;From the experimental results, we can observe that the proposed method can synthesize high intelligible code-switched speech with a cross-lingual speaker (eg: Chinese speaker for English/code-switched text) with only training on a monolingual corpus. But the proposed method failed to beat the baseline in speech fidelity MOS score. The authors stated that the Bilingual PPG extracted was not accurate enough and needs to be further studied in future work.&lt;/p&gt;

&lt;p&gt;Do check out the blog on &lt;a href=&quot;https://tech.skit.ai/Code-Mixing-Metrics/&quot;&gt;Code-Mixing metrics&lt;/a&gt; if you haven’t already. Please write to us @ machine-learning@skit.ai in case you want to join any of our future seminars.&lt;/p&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="asr" /><category term="tts" /><category term="code-mixing" /><summary type="html">Below are some pointers and insights from the papers that we covered in the recently concluded seminar on Code-mixing in NLP and Speech. During the seminar we covered 6 papers, two each from NLP, Speech Synthesis and Speech Recognition.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/tile-work.png" /><media:content medium="image" url="/assets/images/tile-work.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Code Mixing Metrics</title><link href="/Code-Mixing-Metrics/" rel="alternate" type="text/html" title="Code Mixing Metrics" /><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>/Code-Mixing-Metrics</id><content type="html" xml:base="/Code-Mixing-Metrics/">&lt;p&gt;We at skit, recently concluded a seminar series on code-mixing, where we covered research papers that looked at approaches to deal with code-mixed data across ASR, TTS and SLU related services. Code-mixing is a common phenomena in the Indian subcontinent. Code-mixing refers to the situation when two or more languages are present in an utterance or a corpus. It is important to formulate, discuss and deliberate on metrics because, it is improvement on metrics across benchmark datasets that spur continuous research in a field.&lt;/p&gt;

&lt;h1 id=&quot;terminologies&quot;&gt;Terminologies&lt;/h1&gt;

&lt;p&gt;Because how can one study a phenomena without having a requisite vocabulary.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Matrix Language :&lt;/strong&gt; The Matrix Language-Frame (MLF) model is one of the dominant models to analyse code-switching. In this framework there is a Matrix Language and an Embedded Language. The embedding language is inserted into the mono-syntactic frame of the Matrix language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inter-sentential switching :&lt;/strong&gt;  Occurs &lt;em&gt;outside&lt;/em&gt; the sentence or the clause level&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intra-sentential switching&lt;/strong&gt; : Occurs &lt;em&gt;within&lt;/em&gt; a sentence or a clause&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intra-word switching&lt;/strong&gt; : Occurs &lt;em&gt;within&lt;/em&gt; a word itself, sometimes at phoneme boundaries example : “I’m chalaaoing a car” → “i am driving a car”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Language Span:&lt;/strong&gt; The number of monolingual words between 2 consecutive switch points in the corpus. The language span distribution is the aggregate of all such language spans into a discrete pdf.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The performance of models on code-mixed corpora, would depend on the level of code-mixing. A minuscule level of code-mixing could be treated as noise/OOV and ignored. However, as the level of code-mixing increases, the performance of the model will vary. Therefore it is important to quantify the extent of code-mixing in a corpora; “how much” and
“how often”. In the first &lt;a href=&quot;https://arxiv.org/pdf/2004.12376.pdf&quot;&gt;paper&lt;/a&gt; covered in the code-mixing seminar, there were several metrics presented, however they were not discussed in detail. We discuss them below.&lt;/p&gt;

&lt;h1 id=&quot;measuring-the-amount-of-code-mixing&quot;&gt;Measuring the amount of Code-Mixing&lt;/h1&gt;

&lt;h2 id=&quot;word-frequency-based-metrics&quot;&gt;Word-frequency based metrics&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.727.9087&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;&lt;strong&gt;Code-Mixing Index (CMI)&lt;/strong&gt;&lt;/a&gt; : An utterance level metric that measures the fraction of tokens(words) that are not from the matrix language.&lt;/p&gt;

\[CMI = 100*(1 - \frac{max(w_i)}{n-u}) \text{ if } n&amp;gt;u\]

&lt;p&gt;where,&lt;br /&gt;
n = total number of words, irrespective of language&lt;br /&gt;
u = language independent words&lt;br /&gt;
\(w_i\) = number of words in language i&lt;br /&gt;
\(max(w_i)\) measures the number of words in the matrix language&lt;/p&gt;

&lt;p&gt;If \(n=u\) i.e. if the utterance contains only language independent words then CMI = 0.&lt;/p&gt;

&lt;p&gt;One way calculate CMI for the entire corpus is to just calculate the above at a corpus level rather than an utterance level. However, this method doesn’t take into account the switching frequency. Another way is to combine the utterance level CMI as discussed &lt;a href=&quot;https://www.aclweb.org/anthology/L16-1292.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Multilingual Index (M-index)&lt;/strong&gt;&lt;/a&gt; : Quantifies the ratio of languages in the corpora based on the Gini coefficient to measure the inequality distribution of languages in the corpus.&lt;/p&gt;

\[M-index = \frac{1-\sum p_j^2}{(k-1).\sum p_j^2}\]

&lt;p&gt;where,&lt;br /&gt;
k = number of languages&lt;br /&gt;
\(p_j\) = number of words in language j divided by total number of words&lt;/p&gt;

&lt;p&gt;The M-index = 0 when the corpus is monolingual and = 1 when there is equal distribution of token across all the languages i.e. \(p_j\) for all \(j = 1/k\).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Language Entropy (LE)&lt;/strong&gt;&lt;/a&gt; :  An information theoretic alternative to the M-index. Measures the number of bits required to describe the distribution of language tags.&lt;/p&gt;

\[LE = -\sum_{1...k} p_i*log_2(p_i)\]

&lt;p&gt;where,&lt;br /&gt;
k = number of languages&lt;br /&gt;
\(p_i\) = number of words in language j divided by total number of words&lt;/p&gt;

&lt;p&gt;This metric is 0 for a monolingual corpus and is bounded by \(log_2(k)\) for equally distributed k languages. Both LE and M-index can be derived from one another.&lt;/p&gt;

&lt;p&gt;However, just specifying the frequency of words belonging to another language doesn’t provide enough information. A corpus with a higher frequency of language switches per utterance is more complex, therefore we need some measures on the frequency and distribution of code-switching points.&lt;/p&gt;

&lt;h2 id=&quot;measuring-code-switching&quot;&gt;Measuring code-switching&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Probability of Switching (I-index)&lt;/strong&gt;&lt;/a&gt; : The average number of switch points in the corpus.&lt;/p&gt;

\[f_n = P/(N-1)\]

&lt;p&gt;where,&lt;br /&gt;
\(P\) = number of code-switching points&lt;br /&gt;
\(N-1\) = possible switching points in a corpus that contains n tokens&lt;/p&gt;

&lt;p&gt;A token is considered a switch point, if the preceding token is from another language.&lt;/p&gt;

&lt;h2 id=&quot;time-course-metrics&quot;&gt;Time-course Metrics&lt;/h2&gt;

&lt;p&gt;Since, a corpus is a sequential document, it is informative to have a time-series metrics that quantifies the temporal distribution of code-switching across the corpora.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Burstiness&lt;/strong&gt;&lt;/a&gt; : Measures whether code switching occurs in bursts or is periodic is nature. It compares the code-switching behaviour in the corpus to a poisson behaviour where code-switching occurs at random.&lt;/p&gt;

&lt;p&gt;Let, \(\mu_t\) = mean language span and \(\sigma_t\) = s.t. of language span, then&lt;/p&gt;

\[Burstiness = \frac{\sigma_t-\mu_t}{\sigma_t+\mu_t}\]

&lt;p&gt;Burstiness is bounded by \([-1,1]\). For, corpus that have a periodicity in code-switching points, this value is closer to -1 and for corpuses that have less predictable periodicity have a value closer to 1.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Span Entropy (SE)&lt;/strong&gt;&lt;/a&gt; : An information theoretic measure of the language span distribution i.e. the number of bits needed to describe the probability distribution.&lt;/p&gt;

\[SE = -\sum_{1...M} p_l*log_2(p_l)\]

&lt;p&gt;where,&lt;br /&gt;
\(p_l\) is the sample probability of a language span of length l&lt;br /&gt;
M is the maximum language span&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/a&gt; : The above two metrics don’t make any claims about the time ordering of the language spans i.e. say if a long span occurs below or after the current span, then there would be no difference in the above metric values. Corpus’ with the same burstiness can have very different properties. This metric measures the extent to which the \(i^{th}\) span length is influenced by span lengths occurring before it. This metric is bounded by \([-1,1]\). When it is closer to -1, then the length of consecutive spans in negative correlated i.e. long spans followed by short ones.&lt;/p&gt;

&lt;h1 id=&quot;evaluating-code-mixed-asr-and-tts&quot;&gt;Evaluating Code-Mixed ASR and TTS&lt;/h1&gt;

&lt;h2 id=&quot;asr&quot;&gt;ASR&lt;/h2&gt;

&lt;p&gt;Conventional word error rate is not sufficient for measuring the performance of code-mixed models due to cross-transcription, misspellings and borrowing of words.&lt;/p&gt;

&lt;p&gt;One of the issues for transcription of code-mixed speech is transliteration. For example, say the actual transcripts contain first two words of a sentence in Devanagari and the last 2 in Roman but the transcripts from ASR contain only Roman script. Then unless we have transliteration, we can’t get accurate WER scores. This issue is further compounded by non-uniform transliteration standards, across both the training data and the eval data especially since languages often borrow words from other languages.&lt;/p&gt;

&lt;p&gt;However, there are no standard metrics that exist as of now. Below two metrics that measure different aspects are discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cse.iitb.ac.in/~pjyothi/files/IS19.pdf&quot;&gt;&lt;strong&gt;CM-WER&lt;/strong&gt;&lt;/a&gt; : If there are M words on both sides of switch points across all reference transcriptions and N edits in the ASR hypotheses corresponding to words surrounding the switch points in the references, then \(\text{CM-WER} = \frac{N}{M}\) . This metric provides an estimate of how accurately the system predicts words at switch points.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Brij-Srivastava-2/publication/327388676_Homophone_Identification_and_Merging_for_Code-switched_Speech_Recognition/links/5be04751299bf1124fbbf419/Homophone-Identification-and-Merging-for-Code-switched-Speech-Recognition.pdf&quot;&gt;&lt;strong&gt;poWER&lt;/strong&gt;&lt;/a&gt; : Prononciation Optimised WER. It is defined as the Levenshtein distance between the pronunciation optimized hypothesis (H) and reference (R) sentence, normalised by the number of words in the reference sentence.&lt;/p&gt;

\[\text{poWER} = \text{LevDist}(f(H), f(R))/N\]

&lt;p&gt;Here, \(f\) = grapheme-to-phoneme (g2p) conversion of each word in the sentence&lt;/p&gt;

&lt;p&gt;So, for example, the following below sentences would have a poWER of zero.&lt;/p&gt;

&lt;p&gt;HYP : रूम service आपको कै सी लग&lt;br /&gt;
REF : room service आपको कै सी लग&lt;/p&gt;

&lt;h2 id=&quot;tts&quot;&gt;TTS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Degradation MOS :&lt;/strong&gt; Annotators listen to a sample audio which is considered to have no degradation. The next audio is rated in comparison to that, in terms of degradation, out of 5. A score of 5 means no degradation, a score of 1 means the highest degradation. While testing a code-mixed TTS, it one can use degradation score of monolingual sentences generated from Multi-lingual TTS as compared to Mono-lingual TTS.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The above measures can be used to specify different patterns of code switching which have been termed as “&lt;a href=&quot;http://grlmc.wdfiles.com/local--files/slsp-2013/Day2Session2_03Heike_CS-attitude_dependent_LM.pdf&quot;&gt;code-switching attitudes&lt;/a&gt;”. These can be thought of as aspects of personality and are correlated with regional identities and other trait properties.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have any questions please reach out to &lt;em&gt;swaraj@skit.ai&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/L16-1292.pdf&quot;&gt;Comparing the Level of Code-Switching in Corpora&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.727.9087&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;On measuring the complexity of code-mixing&lt;/a&gt;
&lt;a href=&quot;http://amitavadas.com/Social_India/SOCIAL_INDIA_2014_PROCEEDING.pdf#page=6&quot;&gt;&lt;/a&gt;- &lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;Metrics for modeling code-switching across corpora&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/W16-58.pdf#page=24&quot;&gt;Simple tools for exploring variation in code-switching for linguists&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/physics/0610233.pdf&quot;&gt;Burstiness and memory in complex systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="code-mixing" /><category term="tts" /><category term="asr" /><summary type="html">We at skit, recently concluded a seminar series on code-mixing, where we covered research papers that looked at approaches to deal with code-mixed data across ASR, TTS and SLU related services. Code-mixing is a common phenomena in the Indian subcontinent. Code-mixing refers to the situation when two or more languages are present in an utterance or a corpus. It is important to formulate, discuss and deliberate on metrics because, it is improvement on metrics across benchmark datasets that spur continuous research in a field.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/ornament.jpg" /><media:content medium="image" url="/assets/images/ornament.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Normalizing Flows - Part 2</title><link href="/normalizing-flows-part-2/" rel="alternate" type="text/html" title="Normalizing Flows - Part 2" /><published>2021-05-08T00:00:00+00:00</published><updated>2021-05-08T00:00:00+00:00</updated><id>/normalizing-flows-part-2</id><content type="html" xml:base="/normalizing-flows-part-2/">&lt;p&gt;In &lt;a href=&quot;https://tech.skit.ai/normalizing-flows/&quot;&gt;Part-1&lt;/a&gt;, we introduced the concept of normalizing flows. Here, we discuss the different types of normalizing flows. In most blogs that discuss Normalizing Flows, several concepts related to autoregressive flows and residual flows aren’t discussed very clearly. We hope to simplify the explanations of a relatively theoretical topic and make them accessible.&lt;/p&gt;

&lt;h1 id=&quot;topics-covered&quot;&gt;Topics Covered:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Element-wise Flows&lt;/li&gt;
  &lt;li&gt;Linear Flows&lt;/li&gt;
  &lt;li&gt;Planar and Radial Flows&lt;/li&gt;
  &lt;li&gt;Coupling Flows&lt;/li&gt;
  &lt;li&gt;Autoregressive Flows
    &lt;ul&gt;
      &lt;li&gt;Masked Autoregressive Flows&lt;/li&gt;
      &lt;li&gt;Inverse Autoregressive Flows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Residual Flows&lt;/li&gt;
  &lt;li&gt;Convolutions and NFs&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;element-wise-flows&quot;&gt;Element-wise Flows&lt;/h1&gt;

&lt;p&gt;This is the simplest normalizing flow. In this case, the bijective function \(T\) can be broken into individual components i.e.  \(T(x_1, x_2...x_d) = (h_1(x_1), h_2(x_2)...h_d(x_d))\) s.t. every \(h_i\) is a transform from \(R \rightarrow R\). Each dimension has its own bijective function. However, these simple flows don’t capture any dependency between dimensions. Some activation functions like Parametric Leaky RELU (bijective) can be considered element-wise NFs.&lt;/p&gt;

&lt;h1 id=&quot;linear-flows&quot;&gt;Linear Flows&lt;/h1&gt;

&lt;p&gt;Linear flows are able to capture correlations between dimensions and are of the form : \(T(x) = Ax+b\) , where \(A \in R^{D*D}\) and \(b \in R^D\) where \(D\) is the dimensionality of the input. For the transform T to be invertible, only the matrix A needs to be invertible. When matrix A is a non-diagonal matrix, it captures correlation between different dimensions.&lt;/p&gt;

&lt;p&gt;Linear flows are limited in their expressive power. Starting out with a Gaussian \(z \sim N(\mu, \sigma)\), we simply arrive at another Gaussian \(z' = N(A*\mu+b, A^T*\sigma*A)\), similarly for other distributions.&lt;/p&gt;

&lt;p&gt;However, they serve as important building blocks for NFs. In terms of computational complexity, computing the determinant and inverse are both of order \(O(D^3)\) where \(D\) is the dimensionality of matrix A. To make the calculation more efficient, there need to be some restrictions on A.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Possible restrictions on A:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A is a diagonal matrix: this reduces computation time for the calculation of the det. and inverse to \(O(D)\). However, this is equivalent to element-wise bijections.&lt;/li&gt;
  &lt;li&gt;A is a triangular matrix: It is more expressive than a diagonal matrix. The computation time for calculation of the determinant is \(O(D)\) and for the inverse is \(O(D^2)\).&lt;/li&gt;
  &lt;li&gt;A is an orthogonal matrix: Inverse of an orthogonal matrix is simply its transpose and the determinant is either \(\pm 1\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some other methods that use &lt;a href=&quot;https://papers.nips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html&quot;&gt;LU factorisations&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1901.11137&quot;&gt;QR decomposition&lt;/a&gt; which essentially exploit matrix properties to make the computation of inverse and determinant efficient and at the same time allow A to be more expressive.&lt;/p&gt;

&lt;h1 id=&quot;planar-and-radial-flows&quot;&gt;Planar and Radial Flows&lt;/h1&gt;

&lt;p&gt;They were first introduced in &lt;a href=&quot;https://arxiv.org/abs/1505.05770&quot;&gt;Rezende and Mohamed [2015]&lt;/a&gt;, but aren’t widely used in practice and therefore not covered in details here. Planar flows expand/contract the distribution around a certain direction (specified by a plane) and radial flows modify the distribution around specific points. The effect of the flows can be seen below. The left half shows the effect of planar NFs on an initial distribution of a Gaussian and uniform distribution, and the right half shows the effect of a radial flow.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/planar_radial_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1 : Examples of planar and radial flows on initial distributions like Gaussian and uniform distributions.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;coupling-flows&quot;&gt;Coupling Flows&lt;/h1&gt;

&lt;p&gt;Coupling flows are highly expressive and widely used flow architectures. They can either have linear or non-linear invertible transformations.&lt;/p&gt;

&lt;p&gt;Coupling flows are defined by two sets of mappings, an identity map and a coupling function \(h\). Consider a disjoint partition of the input \(x \in \mathbb{R}\) into two subspaces: \((x^A, x^B)\) \(\in\) \(\mathbb{R}^d \times \mathbb{R}^{D-d}\).  A bijective differentiable coupling function is defined as, \(h(\cdot ;\theta)\) : \(\mathbb{R}^d \rightarrow \mathbb{R}^{D}\) that is parameterized by \(\theta\).  The equations for both the mappings are shown below.&lt;/p&gt;

&lt;p&gt;\(y^A\) =  \(h(x^A, \theta(x^B))\)   &lt;br /&gt;
\(y^{B}\) = \(x^B\)&lt;/p&gt;

&lt;p&gt;The bijection \(h\) is called a &lt;strong&gt;coupling function&lt;/strong&gt;, and the resulting function \(g\) that consists of both the mappings is called a &lt;strong&gt;coupling flow&lt;/strong&gt;. Coupling flow is invertible if and only if \(h\) is invertible and has an inverse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let’s consider an example :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let us consider an input with 4 dimensions \((x1,x2,x3,x4)\). Let \(x^A = (x1,x2)\) and \(x^B = (x3,x4)\). Let \(\theta(x^B) = x3+x4\) and let the coupling function \(h(x^A, \theta(x^B)) = \theta(x^B) * x^A\).&lt;/p&gt;

&lt;p&gt;Given this example, the coupling function \(h\) is invertible but only if \(\theta\) isn’t zero. Therefore, the invertibility requirement of \(h\) introduces a limitation on \(\theta\) as well. In this case, it can’t be defined as a simple additive function. The restriction for non-zero values needs to be taken into account.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/coupling_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2 : Example coupling flow architectures.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The figure above shows two examples of coupling flow architectures. In figure (a), \(x^{A}\) and \(x^{B}\) are the two input subspaces. The coupling function \(h\) is applied to \(x^{A}\) directly while it is parameterized on \(x^{B}\). In figure (b), there are two subsequent flows, where the size of input subspace on which the coupling function \(h\) is applied, gradually increases with each flow.&lt;/p&gt;

&lt;p&gt;There are various types of coupling functions. We discuss some of these briefly,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Additive coupling
    &lt;ul&gt;
      &lt;li&gt;This is one of the simplest form of coupling functions defined by,
  \(h(x;\theta) =\)  \(x + \theta\),     where \(\theta\) is a constant \(\in \mathbb{R}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Affine coupling
    &lt;ul&gt;
      &lt;li&gt;\(h(x;\theta) =\)  \(\theta_1x + \theta_2\),     \(\theta_1 \neq 0\),  \(\theta_2 \in \mathbb{R}\)
  Both additive and affine coupling were introduced in &lt;a href=&quot;https://arxiv.org/pdf/1410.8516.pdf&quot;&gt;NICE&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural auto-regressive flows
    &lt;ul&gt;
      &lt;li&gt;This was first introduced by &lt;a href=&quot;https://arxiv.org/pdf/1804.00779.pdf&quot;&gt;Huang et al [2018]&lt;/a&gt;. The coupling function \(h(\cdot;\theta)\) is modelled as a neural network. Every neural network can be considered as a parameterized function, the parameters being the weights. In this case, the weights of the neural network are defined by \(\theta\). To ascertain that the neural network follows bijectivity, the following proposition is applied :
        &lt;ul&gt;
          &lt;li&gt;If \(NN(\cdot)\) :  \(\mathbb{R} \rightarrow \mathbb{R}\) is a multilayer perceptron, such that all weights are positive and all activation functions are strictly monotone, then \(NN({\cdot})\) is a strictly monotone function.
  So, since the neural network is monotone, it is also invertible and hence a normalizing flow.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More types of coupling flows can be found in this &lt;a href=&quot;https://arxiv.org/pdf/1908.09257.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;autoregressive-flows&quot;&gt;Autoregressive Flows&lt;/h1&gt;

&lt;p&gt;An autoregressive flow is a type of normalizing flow where the transformations use autoregressive functions. The term &lt;strong&gt;autoregressive&lt;/strong&gt; originates from time-series models where the predictions at the current time-step are dependent on the observations from the previous time-steps.&lt;/p&gt;

&lt;p&gt;The probability distribution of an autoregressive model is given by, the equation below where the output at time-step \(i\) is conditioned on all the previous outputs.&lt;/p&gt;

\[p(x) = \prod_{i=1..D} p(x_i | x_{1:i-1})\]

&lt;p&gt;An autoregressive function can be represented as a coupling flow:&lt;/p&gt;

&lt;p&gt;Let the coupling function \(h(\cdot ;\theta)\) : \(\mathbb{R} \rightarrow \mathbb{R}\) be a bijection parameterized by \(\theta\), and let \(x_{1:t}\) be the set of inputs Then, an autoregressive model is a function \(g : \mathbb{R}^D \rightarrow \mathbb{R}^D\), in which every entry of the output \(y = g(x)\) is conditioned on the previous entries of the input:&lt;/p&gt;

\[y_t = h(x_t ; \theta_t(x_{1:t-1})\]

&lt;p&gt;The functions \(\theta_t(\cdot)\) are called &lt;strong&gt;conditioners&lt;/strong&gt;. \(\theta_1\) is a constant whereas \(\theta_2\), \(\theta_3\) … , \(\theta_t\) are arbitrary functions mapping \(\mathbb{R}_{t-1}\) to the set of all parameters. The conditioners need to be arbitrarily complex for effective transformations and hence, are usually modelled as neural networks.&lt;/p&gt;

&lt;p&gt;Since each output depends only on the previous inputs, the Jacobian matrix of an autoregressive transformation \(g\) is triangular (refer to &lt;a href=&quot;https://paper.dropbox.com/doc/Part-1-Introducing-Normalizing-Flows--BHah7N6t5K91Tds2Cz_e8oeWAQ-SMbUFvz9GWqRqcTsYW0Vm&quot;&gt;P&lt;/a&gt;&lt;a href=&quot;https://paper.dropbox.com/doc/Part-1-Introducing-Normalizing-Flows--BHah7N6t5K91Tds2Cz_e8oeWAQ-SMbUFvz9GWqRqcTsYW0Vm&quot;&gt;art 1&lt;/a&gt; for explanation). The determinant of a triangular matrix is simply a product of its diagonal entries.&lt;/p&gt;

\[\det{(Dg)} = \prod_{t=1..k} |\frac{\partial y_t}{\partial x_t}|\]

&lt;h2 id=&quot;masked-autoregressive-flows-mafs&quot;&gt;Masked Autoregressive Flows (MAFs)&lt;/h2&gt;

&lt;p&gt;The time taken to train autoregressive flow models is very high because of the need to sequentially generate outputs. Using MAFs one can parallelize the training process.&lt;/p&gt;

&lt;p&gt;MAFs were inspired by the observation that on stacking several autoregressive models, where each model had unimodal conditionals, the normalizing flow can learn multi-modal conditionals (&lt;a href=&quot;https://arxiv.org/pdf/1705.07057.pdf&quot;&gt;MAF [2018]&lt;/a&gt;). The architecture consists of &lt;strong&gt;stacked MADEs&lt;/strong&gt; &lt;strong&gt;with Gaussian conditionals&lt;/strong&gt; which are explained below.&lt;/p&gt;

&lt;p&gt;What are &lt;strong&gt;MADEs&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MADE&lt;/strong&gt; stands for Masked Auto-Encoder for Distribution Estimation (&lt;a href=&quot;https://arxiv.org/pdf/1502.03509.pdf&quot;&gt;MADE [2015]&lt;/a&gt;) are essentially auto-encoders with some modifications like masks. In a MADE, the autoregressive property is enforced by multiplying the weight matrices of the hidden layers of the auto-encoder with a binary mask. These masks ensure that the training of the sequential auto-regressive model can be done in parallel.&lt;/p&gt;

&lt;p&gt;The masking is done such that forward passes with mask are equivalent to conditioning the output only on the earlier sequences of input. This is shown in the below image. On the left, a typical auto-encoder with 2 hidden layers is shown. If one wants to represent the output as an autoregressive sequence specified by the equation below, then masks are multiplied to the weights of the auto-encoder.&lt;/p&gt;

\[p(x) = p(x_2) * p(x_3 | x_2) * p(x_1 | x_2, x_3)\]

&lt;p&gt;The order of the inputs are \(x_2, x_3, x_1\) The image on the right shows the network weights after masking. For example \(p(x_2)\) doesn’t depend on any of the other inputs since it is the first in the sequence. \(p(x_3)\) is dependent on only the nodes that depend on \(x_2\) and similarly for \(p(x_1)\) The general form for masking is discussed in the MADE paper. The MADE architecture parallelizes the sequential autoregressive computation.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/MADEs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3 : This image is taken from the MADE paper an explains the idea of masking, so as to parallelizes the sequential autoregressive computation.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;density-estimation-and-sampling&quot;&gt;Density Estimation and Sampling&lt;/h2&gt;

&lt;p&gt;There are two important concepts when studying flow architectures : density estimation and sampling. Both these passes have different computation complexity and for auto-regressive models, the computation time of one is inversely related to the other.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/MAFs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4 : Forward pass of MAFs (left) vs. Inverse pass of MAFs (right).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The diagrams above demonstrate a forward pass and an inverse pass in a MAF.
The output \(x_i's\) depends upon the input \(z_{i}\) and the scalars \(\alpha_{i}\), \(\mu_{i}\) which are computed using \(x_{1:t-1}\).  It is these scalars that define the density parameters of the distribution. This is also called a scale and shift transform. The reason why it is designed this way is that inverting \(f(x)\) does not require us to invert the scalar functions \(\alpha_{i}\) and \(\mu_{i}\).&lt;/p&gt;

&lt;p&gt;\(f^{-1}(x_{i}) = z_{i} = \frac{x_{i} - \mu_{i}}{exp(\alpha_{i})}\).&lt;/p&gt;

&lt;p&gt;MAFs can compute the density \(p(x)\) very efficiently. During the training process, all the \(x_i's\) are known. One can therefore parallelize this step using masks. Sampling on the other hand (predicting \(x_{D}\)) is slower as it requires it requires performing \(D\) sequential passes (where \(D\) is the dimensionality of \(x\)) to calculate the previous samples (\(x_{1}, x_{2} ... x_{D-1}\)) before computing \(x_{D}\).&lt;/p&gt;

&lt;h2 id=&quot;inverse-autoregressive-flows-iafs&quot;&gt;Inverse Autoregressive Flows (IAFs)&lt;/h2&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/IAFs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5 : Inverse pass of MAF (left) vs. Forward pass of IAF (right).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Since MAFs are better at training by exploiting parallel computation but slower during sampling, we modify the functions to get another flow architecture that is faster at sampling. This gives us Inverse Autoregressive flows (IAFs).&lt;/p&gt;

&lt;p&gt;The figures above show a comparison between the inverse pass of a MAF and the forward pass in an IAF.  The only difference between both the architectures is that for IAF’s the autoregression is based on the latent variables and not the predicted distribution. The scale(\(\alpha_{i}\)) and shift(\(\mu_{i}\)) quantities are computed using previous data points from the base distribution instead of the transformed distribution.&lt;/p&gt;

&lt;p&gt;IAF can generate samples efficiently with one pass through the model since all the \(z_i's\)  are known. However, the training process  is slow, since estimating the \(z_i\) requires i sequential passes to calculate all the required input variables \(z_{1}\), \(z_{2}\), .. \(z_{i-1}\).&lt;/p&gt;

&lt;p&gt;Aside : According to our understanding, during training, both the weights of the network parameters(\(\alpha, \mu\)) and \(z_i's\) are estimated i.e. the ith latent variable is itself treated as a parameter that is to be learned.&lt;/p&gt;

&lt;p&gt;One should use IAFs if fast sampling is needed, and MAFs if fast density estimation is desirable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.10433&quot;&gt;Parallel WaveNet&lt;/a&gt; [2017] which was once the state-of-art model for speech synthesis made use of a MAF and an IAF. A fully trained “teacher” network that used MAF architecture was used to train a smaller and parallel “student” network that used IAF architecture. Since the teacher used MAF architecture it could be trained fast. Once the student network, an IAF model, was trained, it could then generate all the audio samples in parallel without depending on the previously generated audio samples and with no loss in audio quality.&lt;/p&gt;

&lt;h1 id=&quot;residual-flows&quot;&gt;Residual Flows&lt;/h1&gt;

&lt;p&gt;During training, deep networks find it difficult to reach a convergence point due to the vanishing/exploding gradient. In such cases, adding more layers to the network only results in a higher training error. Residual Networks or &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;ResNets&lt;/a&gt; were introduced to solve this problem. ResNets consist of skip-connection blocks.&lt;/p&gt;

&lt;p&gt;A residual block is shown below. The output of the residual block is represented by the equation below, where \(F(x)\) represented the pass through the neural layers.&lt;/p&gt;

\[M(x) = F(x) + x\]

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/residual_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 6 : A residual block.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;How does this help?
Skip connections in a deep neural network allow the back-propagation signal to reach the initial layers of the neural network thereby solving the vanishing gradient problem.
Instead of treating the number of layers as an important hyper-parameter to tune, by adding skip connections to our network, we are allowing the network to skip training for the layers that are not useful and do not add value to overall accuracy. In a way, skip connections make our neural networks dynamic, so that they may optimally tune the number of layers during training.&lt;/p&gt;

&lt;p&gt;Residual flows in their general form aren’t invertible and can only be used in flow architectures after applying some constraints(first introduced in &lt;a href=&quot;https://arxiv.org/pdf/1906.02735.pdf&quot;&gt;Chen et al. [2019]&lt;/a&gt;). This is discussed next.&lt;/p&gt;

&lt;p&gt;A Lipschitz condition is necessary to enforce invertibility in residual flows.&lt;/p&gt;

&lt;p&gt;Lipchitz condition :
A real valued function : \(f : \mathbb{R} \rightarrow \mathbb{R}\), is Lipchitz continuous if there exists a positive real constant \(K\) such that, for all real \(x_1\) and \(x_2\), \(|f(x_1) - f(x_2)| \leq  K| x_1 - x_2 |\)&lt;/p&gt;

&lt;p&gt;If \(K = 1\) the function is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Short_map&quot;&gt;&lt;strong&gt;short map&lt;/strong&gt;&lt;/a&gt;, and if \(0 \leq K &amp;lt; 1\) the function is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Contraction_mapping&quot;&gt;&lt;strong&gt;contraction&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invertible Residual Flows:&lt;/strong&gt;
Given a residual flow of the form, \(F(x)=x+g(x)\), if the mapping \(g(x)\) has a lipschitz constant \(K\), \(0 \leq K &amp;lt; 1\) then the residual flow is invertible. We discuss a short proof of this next.&lt;/p&gt;

&lt;p&gt;For the residual flow to be invertible, proving that \(F(x)\) is monotonically increasing is sufficient i.e. we need to prove \(F(x+\delta) - F(x) &amp;gt;= 0\)&lt;/p&gt;

\[F(x+\delta) - F(x) = x+\delta + g(x+\delta) - (x - g(x)) = \delta + g(x+\delta) - g(x)\]

&lt;p&gt;From the Lipschitz condition for \(g\), we have \(|g(x+\delta) - g(x)| \le |\delta|\), since \(0 \leq K &amp;lt; 1\).
Therefore, \(F(x+\delta) - F(x)\) is always &amp;gt;= 0 and hence monotonically increasing.&lt;/p&gt;

&lt;p&gt;The addition of invertible residual blocks greatly enhances the class of neural network flow models and also ensures we can train deeper models without the fear of vanishing gradient.&lt;/p&gt;

&lt;h1 id=&quot;convolutions-and-nfs&quot;&gt;Convolutions and NFs&lt;/h1&gt;

&lt;p&gt;Convolutions are one of the most common operations used in the design of DNNs, however, the computation of their inverse and determinant is non-obvious. Since they aren’t invertible they can’t be used in the family of NF flows. However, recently [Hoogeboom et al. &lt;a href=&quot;https://arxiv.org/abs/1901.11137&quot;&gt;2019&lt;/a&gt;] defined invertible \(d*d\) convolutions and designed an architecture using stacked masked autoregressive convolutions. This is explained next.&lt;/p&gt;

&lt;p&gt;In the figure below, on the right, the input x is shown along with the kernel matrix w. The convolution operation between these two can be represented as a matrix multiplication of the form \(A*x\)  shown on the right.  The matrix A is non-invertible here, however, if we consider \(f = g = h = i = 0\), then, we get a triangular matrix that can represent a normalizing flow.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/convolutions_nfs_1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 7 : A convolution represented as a matrix multiplication.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;One of the issues of using a filter where \(f = g = h = i = 0\) is that the receptive field is narrow. However, one can rotate the filter and combine two filters to get different receptive fields as shown on the left in the image below. These can then which can be stacked to capture more expressive combinations as shown on the right. These are called “expressive convolutions” in the paper. The grey and orange filters are rotations of the same filter that result in a triangular matrix. They can be stacked to get different receptive fields shown in blue. These blue receptive fields can be stacked further to arrive at more expressive transforms that have the desired receptive field.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/bigger_kernels.png&quot; /&gt;
  &lt;figcaption&gt;Fig 8 : The part in grey is an example of an individual kernel that would result in a triangular matrix. The grey and orange matrices are all rotations of the standard matrix and the blue matrix shows the receptive field when the grey and orange filters are combined..&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;This concludes our discussion of normalizing flow architectures. Incase of any doubts/questions please reach out to us at &lt;em&gt;kriti@skit.ai&lt;/em&gt; or &lt;em&gt;swaraj@skit.ai&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.09257.pdf&quot;&gt;Normalizing Flows IEEE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.07057.pdf&quot;&gt;MAFs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://akosiorek.github.io/ml/2018/04/03/norm_flows.html&quot;&gt;Adam Kosiorek’s Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&quot;&gt;Lilian Weng’s Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs236.stanford.edu/assets/slides/cs236_lecture8.pdf&quot;&gt;Stanford Lecture&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="normalizing-flows" /><category term="tts" /><summary type="html">In Part-1, we introduced the concept of normalizing flows. Here, we discuss the different types of normalizing flows. In most blogs that discuss Normalizing Flows, several concepts related to autoregressive flows and residual flows aren’t discussed very clearly. We hope to simplify the explanations of a relatively theoretical topic and make them accessible.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/normalising_flow.jpg" /><media:content medium="image" url="/assets/images/normalising_flow.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What’s New in Kaldi-Serve 1.0</title><link href="/whats-new-kaldi-serve-10/" rel="alternate" type="text/html" title="What’s New in Kaldi-Serve 1.0" /><published>2021-03-25T00:00:00+00:00</published><updated>2021-03-25T00:00:00+00:00</updated><id>/whats-new-kaldi-serve-10</id><content type="html" xml:base="/whats-new-kaldi-serve-10/">&lt;p&gt;&lt;a href=&quot;https://github.com/skit-ai/kaldi-serve&quot;&gt;Kaldi-Serve&lt;/a&gt; is our open source high performance Speech Recognition server framework capable of serving &lt;a href=&quot;https://github.com/kaldi-asr/kaldi&quot;&gt;Kaldi ASR&lt;/a&gt; models in production environments for real-time inference, and it’s got an upgrade!&lt;/p&gt;

&lt;h2 id=&quot;whats-changed&quot;&gt;What’s Changed?&lt;/h2&gt;

&lt;p&gt;Originally designed as a standalone application, kaldi-serve had some issues mostly pertaining to it’s usability or extensibility for custom use cases thus greatly reducing the core’s potential to just a handful of rigid situations and dependencies.&lt;/p&gt;

&lt;p&gt;After one too many requests for changes in the architecture to better fit the various production needs of our customers, we realised it’s time for a rewrite of the framework that allows us to better utilise the core’s scope and extend it’s capabilities in the form of general API consumable in most use cases.&lt;/p&gt;

&lt;h3 id=&quot;library&quot;&gt;Library&lt;/h3&gt;

&lt;p&gt;Kaldi-Serve is now a general extensible library with all the functionality necessary to serve Kaldi ASR models in production with any server framework of your choice, for ex. gRPC, Open CGI, HTTP, etc. or even import it in your own custom offline application.&lt;/p&gt;

&lt;p&gt;The earlier standalone gRPC server is now and application that extends the core kaldi-serve library and only contains the frontend gRPC methods that call the general API. We’ll talk about how you can extend the library in your own applications below.&lt;/p&gt;

&lt;h3 id=&quot;python-port&quot;&gt;Python Port&lt;/h3&gt;

&lt;p&gt;We also made it easier to use kaldi-serve by porting the core library to python which can be easily installed as a package via pip. The transcription interface is much simpler and faster to get up and running.&lt;/p&gt;

&lt;p&gt;Below is sample code snippet for transcribing a wav file with your Kaldi Chain model using the kaldiserve python package:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;kaldiserve&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ChainModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse_model_specs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_decoding&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# chain model contains all const components to be shared across multiple threads
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ChainModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_model_specs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model-spec.toml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initialize a decoder that references the chain model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# read audio file as bytes
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sample.wav&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;audio_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getvalue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_decoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# decode the audio
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode_wav_audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;audio_bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# get the n-best alternatives
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;alts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_decoded_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;More Features Coming Soon&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Ontology&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU Inference&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MACE Integration&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Prabhsimran Singh</name></author><category term="Machine Learning" /><category term="speech recognition" /><category term="framework" /><category term="new release" /><summary type="html">Kaldi-Serve is our open source high performance Speech Recognition server framework capable of serving Kaldi ASR models in production environments for real-time inference, and it’s got an upgrade!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our new Tech blog</title><link href="/new-blog/" rel="alternate" type="text/html" title="Our new Tech blog" /><published>2021-02-28T00:00:00+00:00</published><updated>2021-02-28T00:00:00+00:00</updated><id>/new-blog</id><content type="html" xml:base="/new-blog/">&lt;p&gt;We are merging past webpages of our
&lt;a href=&quot;https://skit-ai.github.io/engineering/&quot;&gt;Engineering&lt;/a&gt; and
&lt;a href=&quot;https://skit-ai.github.io/ml/&quot;&gt;ML&lt;/a&gt; team in this new, central, Skit
Tech page. From here on, this is going to be the main source of updates on
speech technology work from &lt;a href=&quot;https://skit.ai&quot;&gt;Skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stay tuned to our &lt;a href=&quot;/feed.xml&quot;&gt;rss feed&lt;/a&gt; for updates.&lt;/p&gt;</content><author><name>Abhinav Tushar</name></author><category term="Engineering" /><category term="Machine Learning" /><category term="sticky" /><summary type="html">We are merging past webpages of our Engineering and ML team in this new, central, Skit Tech page. From here on, this is going to be the main source of updates on speech technology work from Skit.ai.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>