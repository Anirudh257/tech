<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-05-05T08:54:04+00:00</updated><id>/feed.xml</id><title type="html">Skit Tech</title><subtitle>Speech Technology from Skit</subtitle><entry><title type="html">TTS Enhancement</title><link href="/woc/" rel="alternate" type="text/html" title="TTS Enhancement" /><published>2022-03-09T00:00:00+00:00</published><updated>2022-03-09T00:00:00+00:00</updated><id>/woc</id><content type="html" xml:base="/woc/">&lt;h1 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h1&gt;

&lt;p&gt;Text-To-Speech (TTS) systems of Skit, as well as TTS systems in general, have a tendency to mix some ambient noise along with the speech it outputs. This aim of this research project was to remove that noise and quantify how well the noise has been removed using standard metrics.&lt;/p&gt;

&lt;p&gt;Listen to the clean speech sample here for reference-&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws4 = WaveSurfer.create({
           container: '#waveform-4',
           backend: 'MediaElement'
         });
         ws4.load('/assets/audios/posts/woc/sp01.wav');

         ws4.on('audioprocess', function () {
           let progressText = ws4.getCurrentTime().toFixed(2) + ' / ' + ws4.getDuration().toFixed(2)
           document.getElementById('player-progress-4').innerHTML = progressText
         });

         ws4.on('ready', function () {
           let progressText = ws4.getCurrentTime().toFixed(2) + ' / ' + ws4.getDuration().toFixed(2)
           document.getElementById('player-progress-4').innerHTML = progressText
         });

         ws4.on('finish', function () {
           let button = $('#controls-4 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-4').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws4.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws4.skipBackward()
                 break
               case 'forward':
                 ws4.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-4&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-4&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-4&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;and the distorted sample-&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws5 = WaveSurfer.create({
           container: '#waveform-5',
           backend: 'MediaElement'
         });
         ws5.load('/assets/audios/posts/woc/sp01_car_sn0.wav');

         ws5.on('audioprocess', function () {
           let progressText = ws5.getCurrentTime().toFixed(2) + ' / ' + ws5.getDuration().toFixed(2)
           document.getElementById('player-progress-5').innerHTML = progressText
         });

         ws5.on('ready', function () {
           let progressText = ws5.getCurrentTime().toFixed(2) + ' / ' + ws5.getDuration().toFixed(2)
           document.getElementById('player-progress-5').innerHTML = progressText
         });

         ws5.on('finish', function () {
           let button = $('#controls-5 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-5').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws5.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws5.skipBackward()
                 break
               case 'forward':
                 ws5.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-5&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-5&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-5&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Speech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. In this project, we mainly focused on the signal processing aspects of noise reduction. Signal processing techniques can be further divided into 3 more categories-&lt;/p&gt;

&lt;h2 id=&quot;spectral-subtractive-algorithms&quot;&gt;Spectral Subtractive algorithms&lt;/h2&gt;

&lt;p&gt;The main principle is as follows- assuming additive noise, one can obtain an estimate of the clean signal spectrum by subtracting an estimate of the noise spectrum from the noisy speech spectrum. The noise spectrum can be estimated and updated, during periods when the signal is absent. The assumption made is that noise is stationary or a slowly varying process and that the noise spectrum does not change significantly between the updating periods. The enhanced signal is obtained by computing the IDFT of the estimated signal spectrum using the phase of the noisy signal.&lt;/p&gt;

&lt;h2 id=&quot;statistical-model-based-algorithms&quot;&gt;Statistical Model based algorithms&lt;/h2&gt;

&lt;p&gt;Given a set of measurements that depend on an unknown parameter, we wish to find a nonlinear estimator of the parameter of interest. These measurements correspond to the set of DFT coefficients of the noisy signal and the parameters of interest are the set of DFT coefficients of the clean signal. Various techniques from estimation theory which include maximum-likelihood (ML) estimators and the Bayesian estimators like MMSE and MAP estimators are used for this purpose.&lt;/p&gt;

&lt;h2 id=&quot;subspace-algorithms&quot;&gt;Subspace algorithms&lt;/h2&gt;

&lt;p&gt;These algorithms are based on the principle that the clean signal might be confined to a subspace of the noisy Euclidean space. Given a method for decomposing the vector space of the noisy signal into a direct sum of the subspace that is occupied by the clean signal and a subspace occupied by the noise signal, for example SVD, we could estimate the clean signal simply by nulling the component of the noisy vector residing in the noisy subspace.&lt;/p&gt;

&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;
&lt;h2 id=&quot;filters&quot;&gt;Filters&lt;/h2&gt;

&lt;p&gt;Speech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. We hypothesised that signal processing techniques would be suitable for task and tested them out. We implemented some of the popular speech enhancement methods which were suitably modified to tackle the problem at hand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wiener Filter&lt;/strong&gt;&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Block Diagram of Wiener Filter&quot; src=&quot;/assets/images/posts/woc/wiener.png&quot; /&gt;
  &lt;figcaption&gt;Block Diagram of Wiener Filter&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The input signal w[n] goes through a linear and time-invariant system to produce an output signal x[n]. We are to design the system in such a way that the output signal, x[n], is as close to the desired signal, s[n], as possible. This can be done by computing the estimation error, e[n], and making it as small as possible. The optimal filter that minimizes the estimation error is called the &lt;em&gt;Wiener filter.&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws6 = WaveSurfer.create({
           container: '#waveform-6',
           backend: 'MediaElement'
         });
         ws6.load('/assets/audios/posts/woc/wiener_filtered_sp01_car_sn0.wav');

         ws6.on('audioprocess', function () {
           let progressText = ws6.getCurrentTime().toFixed(2) + ' / ' + ws6.getDuration().toFixed(2)
           document.getElementById('player-progress-6').innerHTML = progressText
         });

         ws6.on('ready', function () {
           let progressText = ws6.getCurrentTime().toFixed(2) + ' / ' + ws6.getDuration().toFixed(2)
           document.getElementById('player-progress-6').innerHTML = progressText
         });

         ws6.on('finish', function () {
           let button = $('#controls-6 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-6').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws6.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws6.skipBackward()
                 break
               case 'forward':
                 ws6.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-6&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-6&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-6&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;MMSE and MMSE Log Filter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These fall under the umbrella of Bayesian estimation techniques. We saw above that the Wiener estimator can be derived by minimizing the error between a linear model of the clean spectrum and the true spectrum. The Wiener estimator is considered to be the optimal (in the mean-square-error sense) complex spectral estimator, but is not the optimal spectral magnitude estimator. Acknowledging the importance of the short-time spectral amplitude (STSA) on speech intelligibility and quality, several authors have proposed optimal methods for obtaining the spectral amplitudes from noisy observations. In particular, we are looking for sought that minimized the mean-square error between the estimated and true magnitudes:&lt;/p&gt;

\[e = E{ (\hat{X_k} - X_k)^2 }\]

&lt;p&gt;where \(\hat{X_k}\) is the estimate spectral magnitude at frequency \(\omega_k\) and \(X_k\) is the true magnitude of the clean signal.&lt;/p&gt;

&lt;p&gt;The MMSE Log is an improvement upon the MMSE estimator. Although a metric
based on the squared error of the magnitude spectra is mathematically tractable, it may not be subjectively meaningful. It has been suggested that a metric based on the squared error of the log-magnitude spectra may be more suitable for speech processing. So we minimize :&lt;/p&gt;

\[e = E \{ (log \hat X_k - log X_k)^2 \}\]

&lt;p&gt;and we notice a significant improvement in the results compared to the original MMSE estimator.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws7 = WaveSurfer.create({
           container: '#waveform-7',
           backend: 'MediaElement'
         });
         ws7.load('/assets/audios/posts/woc/mmse_filtered_sp01_car_sn0.wav');

         ws7.on('audioprocess', function () {
           let progressText = ws7.getCurrentTime().toFixed(2) + ' / ' + ws7.getDuration().toFixed(2)
           document.getElementById('player-progress-7').innerHTML = progressText
         });

         ws7.on('ready', function () {
           let progressText = ws7.getCurrentTime().toFixed(2) + ' / ' + ws7.getDuration().toFixed(2)
           document.getElementById('player-progress-7').innerHTML = progressText
         });

         ws7.on('finish', function () {
           let button = $('#controls-7 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-7').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws7.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws7.skipBackward()
                 break
               case 'forward':
                 ws7.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-7&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-7&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-7&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws8 = WaveSurfer.create({
           container: '#waveform-8',
           backend: 'MediaElement'
         });
         ws8.load('/assets/audios/posts/woc/mmse_log_filtered_sp01_car_sn0.wav');

         ws8.on('audioprocess', function () {
           let progressText = ws8.getCurrentTime().toFixed(2) + ' / ' + ws8.getDuration().toFixed(2)
           document.getElementById('player-progress-8').innerHTML = progressText
         });

         ws8.on('ready', function () {
           let progressText = ws8.getCurrentTime().toFixed(2) + ' / ' + ws8.getDuration().toFixed(2)
           document.getElementById('player-progress-8').innerHTML = progressText
         });

         ws8.on('finish', function () {
           let button = $('#controls-8 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-8').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws8.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws8.skipBackward()
                 break
               case 'forward':
                 ws8.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-8&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-8&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-8&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Berouti’s Oversubstraction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This method consists of subtracting an overestimate of the noise power spectrum, while preventing the resultant spectral components from going below a preset minimum value (spectral floor).&lt;/p&gt;

\[\hat X(\omega)=\begin{cases} |Y(\omega)|^2 - \alpha |\hat D(\omega)|^2&amp;amp; \text{if } |Y(\omega)|^2 \geq (\alpha + \beta) |D(\omega)|^2 \\ \beta |\hat D(\omega)|^2 &amp;amp; \text{else} \end{cases}\]

&lt;p&gt;where \(\alpha (\geq 1)\) is the oversubtraction factor and \(0 \leq \beta \leq 1\) is the spectral floor parameter.&lt;/p&gt;

&lt;p&gt;When we subtract the estimate of the noise spectrum from the noisy speech
spectrum, there remain peaks in the spectrum. Some of those peaks are broadband (encompassing a wide range of frequencies) whereas others are narrow band, appearing as spikes in the spectrum. By oversubtracting the noise spectrum, that is, by using \(\alpha\), we can reduce the amplitude of the broadband peaks and, in some cases, eliminate them altogether. This by itself, however, is not sufficient because the deep valleys surrounding the peaks still remain in the spectrum. For that reason, spectral flooring is used to “fill in” the spectral valleys and possibly mask the remaining peaks by the neighbouring spectral components of comparable value. The valleys between peaks are no longer deep when \(\beta &amp;gt; 0\) compared to when \(\beta = 0\).&lt;/p&gt;

&lt;p&gt;The parameter \(\beta\) controls the amount of remaining residual noise
and the amount of perceived musical noise. If the spectral floor parameter \(\beta\) is too
large, then the residual noise will be audible but the musical noise will not be perceptible. Conversely, if \(\beta\) is too small, the musical noise will become annoying but the residual noise will be markedly reduced.&lt;/p&gt;

&lt;p&gt;The parameter \(\alpha\) affects the amount of speech spectral distortion caused by
the subtraction. If \(\alpha\) is too large, then the resulting signal will be severely distorted to the point that intelligibility may suffer.&lt;/p&gt;

\[\alpha = \alpha_0 - \frac{3}{20}  \textit{SNR} : \text{ for} -5 \leq \textit{SNR} \leq -20\]

&lt;p&gt;where \(\alpha_0\) is the desired value of  \(\alpha\) at 0 dB SNR and the \(\textit{SNR}\) is the short term SNR estimated at each frame.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws9 = WaveSurfer.create({
           container: '#waveform-9',
           backend: 'MediaElement'
         });
         ws9.load('/assets/audios/posts/woc/ss_filtered_sp01_car_sn0.wav');

         ws9.on('audioprocess', function () {
           let progressText = ws9.getCurrentTime().toFixed(2) + ' / ' + ws9.getDuration().toFixed(2)
           document.getElementById('player-progress-9').innerHTML = progressText
         });

         ws9.on('ready', function () {
           let progressText = ws9.getCurrentTime().toFixed(2) + ' / ' + ws9.getDuration().toFixed(2)
           document.getElementById('player-progress-9').innerHTML = progressText
         });

         ws9.on('finish', function () {
           let button = $('#controls-9 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-9').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws9.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws9.skipBackward()
                 break
               case 'forward':
                 ws9.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-9&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-9&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-9&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;The Kalman filter is a general recursive state estimation technique which is modified to work on the speech denoising problem.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws10 = WaveSurfer.create({
           container: '#waveform-10',
           backend: 'MediaElement'
         });
         ws10.load('/assets/audios/posts/woc/kalman_filtered_sp01_car_sn0.wav');

         ws10.on('audioprocess', function () {
           let progressText = ws10.getCurrentTime().toFixed(2) + ' / ' + ws10.getDuration().toFixed(2)
           document.getElementById('player-progress-10').innerHTML = progressText
         });

         ws10.on('ready', function () {
           let progressText = ws10.getCurrentTime().toFixed(2) + ' / ' + ws10.getDuration().toFixed(2)
           document.getElementById('player-progress-10').innerHTML = progressText
         });

         ws10.on('finish', function () {
           let button = $('#controls-10 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-10').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws10.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws10.skipBackward()
                 break
               case 'forward':
                 ws10.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-10&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-10&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-10&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;h2 id=&quot;intelligibility-metrics&quot;&gt;Intelligibility Metrics&lt;/h2&gt;

&lt;p&gt;Along with techniques for speech enhancement, it is important to quantify the degree of enhancement which our methods provide. For this, we tested several metrics as discussed below-&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perceptual Evaluation of Speech Quality (PESQ)&lt;/strong&gt; is a full-reference algorithm and analyzes the speech signal sample-by-sample after a temporal alignment of corresponding excerpts of reference and test signal. PESQ results essentially model mean opinion score (MOS) that cover a scale from 1 (bad) to 5 (excellent).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Short-Time Objective Intelligibility (STOI)&lt;/strong&gt; is an objective metric showing high correlation (\(\rho=0.95\)) with the intelligibility of both noisy, and TF-weighted noisy speech&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gross Pitch Error (GPE)&lt;/strong&gt; is the proportion of frames, considered voiced by both pitch tracker and ground truth, for which the relative pitch error is higher than a certain threshold, which is usually set to 20%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Voicing Error Decision (VED)&lt;/strong&gt; is the proportion of frames for which an incorrect voiced/unvoiced decision is made.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;F0 Frame Error (FFE)&lt;/strong&gt; is the proportion of frames for which an error (either according to the GPE or the VDE criterion) is made. FFE can be seen as a single measure for assessing the overall performance of a pitch tracker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mel Cepstral Distortion (MCD)&lt;/strong&gt; is a measure of how different two sequences of mel cepstra are. It is used in assessing the quality of parametric speech synthesis systems, including statistical parametric speech synthesis systems, the idea being that the smaller the MCD between synthesized and natural mel cepstral sequences, the closer the synthetic speech is to reproducing natural speech.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We apply our methods on 2 different datasets: First on the public &lt;a href=&quot;https://ecs.utdallas.edu/loizou/speech/noizeus/&quot;&gt;NOIZEUS&lt;/a&gt; dataset and next on a dataset created by the in-house TTS systems of Skit. The results are quite satisfactory when we apply our methods on the NOIZEUS dataset and we found that the Wiener Filter and the Kalman Filters perform the best outperforming one another for different signal-to-noise ratios (SNR).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;MCD metric&quot; src=&quot;/assets/images/posts/woc/res1.png&quot; /&gt;
  &lt;figcaption&gt;Effect of filters wrt MCD metric&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;PESQ metric&quot; src=&quot;/assets/images/posts/woc/res2.png&quot; /&gt;
  &lt;figcaption&gt;Effect of filters wrt PESQ metric&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;However they do not perform as well as we want on the TTS dataset. In fact, we observe that our models adversely affecting the input speech. There can be various reasons attributed to this, the primary one being that speech denoising of real life data and TTS Systems are quite different, since both have different noise types. Real life noise is either additive and can be subtracted by noise estimation or can be decomposed as a direct sum of a clean subspace and a pure noise subspace. But the noise in TTS systems are much more subtle and the noise cannot be modelled to be simply additive. Here the noise is generated along with the speech. Hence most of the traditional filters which although work well for real life noise separation, do not work well for this use case. This is where we planned to resort to deep learning models like the Facebook denoiser and SeGAN.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;You can find more information on the &lt;a href=&quot;https://github.com/skit-ai/woc-tts-enhancement&quot;&gt;Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/224738211_Enhancement_of_speech_corrupted_by_acoustic_noise&quot;&gt;Berouti’s Spectral Subtraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ccrma.stanford.edu/~orchi/Documents/thesis_KF.pdf&quot;&gt;Kalman Filter for Speech Enhancement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.perlego.com/book/2193587/speech-enhancement-theory-and-practice-second-edition-pdf&quot;&gt;Speech Enhancement by Philipos C. Loizou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-00923967/document&quot;&gt;A comparative study of pitch extraction algorithms on a large variety of singing sounds&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/941023&quot;&gt;PESQ&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Author: Ananyapam De, a final year student at IISER Kolkata, majoring in Statistics, while minoring in the Computational Sciences.&lt;/p&gt;</content><author><name></name></author><category term="Machine Learning" /><category term="TTS" /><category term="speech-enhancement" /><summary type="html">Problem Statement</summary></entry><entry><title type="html">Turn Taking Dynamics in Voice Bots</title><link href="/Turn_Taking_Dynamics_in_Voice_Bots/" rel="alternate" type="text/html" title="Turn Taking Dynamics in Voice Bots" /><published>2022-03-07T00:00:00+00:00</published><updated>2022-03-07T00:00:00+00:00</updated><id>/Turn_Taking_Dynamics_in_Voice_Bots</id><content type="html" xml:base="/Turn_Taking_Dynamics_in_Voice_Bots/">&lt;p&gt;One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette.  Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.&lt;/p&gt;

&lt;p&gt;Turn-taking is even more difficult in a multi-party setting. You might remember the last group call you had and just when you were about to take the turn, someone else jumped right in (because you waited for a tad bit too long) and you never got to speak. Turn-taking behaviour also differs culturally. In some cultures, interruptions and barge-ins are a lot more natural. There is also a difference in the inter-turn pause duration. These factors often lead to an unnatural conversation flow when speaking to a person from a different culture.&lt;/p&gt;

&lt;p&gt;Note : Bots with explicit turn-taking signalling like wake-words are out of scope for this blog.&lt;/p&gt;

&lt;h2 id=&quot;natural-turn-taking-dynamics&quot;&gt;Natural Turn Taking Dynamics&lt;/h2&gt;

&lt;p&gt;Irrespective of nuances, there are aspects of turn taking behaviour which are globally present in natural human-human conversation and one’s that we would want to imbibe in a human-bot interaction as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Barge-ins: These are situations when one agent interrupts the other. They occur very commonly. Examples of situations are : when one feels the other person is making a mistake or when ones feels the need to add some essential information, one naturally barges in.&lt;/li&gt;
  &lt;li&gt;Full Duplex Conversations : A half duplex conversation is one where turns are alternatively taken, like playing a tennis match, however in natural conversations, there are often instances when both people are saying something at the same time.
    &lt;ul&gt;
      &lt;li&gt;backchannels : words and fillers like “okay”, “alright” or “hmm” provide a lot of context about the state of the other person(for example attentiveness), especially when one is talking over the phone and visual cues are absent.&lt;/li&gt;
      &lt;li&gt;corrections : at times, when a person is saying something, one might want to make a small correction. For example, if there is an announcement being made “for the next meeting, you are supposed to finish submissions by 12th December, at so and so time….”. When the person is saying 12th December someone might correct by saying 13th December. This information is assimilated by the person and they often correct themselves. So, humans have the ability to hear and understand even while speaking and are active listeners.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/duplex-conversations.png&quot; /&gt;
    &lt;figcaption&gt;Fig 1: Full duplex vs half duplex conversations.&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Minimal inter-turn pauses : if you’ve ever spoken with a voice assistant, one of the first observations is that it takes too long to start speaking after you are done and the other way around. Human conversations have a much lower turn taking latency. If this latency is near optimal, it also lends to a feeling that the other person is understanding you and the left over impression is that of a conversation gone well. Human’s have an average pause duration of 200ms as shown below, while bots have a much higher latency.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/pause-duration.png&quot; /&gt;
    &lt;figcaption&gt;Fig 2: Turn Taking Pause duration as measured from the Switchboard corpus. Image is taken from [1].&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Turn taking cues : often in natural conversations, people produce small vocal cues like filler words “umm” or “uhhh” to convey that they want to say something and take the turn.&lt;/li&gt;
  &lt;li&gt;Turn yielding cues : there are markers is conversations when one knows that the person is done speaking. This is how we are able to separate pauses, which happen when a person is thinking in between his utterance vs one when he is done speaking.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;turn-taking-dynamics-in-voice-bots&quot;&gt;Turn-Taking Dynamics in Voice Bots&lt;/h2&gt;

&lt;p&gt;Below, we discuss different versions of turn-taking dynamics implemented in voice-bots each with more features and increasing levels of difficulty.&lt;/p&gt;

&lt;h3 id=&quot;version---10&quot;&gt;Version - 1.0&lt;/h3&gt;

&lt;p&gt;These are some characteristics of a bare bone turn-taking behaviour that one would need in a voice bot deployment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initial patience : the time that the bot waits for the person to starts speaking&lt;/li&gt;
  &lt;li&gt;Silence detection : if the bot detects silence for a certain duration after the person has started speaking, it assumes the person’s turn is over.&lt;/li&gt;
  &lt;li&gt;Max turn duration : it doesn’t make sense to just be listening (because of error compounding, loss of context, maybe one is hearing just noise), so usually voice bots have a maximum duration to which they listen to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---20&quot;&gt;Version - 2.0&lt;/h3&gt;

&lt;p&gt;This version add robustness for real life situations, make the bot more human-like and tries to reduce the latency between turns.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VAD instead of silence detection : Often existence of background noise, speech and other signals causes the bot to keep listening. Instead one could train a Voice-Activity Detection system rather than use silence detection, to have robustness to background events and to listen to the user only when they are speaking.&lt;/li&gt;
  &lt;li&gt;Variable thresholds for silence detection and max duration : In some states for example, when the bot is expecting a yes/no answer, it makes sense to use smaller thresholds. In general dynamic thresholding should be used.&lt;/li&gt;
  &lt;li&gt;For turn-switching, instead of a simple VAD, use an IPU based model discussed &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;here&lt;/a&gt;. This uses a smaller VAD threshold + cues to predict the turn is over. One could start with some verbal cues for example phrase completion.&lt;/li&gt;
  &lt;li&gt;Adding backchannels as bot responses : So far we’ve only discussed aspects of perception, but backchannels are a very useful response feature. It makes the user feel that the bot is more attentive and is actively listening.
    &lt;ul&gt;
      &lt;li&gt;One could also add filler words in the main channel, when the bot is taking too long to produce a response in cases of high latency. This would prevent the user from asking a question to verify if the bot is there or not. Without this, the user’s speech would lead to further increase in latency as it would be perceived as a case when the user wants to take the turn and say something useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---30&quot;&gt;Version - 3.0&lt;/h3&gt;

&lt;p&gt;There are no good baselines for these and working on improvements would constitute state of the art performance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-party situations : These are a lot more complex and require modelling multiple parties. An application could be when the bot is overseeing a human-machine interaction say, between a call centre agent and a human. Another common use is when during a typical 2 party interaction, someone interrupts the user. This requires the bot being aware that the user is speaking to someone else and then waiting.&lt;/li&gt;
  &lt;li&gt;Full - Duplex Conversations : Unlike human-human conversations a bots can attentively listen at the same time, while saying something. This offers a possibility of redesigning interactions which can leverage this feature.&lt;/li&gt;
  &lt;li&gt;Personalisation of Turn taking behaviour : This involves changing the parameters based on user characteristics. One could entrain one’s system to be more in line with the user’s behaviour. At times when the user is angry it might involve changing the durations to feel that they are being heard.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;Turn-taking in Conversational Systems and Human-Robot Interaction: A Review&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="Turn-taking" /><category term="barge-in" /><category term="duplex conversations" /><summary type="html">One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette. Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.</summary></entry><entry><title type="html">Feature Disentanglement - I</title><link href="/feature-disentanglement1/" rel="alternate" type="text/html" title="Feature Disentanglement - I" /><published>2022-02-22T00:00:00+00:00</published><updated>2022-02-22T00:00:00+00:00</updated><id>/feature-disentanglement1</id><content type="html" xml:base="/feature-disentanglement1/">&lt;p&gt;The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.&lt;/p&gt;

&lt;h1 id=&quot;data-representation-and-latent-code&quot;&gt;Data Representation and Latent Code&lt;/h1&gt;
&lt;p&gt;An image \((x)\) from the MNIST dataset has 28x28 = 784 dimensions which is a sparse representation of the image that can be visualized. But all these dimensions are not required to represent the image. The content of the images can be represented in a condensed form using lesser dimensions called latent code. Although the actual image has 784 dimensions \(x \in R^{784}\), one way of representing MNIST image can be with just an integer ie: \(z \in \{0, 1, 2, …, 9\}\). This representation \(z\) reduces the dimension of representing the image \(x\) to 1 which captures the content of which number is present in the image and the variability in the dataset. This is one example of discrete latent code for the MNIST dataset, a continuous latent code will contain more information about the image such as the style of the image, position of the number, size of the number in the image, etc.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;600&quot; height=&quot;300&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://www.mdpi.com/applsci/applsci-09-03169/article_deploy/html/images/applsci-09-03169-g001.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2:  Sample Images of MNIST from [1]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;autoencoder&quot;&gt;AutoEncoder&lt;/h2&gt;

&lt;p&gt;Autoencoder[2] models are popularly used to learn such latent code in an unsupervised manner by compressing the image to a fixed dimension code \(z\) and generating the image back using this latent code with an encoder-decoder model.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/08b0b21725c236fb1860285677a00248f77c7587/2-Figure1-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Autoencoder architecture from Autoencoders[2]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The encoder \(q_{\phi}(z \mid x)\) of the autoencoder compresses the image to a fixed dimension\((d)\) latent code\((z)\), and the decoder \(p_{\theta}(x \mid z)\) is a conditional image generator. The dimension of z has to be such that, the image can be completely reconstructed by the decoder with the latent code. Choosing the dimension of the latent code is a problem on its own[3].&lt;/p&gt;

&lt;p&gt;The autoencoder models trained will successfully encode the images into a latent code \(z\), but there is no guarantee that the latent code can be easily inferred, ie: we do not know where in the d-dimensional space the model encoded the image into, and thus difficult to choose a latent code to generate image during inference. So the conclusion is we have no idea how and where the encoder encodes the images, so we do not have control over synthesis during inference. The following figure shows the latent code learned by the AutoEncoder model with different training, as we can observe the latent space keep changing the range and quadrant and thus difficult to infer.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Latent code of MNIST images learned by an Auto Encoder [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational AutoEncoder(VAE)&lt;/h2&gt;

&lt;p&gt;Variational autoencoders(VAE) [6] solve this problem by forcing the latent code (z) to be close to a known prior distribution(Gaussian), this gives us control over the latent space. During inference, the latent space can be sampled from this known distribution for image generation. The following figure shows the latent code learned by VAE with different training, and the latent space across training is centered to the mean 0 across dimensions.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig2.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Latent code of MNIST images learned by a VAE [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;VAE allows us to have control over the latent space and sample from the known prior distribution. But this again does not give us control over the generation of the image. Say if you want to generate an image of the number ‘3’ or ‘7’, you cannot do that(at least not directly). This is where the term “disentanglement” comes into play.&lt;/p&gt;

&lt;h1 id=&quot;disentanglement&quot;&gt;Disentanglement&lt;/h1&gt;
&lt;p&gt;Feature disentanglement is isolating the source of variation in observation data. There is a lot more factors/feature of an MNIST image other than the number itself, such as the location of the number in the image, size of the image, angle of the number, etc. These factors are independent of each other.&lt;/p&gt;

&lt;p&gt;Feature disentanglement involves separating underlying concepts of “Big one in the left”: ie: size(big), number(one), location(left).
Our interest here is to see if we can isolate these factors in the latent code so that we can have control over the generation of the images. So we want the encoder to disentangle the representation into different factors and then we generate the image with desired factors say “small seven in the top rotated 30 degrees”.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; height=&quot;400&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/35da0a2001eea88486a5de677ab97868c93d0824/6-Figure2-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5: Generated MNIST images by InfoGAN [5] varied digit, thickness and roatation.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-vae&quot;&gt;Beta-VAE&lt;/h2&gt;
&lt;p&gt;Beta-VAE is a variant of VAE which allows disentanglement of the learned latent code. Beta-VAE adds hyperparameter to the loss function which modulates the learning constraint of VAE.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://miro.medium.com/max/1400/1*Z6tj5bVoArekVgv65gfkfg.png&quot; /&gt;
  &lt;figcaption&gt;Fig 6: Loss function of beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The first part of the loss function takes care of the reconstruction of the image, it is the second term that learns the latent code of VAE. Different dimensions that span across Gaussians are independent, so by making the prior distribution gaussian, we force the dimensions of the latent code to be independent of each other. So increasing the weight of the second part of the loss, makes the latent code to be disentangled and independent. But this also brings a tradeoff between disentanglement and the reconstruction capability of the VAE. Although Beta-VAE models are good in disentangling the features, the reconstruction ability of this model is not the best.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.00.13_PM.png&quot; /&gt;
  &lt;figcaption&gt;Fig 7: Samples generated by beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-tcvae&quot;&gt;Beta-TCVAE&lt;/h2&gt;
&lt;p&gt;beta-TCVAE decomposes the KL divergence[10] term of the loss function of VAE into reconstruction loss, Index-code mutual information[8] between data and latent variable, Total Correlation[9] of z, and Dimension wise KL divergence[10] of \(z\)(respectively in the following formula). This helps to break the overall KL Divergence of \(z\) into dimension-wise quantities, which will focus on each dimension of the latent code \(z\). In this formulation, the beta hyperparameter is only on the Total Correlation term which is more important for disentanglement without affecting the reconstruction. So, Beta-TCVAE has better reconstruction ability than Beta-VAE with similar disentanglement property.&lt;/p&gt;

\[\mathcal{L}_{\beta-\mathrm{TC}}:=\mathbb{E}_{q(z \mid n) p(n)}[\log p(n \mid z)]-\alpha I_{q}(z ; n)-\beta \operatorname{KL}\left(q(z) \| \prod_{j} q\left(z_{j}\right)\right)-\gamma \sum_{j} \operatorname{KL}\left(q\left(z_{j}\right) \| p\left(z_{j}\right)\right)\]

&lt;p&gt;where \(\alpha = \gamma = 1\) and only \(\beta\) is varies as the hyperparameter.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://vitalab.github.io/article/images/IsolatingSourcesOfDisentanglementInVAEs/figure1.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 8: Samples generated by beta-TCVAE [8].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;In future posts, we will examine many new methods for feature disentanglement and how these methods can be applied to speech signals.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://www.mdpi.com/2076-3417/9/15/3169/htm&quot;&gt;A Survey of Handwritten Character Recognition with MNIST and EMNIST&lt;/a&gt; (2019)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://arxiv.org/abs/2003.05991&quot;&gt;Autoencoders&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0925231215015994&quot;&gt;Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://www.youtube.com/watch?v=itOlzH9FHkI&quot;&gt;Disentangled Representations - How to do Interpretable Compression with Neural Models&lt;/a&gt; (2020)&lt;/p&gt;

&lt;p&gt;[5] : &lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot;&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-Encoding Variational Bayes&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://openreview.net/forum?id=Sy2fzU9gl&quot;&gt;beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework &lt;/a&gt; (2017)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://arxiv.org/abs/1802.04942&quot;&gt;Isolating Sources of Disentanglement in Variational Autoencoders&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_correlation&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;</content><author><name>Shangeth Rajaa</name></author><category term="Machine Learning" /><summary type="html">The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.</summary></entry><entry><title type="html">Google Summer of Code, 2022</title><link href="/gsoc-2022/" rel="alternate" type="text/html" title="Google Summer of Code, 2022" /><published>2022-02-18T00:00:00+00:00</published><updated>2022-02-18T00:00:00+00:00</updated><id>/gsoc-2022</id><content type="html" xml:base="/gsoc-2022/">&lt;h1 id=&quot;google-summer-of-code---2022&quot;&gt;Google Summer of Code - 2022&lt;/h1&gt;

&lt;p&gt;This page contains ideas which we’d like to get help from GSoC Contributors. But before all that, if you haven’t heard about Skit.&lt;/p&gt;

&lt;h2 id=&quot;what-is-skit&quot;&gt;What is Skit?&lt;/h2&gt;

&lt;p&gt;We are a series B funded, AI-first SaaS voice automation company specializing in delivering multilingual voice bots for contact center automation. We have our Speech Bots deployed in major banks and large enterprises in several verticals. We have a foothold in India and are expanding in the US and South-East Asia.&lt;/p&gt;

&lt;p&gt;We have been listed in &lt;a href=&quot;https://www.forbes.com/sites/johnkang/2021/04/19/the-forbes-30-under-30-asia-startups-unshackling-businesses-using-ai/?sh=9268fa85f9aa&quot;&gt;Forbes 30 Under 30 Asia 2021&lt;/a&gt; and have been named by Gartner as a &lt;a href=&quot;https://www.businesswire.com/news/home/20211221005315/en/Skit-Named-as-a-Cool-Vendor-in-Gartner-Cool-Vendors-in-Conversational-and-NLT-Widen-Use-Cases-Domain-Knowledge-and-Dialect-Support?utm_medium=email&amp;amp;_hsmi=202791220&amp;amp;_hsenc=p2ANqtz--yQEAO9Q810nr71J9I8MPppXkOWpWg51LqIrOdv_Wc2X_Hj-ydmia5ruRLbQEEat7EPQ6fn_GHMMVWu4tUV8beoU2BQA&amp;amp;utm_content=202791220&amp;amp;utm_source=hs_email&quot;&gt;Cool vendor in Conversational and NLT Widen Use Cases&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our goal is to build the most natural and robust multi lingual voice bot with state of the art human-machine interaction capabilities.&lt;/p&gt;

&lt;p&gt;We build voicebots, so that agents don’t have to sit and answer user queries 24/7 for 365 days.&lt;/p&gt;

&lt;p&gt;You can get to know more about us over &lt;a href=&quot;https://skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our tech blog is present &lt;a href=&quot;https://tech.skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;p&gt;You can reach out to us at our skit-gsoc community discord, link to join here : &lt;a href=&quot;https://discord.gg/Y9sJwz5Sw8&quot;&gt;https://discord.gg/Y9sJwz5Sw8&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gsoc-2022-ideas&quot;&gt;GSoC 2022 Ideas&lt;/h2&gt;

&lt;h1 id=&quot;idea-1-enhancements-to-dialogy-via-core-code-or-plugins&quot;&gt;Idea 1: Enhancements to dialogy via core code or plugins.&lt;/h1&gt;

&lt;h3 id=&quot;project-description&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://skit-ai.github.io/dialogy/&quot;&gt;Dialogy&lt;/a&gt; is a framework to build machine-learning solutions for speech applications speech dialogue systems.&lt;/p&gt;

&lt;p&gt;The main principles which form the backbone of dialogy are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plugin-based: Makes it easy to import/export components to projects.&lt;/li&gt;
  &lt;li&gt;Stack-agnostic: No assumptions made on ML stack; your choice of machine learning library will not be affected by using Dialogy.&lt;/li&gt;
  &lt;li&gt;Progressive: Minimal boilerplate writing to let you focus on your machine learning problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At current shape, dialogy allows one to train models, test performance on metrics and deploy applications. We want to add more features which are desirable in an complete SLU framework. These features could include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyperparameter Tuning- There are multiple hyperparameters involved in using a transformers. A hyperparameter tuning integration would allow developer with a seamless way to experiment with hyperparams and train optimal models.&lt;/li&gt;
  &lt;li&gt;Model interpretability via Captum, LIT integration- Interpretability is necessary from both business and development perspectives. Such tools would allow developers to explain why a certain prediction was made, as well as discover biases and faults in the model.&lt;/li&gt;
  &lt;li&gt;Integration with experiment tracking platforms- It gets difficult to keep track of models being trained by developers- some are meant for production releases, some are meant for experimentation. An experiment tracking integration would enable developers to manage their models and results easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;github-links&quot;&gt;GitHub Link(s):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy&quot;&gt;dialogy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy-template-simple-transformers&quot;&gt;dialogy-template-simple-transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes&quot;&gt;Expected Outcomes&lt;/h3&gt;

&lt;p&gt;Dialogy as a platform would have following features after the project&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Integrated with experiment tracking for better project management&lt;/li&gt;
  &lt;li&gt;Integrated with model explainability and interpretability frameworks allowing users to use these tools&lt;/li&gt;
  &lt;li&gt;Integrated with an hyperparameter tuning service/library&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Understanding of Machine Learning frameworks like PyTorch, Huggingface, etc.
And of course Python.&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Himansu - &lt;a href=&quot;https://www.linkedin.com/in/himansu-didwania/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/hdidwania&quot;&gt;github&lt;/a&gt;, email - himansu@skit.ai&lt;/li&gt;
  &lt;li&gt;Jaivarsan - &lt;a href=&quot;https://www.linkedin.com/in/jaivarsan-b-50264b148/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/greed2411&quot;&gt;github&lt;/a&gt;, email - jaivarsan@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-2-speaker-anonymization&quot;&gt;Idea 2: Speaker Anonymization&lt;/h1&gt;

&lt;h3 id=&quot;project-description-1&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;The goal of this project is to explore and implement methods to anonymize speech to remove the speaker information from the signal by distorting the speaker prosodic features or with techniques like voice conversion. The speaker information in the signal can be used to attack the speaker verification systems which leads to privacy and security concerns. The idea is to explore simple signal/speech processing techniques which are faster in both implementation and deployment, as well as neural methods which use deep learning models and architectures to resynthesis the speech signal with the original speaker’s information removed. Removing the speaker’s information can help us to use speech datasets without worrying about privacy attacks on the speakers in the dataset. This project involves the implementation of various research papers in the field and improving upon them and creating a python package for speaker anonymization.&lt;/p&gt;

&lt;h3 id=&quot;github&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/aunom&quot;&gt;aunom&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-1&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Literature Review of research papers in the field&lt;/li&gt;
  &lt;li&gt;Implementation of various research papers which are interesting to the project in python.&lt;/li&gt;
  &lt;li&gt;Github repository for experiments and final python package for speaker anonymization with proper documentation.&lt;/li&gt;
  &lt;li&gt;Demonstration of the methods implemented and final package.&lt;/li&gt;
  &lt;li&gt;Talk/presentation on the work done during the GSoC period.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required-1&quot;&gt;Skills Required&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Experience in Signal/Speech processing (Preferred)&lt;/li&gt;
  &lt;li&gt;Experience in Deep learning for speech tasks like Automatic Speech Recognition, Speech synthesis, speaker-related tasks  etc.&lt;/li&gt;
  &lt;li&gt;Python and deep learning frameworks like PyTorch/TensorFlow.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;possible-mentors-1&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Shangeth Rajaa - &lt;a href=&quot;https://www.linkedin.com/in/shangeth/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/shangeth&quot;&gt;github&lt;/a&gt;, email - shangeth.rajaa@skit.ai&lt;/li&gt;
  &lt;li&gt;Swaraj - &lt;a href=&quot;https://www.linkedin.com/in/swaraj-dalmia/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/swarajdalmia&quot;&gt;github&lt;/a&gt;, email - swaraj@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size-1&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 Hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-1&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-3-improving-kaldi-serve-performance&quot;&gt;Idea 3: Improving kaldi-serve performance&lt;/h1&gt;

&lt;h3 id=&quot;project-description-2&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;​​Kaldi Serve is a plug-and-play abstraction over the &lt;a href=&quot;https://kaldi-asr.org/&quot;&gt;Kaldi ASR&lt;/a&gt; toolkit, designed for ease of deployment and optimal runtime performance. It currently has the following key features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real-time streaming (uni &amp;amp; bi-directional) audio recognition.&lt;/li&gt;
  &lt;li&gt;Thread-safe concurrent Decoder queue for server environments.&lt;/li&gt;
  &lt;li&gt;RNNLM lattice rescoring.&lt;/li&gt;
  &lt;li&gt;N-best alternatives with AM/LM costs, word-level timings, and confidence scores.&lt;/li&gt;
  &lt;li&gt;Easy extensibility for custom applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are mainly looking at improving the runtime performance of the ASR pipeline by offloading Decoder computation to the GPU to be performed in mini-batches, and implementing a request queueing mechanism within the gRPC server to be able to utilize the parallel computing capability in order to boost latencies under high concurrent loads.&lt;/p&gt;

&lt;h3 id=&quot;github-1&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/kaldi-serve&quot;&gt;kaldi-serve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-2&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;p&gt;Integration with Kaldi Batched Threaded NNet3 CUDA pipeline for enabling batched computation in kaldi-serve gRPC application.&lt;/p&gt;

&lt;h3 id=&quot;skills-required-2&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Having some combination of these:
Languages: C++, CUDA, Python
Frameworks: Kaldi ASR, Pytorch, gRPC, Pybind11
Basics: Speech Recognition, Language Modeling, Deep Learning&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors-2&quot;&gt;Possible Mentors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Prabhsimran - &lt;a href=&quot;https://www.linkedin.com/in/pskrunner14/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/pskrunner14&quot;&gt;github&lt;/a&gt;, email - prabhsimran@skit.ai&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-size-2&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-2&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;</content><author><name>Jaivarsan B</name></author><category term="Machine Learning" /><category term="work" /><summary type="html">Google Summer of Code - 2022</summary></entry><entry><title type="html">Speaker Entrainment</title><link href="/speaker-entrainment/" rel="alternate" type="text/html" title="Speaker Entrainment" /><published>2022-02-04T00:00:00+00:00</published><updated>2022-02-04T00:00:00+00:00</updated><id>/speaker-entrainment</id><content type="html" xml:base="/speaker-entrainment/">&lt;p&gt;In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Speaker Entrainment (also known as accomodation or alignment) is a psycho-social phenomenon that has been observed in human-human conversations in which interlocutors tend to match each other’s speech features. Believed to be crucial to the success and naturalness of human-human conversations, this can look like matching style related aspects such as pitch, rate of articulation or intensity, or content related factors, such as lexical patterns.&lt;/p&gt;

&lt;p&gt;This phenomenon essentially helps one manage the social “distance” between the two speakers, and hence serves to build trust. This trust has the potential to increase call resolution rates and improve customer satisfaction in task-oriented dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/user-study.jpg&quot; /&gt;
  &lt;figcaption&gt;User study for speaker entrainment.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Much research has gone into what features are relevant for speaker entrainment, such as phonetic features [1], linguistic features such as word choice [2], structure/syntax [3], style [4] and acoustic-prosodic features (pitch, intensity, rate of articulation, NHR, jitter, shimmer) [5, 6, 7].&lt;/p&gt;

&lt;p&gt;Based on this, our research work in this project aims at building a baseline bot using low-level understanding of the above features to establish the statistical significance of speaker entrainment as well as understand its potential to improve customer experience. We also publish a &lt;a href=&quot;https://tech.skit.ai/explore/speaker-entrainment&quot;&gt;demo&lt;/a&gt; to show what this looks like in real-world conversations.&lt;/p&gt;

&lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt;

&lt;p&gt;There have been a few implementations of speaker entrainment modules in research, such as Lubold &lt;em&gt;et al.&lt;/em&gt; (2016) [8], which discusses a system of entrainment based only on modelling pitch and Levitan &lt;em&gt;et al.&lt;/em&gt; (2016) [9] which details a system based on \(f_0\), intensity and rate of articulation. Subsequently Hoegen &lt;em&gt;et al.&lt;/em&gt; (2019) [10] discusses a system of modelling acoustic and content (lexical) variables separately, and Entrainment2Vec (2020) [11] details a graph based model of entrainment with vector representations in multi-party dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/entrainer.png&quot; /&gt;
  &lt;figcaption&gt;Basic structure of a Speaker Entrainment dialog system (reproduced from [14]).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;For our baseline model, we choose mean \(f_0\), intensity and rate of articulation (calculated by utterances/sec). We average over these over the past three utterances (two speaker utterances and one bot utterance) to calculate the value for next bot utterance. This is inspired from [10] and prevents drastic jumps in the bot’s voice profile, which might lead to unnaturalness. For the value of any feature \(F\) at turn \(i\) we have,&lt;/p&gt;

\[F_{bot, i} = \frac{F_{user, i-1} + F_{bot, i-1} + F_{user, i-2}}{3}\]

&lt;p&gt;as the value of the feature at \(i\)-th turn in the bot’s speech.&lt;/p&gt;
&lt;h1 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h1&gt;

&lt;p&gt;We record 11 scripts with varying measures in each of the three features (e.g. pitch rising, intensity low/high, rate of articulation low) and one with an angry user using the entraining bot and a control bot which does not entrain. We involve 30 participants in this experiment who are asked to rate the bots on factors such as likeability and naturalness. We then conduct a paired right taled t-test to determine the statistical significance of speaker entrainment over the three features and combinations thereof.&lt;/p&gt;

&lt;p&gt;The questions have been inspired from Shamekhi &lt;em&gt;et al.&lt;/em&gt; [12], which are posed comparatively, e.g. “Does Bot A sound more natural than Bot B?”. These are answered on a comparative scale as well (from Strongly Disagree to Strongly Agree) to encourage decisiveness in differentiation among participants. Note that if one assumes the comparative scale comes from a difference of scores that the participant evaluates internally, the paired t-test can still be conducted. This is because if we assume \(X_C\) to be the comparative score arising from the difference of \(X_A\) and \(X_B\) (score for Bot A and Bot B respectively), we have,&lt;/p&gt;

\[X_C = X_B - X_A\]

&lt;p&gt;Note that for performing a t-test we only need the difference in mean and the sum of squares of standard deviation.&lt;/p&gt;

\[t=\frac{E[X_B]-E[X_A]}{\sqrt{\frac{Var(X_B)}{n}+\frac{Var(X_B)}{n}}}\]

&lt;p&gt;This can be easily rearranged to&lt;/p&gt;

\[t=\frac{E[X_c]}{\sqrt{\frac{Var(X_c)}{n}}}\]

&lt;p&gt;With this t-value, knowing what side of the tail our data lies on, we use the right tailed cumulative distribution and calculate the p-values accordingly.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We find that the entrained bot performs better than the non-entrained bot in most cases. Keeping our \(\alpha=0.01\), we reject the null hypothesis for entrainment in multiple feature-sets, namely: pitch, intensity, pitch and rate of articulation combined and loudness and rate of articulation combined. In most other cases, including that of the angry user, we find that the entrained bot performs better as well, however the p-values aren’t as signficant. There are cases when the non-entrained bot performs better than the the entrained bot, but they have a large overlap with the cases in which our perception module performs inaccurately, e.g. rate of articulation is higher in shorter utterances due to lesser number of silent periods.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;

&lt;p&gt;There are a few issues with our investigations in terms of the insights we can draw. Firstly, in cases of the entrained bot performing better, it is difficult to disentangle whether this is a result of the changed voice sounding better in absolute (e.g. perhaps the participants have a preference to higher pitched/faster speaking voices as a result of shared socio-cultural factors).&lt;/p&gt;

&lt;p&gt;Secondly, in the cases of entrained bot performing worse, it is again difficult to disentangle if this poor performance arises from entraining poorly or whether speaker entrainment over this feature leads to bad performance in general.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future Work&lt;/h1&gt;

&lt;p&gt;Levitan (2020) [13] is an inspiration of future directions in speaker entrainment research. So far, there is a significant lacunae in speaker entrainment research as far as incorporation of deep learning is concerned, be it in the perception module (i.e. rich representation spaces for audio) or control module (TTS with a natural control over features and emotions).&lt;/p&gt;

&lt;p&gt;Classifying user speech should also be incorporated to understand the degree of entrainment that is necessary, rather than just entraining for every user, which can look like discriminating based on the style of conversation such as High Involvement and High Consideration (described in Hoegen &lt;em&gt;et al.&lt;/em&gt; [10]). This layer of classification serves to decide the quality/degree of entrainment, which has the potential to improve customer experience. Furthermore, this layer can help detect angry/dissatisfied users as well to plan appropriate course of action by detecting high intensity speech, hyper-articulation, etc.&lt;/p&gt;

&lt;p&gt;One can stand to improve the quality of experimentation as well. Apart from having more granular options for providing opinions (like a 7-point scale), we can have more speakers as users and more voice profiles for the bot to disentangle the experiment’s results from inherent voice qualities. Moreover, any improvement in the entrainment module will improve the quality of results as well.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Many psycho-sociologists deem speaker entrainment to be crucial to naturalness and trust building in human-human conversations. Recent advancements discussed in Levitan (2020) [13] imply that speaker entrainment is more nuanced than previously thought, but this means if implemented and modelled well, speaker entrainment has the potential of significantly changing the way voicebots interact with users.&lt;/p&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;p&gt;[1] J. S. Pardo, “On phonetic convergence during conversational interaction,” J. Acoust. Soc. Am., vol. 119, no. 4, pp. 2382–2393, 2006.&lt;/p&gt;

&lt;p&gt;[2]	K. G. Niederhoffer and J. W. Pennebaker, “Linguistic style matching in social interaction,” J. Lang. Soc. Psychol., vol. 21, no. 4, pp. 337–360, 2002.&lt;/p&gt;

&lt;p&gt;[3]	D. Reitter, J. D. Moore, and F. Keller, “Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,” 2010.&lt;/p&gt;

&lt;p&gt;[4]	C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs,” ArXiv Prepr. ArXiv11063077, 2011.&lt;/p&gt;

&lt;p&gt;[5]	R. Levitan and J. Hirschberg, “Measuring Acoustic-Prosodic Entrainment with Respect to Multiple Levels and Dimensions,” 2011.&lt;/p&gt;

&lt;p&gt;[6]	R. Levitan, A. Gravano, L. Willson, Š. Beňuš, J. Hirschberg, and A. Nenkova, “Acoustic-prosodic entrainment and social behavior,” in Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, 2012, pp. 11–19.&lt;/p&gt;

&lt;p&gt;[7]	N. Lubold and H. Pon-Barry, “Acoustic-prosodic entrainment and rapport in collaborative learning dialogues,” in Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, 2014, pp. 5–12.&lt;/p&gt;

&lt;p&gt;[8] N. Lubold, H. Pon-Barry, and E. Walker, “Naturalness and rapport in a pitch adaptive learning companion,” Dec. 2015, pp. 103–110. doi: 10.1109/ASRU.2015.7404781.&lt;/p&gt;

&lt;p&gt;[9]	R. Levitan et al., “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar,” in Proc. Interspeech 2016, 2016, pp. 1166–1170. doi: 10.21437/Interspeech.2016-985.&lt;/p&gt;

&lt;p&gt;[10]	R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski, “An End-to-End Conversational Style Matching Agent,” Proc. 19th ACM Int. Conf. Intell. Virtual Agents, pp. 111–118, Jul. 2019, doi: 10.1145/3308532.3329473.&lt;/p&gt;

&lt;p&gt;[11]	Z. Rahimi and D. Litman, “Entrainment2vec: Embedding entrainment for multi-party dialogues,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, no. 05, pp. 8681–8688.&lt;/p&gt;

&lt;p&gt;[12] A. Shamekhi, M. Czerwinski, G. Mark, M. Novotny, and G. Bennett, “An Exploratory Study Toward the Preferred Conversational Style for Compatible Virtual Agents,” Oct. 2017, Accessed: May 28, 2021. &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/exploratory-study-toward-preferred-conversational-style-compatible-virtual-agents-2/&quot;&gt;Online Available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[13] R. Levitan, “Developing an Integrated Model of Speech Entrainment,” in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Japan, Jul. 2020, pp. 5159–5163. doi: 10.24963/ijcai.2020/727.&lt;/p&gt;

&lt;p&gt;[14] Levitan, Rivka, et al. “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar.” Interspeech. Vol. 16. 2016.&lt;/p&gt;</content><author><name>Shikhar Mohan</name></author><category term="Machine Learning" /><category term="ASR" /><summary type="html">In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/entrain.png" /><media:content medium="image" url="/assets/images/entrain.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Speech-First Conversational AI</title><link href="/speech-first-conversational-ai/" rel="alternate" type="text/html" title="Speech-First Conversational AI" /><published>2022-02-02T00:00:00+00:00</published><updated>2022-02-02T00:00:00+00:00</updated><id>/speech-first-conversational-ai</id><content type="html" xml:base="/speech-first-conversational-ai/">&lt;p&gt;We often get asked about the differences between voice and chat bots. The most
common perception is that the voice bot problem can be reduced to chat bot after
plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS)
system. We believe that’s an overly naive assumption about spoken conversations,
even in restricted goal-oriented dialog systems. This post is an attempt to
describe the differences involved and define what &lt;em&gt;Speech-First&lt;/em&gt; Conversational
AI means.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Speech is the most sophisticated behavior of the most complex organism in the
known universe. - &lt;a href=&quot;https://youtu.be/Zy3Ny-WjyGE?t=251&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Conversational AI systems solve problems of conversations, either using text or
voice. Since &lt;em&gt;conversations&lt;/em&gt; are specific to humans, there are many
anthropomorphic expectations from these systems. These expectations, while still
strong, are less restraining in text conversations as compared to speech. Speech
is deeply ingrained in human communication and minor misses could lead to
violation of user expectations. Contrast this with text messaging which is a,
relatively new, human-constructed channel&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; where expectations are different
and more lenient.&lt;/p&gt;

&lt;p&gt;There are multiple academic sources on differences between speech and text, here
we will describing a few key differences that we have noticed while building
speech-first conversation systems in more practical settings.&lt;/p&gt;

&lt;h2 id=&quot;signal&quot;&gt;Signal&lt;/h2&gt;

&lt;p&gt;In addition to the textual content, speech signals contain information about the
user’s state, trait, and the environment. Speech isn’t merely a redundant
modality, but adds valuable extra information. Different style of uttering the
same utterance can drastically change the meaning, something that’s used a lot
in human-human conversations.&lt;/p&gt;

&lt;p&gt;Environmental factors like recording quality, background ambience, and audio
events impact signals’ reception and semantics. Even beyond the immediate
environment, a lot of socio-cultural factors are embedded in speech beyond the
level they are in text chats. Because the signals are rich, the difficulty of a
few common problems across text and speech, like low-resource languages, is
higher.&lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;Once you go on transcribing audios utterances using ASRs, transcription errors
will add on to your burden. While ASR systems are improving day-on-day, there
still is error potential in handling acoustically similar utterances. Overall,
an entirely new set of problems like far-field ASR, signal enhancement, etc.
exist in spoken conversations.&lt;/p&gt;

&lt;p&gt;Additionally many &lt;em&gt;noisy&lt;/em&gt; deviations from fluent speech are not mere errors but
develop their own pragmatic sense and convey strong meaning. Speech
&lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_disfluency&quot;&gt;disfluencies&lt;/a&gt; are commonly
assumed behaviors of natural conversations and lack of them could even cause
discomfort.&lt;/p&gt;

&lt;h2 id=&quot;interaction-behavior&quot;&gt;Interaction Behavior&lt;/h2&gt;

&lt;p&gt;We don’t take turns in a half-duplex manner while talking. Even then, most
dialog management systems are designed like sequential turn-taking state
machines where party A says something, then hands over control to party B, then
takes back after B is done. The way we take turns in true spoken conversations
is more &lt;em&gt;full-duplex&lt;/em&gt; and that’s where a lot of interesting conversational
phenomenon happen.&lt;/p&gt;

&lt;p&gt;While conversing, we freely barge-in, attempt corrections, and show other
backchannel behaviors. When the other party also start doing the same and
utilizing these both parties can have much more effective and grounded
conversations.&lt;/p&gt;

&lt;p&gt;Additionally, because of lack of a visual interface to keep the context, user
recall around dialog history is different and that leads to different flow
designs.&lt;/p&gt;

&lt;h2 id=&quot;personalization-and-adaptations&quot;&gt;Personalization and adaptations&lt;/h2&gt;

&lt;p&gt;With all the extra added richness in the signals, the potential of
personalization and adaptations goes up. A human talking to another human does
many micro-adaptations including the choice-of-words (common with text
conversations) and the acoustics of their voices based on the ongoing
conversation.&lt;/p&gt;

&lt;p&gt;Sometimes these adaptations get ossified and form &lt;em&gt;sub-languages&lt;/em&gt; that need
different approaches of designing conversations. In our experience, people
talking to voice bots talks in a different sub-language, a relatively
understudied phenomenon.&lt;/p&gt;

&lt;h2 id=&quot;response-generation&quot;&gt;Response Generation&lt;/h2&gt;

&lt;p&gt;Similar to the section on input &lt;em&gt;signals&lt;/em&gt;, the output &lt;em&gt;signal&lt;/em&gt; from the voice
bot is also (should also be) extremely rich. This puts a lot of stake in
response production for natural conversation. The timing and content of sounds,
along with their tones impart strong semantic and pragmatic sense to the
utterance. Clever use of these also drive the conversations in a more fruitful
direction for both parties.&lt;/p&gt;

&lt;p&gt;Possibilities concerning this area of work is &lt;em&gt;extremely&lt;/em&gt; limited in text
messaging.&lt;/p&gt;

&lt;h2 id=&quot;development&quot;&gt;Development&lt;/h2&gt;

&lt;p&gt;Finally, working with audios is more difficult than text because of additional
and storage processing capabilities needed. Here is an audio utterance for the
text “1 2 3”:&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws3 = WaveSurfer.create({
           container: '#waveform-3',
           backend: 'MediaElement'
         });
         ws3.load('/assets/audios/posts/speech-first-conversational-ai/counts.wav');

         ws3.on('audioprocess', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('ready', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('finish', function () {
           let button = $('#controls-3 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-3').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws3.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws3.skipBackward()
                 break
               case 'forward':
                 ws3.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-3&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-3&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-3&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;❯ file counts.wav  # 48.0 kB (48,044 bytes)
counts.wav: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 Hz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compare this with just 6 bytes needed for the string itself (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;1 2 3&quot; | wc
--bytes&lt;/code&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;These differences lead to gaps that are difficult to bridge and that’s what
keeps us busy at Skit. If these problems interest you, you should reach out to
us on &lt;a href=&quot;mailto:join@skit.ai&quot;&gt;join@skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Epistolary communication aside. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">We often get asked about the differences between voice and chat bots. The most common perception is that the voice bot problem can be reduced to chat bot after plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS) system. We believe that’s an overly naive assumption about spoken conversations, even in restricted goal-oriented dialog systems. This post is an attempt to describe the differences involved and define what Speech-First Conversational AI means.</summary></entry><entry><title type="html">Evaluating an ASR in a Spoken Dialogue System</title><link href="/evaluating-an-asr-in-a-spoken-dialogue-system/" rel="alternate" type="text/html" title="Evaluating an ASR in a Spoken Dialogue System" /><published>2022-01-21T00:00:00+00:00</published><updated>2022-01-21T00:00:00+00:00</updated><id>/evaluating-an-asr-in-a-spoken-dialogue-system</id><content type="html" xml:base="/evaluating-an-asr-in-a-spoken-dialogue-system/">&lt;p&gt;An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.&lt;/p&gt;

&lt;p&gt;The ASR module takes as input a spoken utterance and outputs the most likely transcription. Most ASR’s output multiple alternatives with certain confidence scores. Some ASR’s including Kaldi’s implementations output N-Best alternatives in an order not necessarily reflective of the confidence. Some ASR systems output likelihood information of word sequences in the form of word-lattices or confusion networks [1] with probability information.&lt;/p&gt;

&lt;h2 id=&quot;what-is-wer-&quot;&gt;What is WER ?&lt;/h2&gt;

&lt;p&gt;Word Error Rate measures the transcription errors, treating words as the smallest unit. It takes 2 inputs, the actual transcript and a hypothesis transcript.&lt;/p&gt;

&lt;p&gt;There are two types of WER:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Isolated word recognition (IWR-WER)&lt;/li&gt;
  &lt;li&gt;Connected Speech Recognition (CSR-WER)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Isolated Word Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This considers the words in isolation and is not based on any alignment. It simply measures the number of non-hits.&lt;/p&gt;

\[IWR-WER = 1-\frac{H}{N}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits&lt;br /&gt;
N = total matched I/O words&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Connected Speech Recognition WER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is calculated on the basis of alignment and uses the Levenshtein distance for words which measures the minimum edit distance. This is efficiently calculated using dynamic programming. It calculates the best alignment which minimises the number of substitutions, insertions and deletions necessary to map the actual transcript to the hypothesis transcript, giving equal weight to all the operations. WER is not input/output symmetric, as N is the total number of words in the actual transcripts.&lt;/p&gt;

\[CSR-WER = (I+S+D)/(N)\]

&lt;p&gt;where,
I = insertions&lt;br /&gt;
S = substitutions&lt;br /&gt;
D = deletions&lt;br /&gt;
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;An example calculation of the best alignment to calculate the CSR WER is shown below.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/wer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1: Taken from Speech and Language Processing by Jurafsky and Martin [2].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The CSR WER for the above hypothesis is = (6+3+1)/13 = 0.77&lt;/p&gt;

&lt;p&gt;The upper bound of CSR WER is not 1, but \(\frac{max(N1, N2)}{N1}\) where N1 is length of true transcripts and N2 is length of hypothesis transcript.&lt;/p&gt;

&lt;h2 id=&quot;statistical-significance-of-wer-&quot;&gt;Statistical Significance of WER ?&lt;/h2&gt;

&lt;p&gt;Improvements in WER’s over a test set is one of the standard ways of evaluating upgrades in an ASR. Often, what is missed out is in this evaluation is whether the gain is statistically significant or not. One of the standard statistical tests is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989) [2]. For an example on how to calculate the statistic, consult the book by Jurafsky.&lt;/p&gt;

&lt;h2 id=&quot;issues-with-wer&quot;&gt;Issues with WER&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It gives the same importance to words like “a”, “the” compared to verbs and nouns that carry more semantic value.&lt;/li&gt;
  &lt;li&gt;WER is a purely 1 : 1 transcript based metric, and doesn’t take into account the rich output of ASR systems like alternatives or word lattices.&lt;/li&gt;
  &lt;li&gt;The concept of edit-distance that WER is based on is appropriate for a dictation machine where the additional cost is that of correcting the transcripts rather than applications where communicating the meaning is of primary importance&lt;/li&gt;
  &lt;li&gt;It doesn’t take into account the performance of an ASR in the context of the dialogue pipeline. For example if a higher WER makes no difference in the information retrieval for the downstream SLU then is the improvement worth it ?
    &lt;ul&gt;
      &lt;li&gt;An example of a context dependent metric is discussed in [4].&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;It is not a true % based metric, because it has no upper bound therefore it doesn’t tell you how good a system is, but only that one is better than another. Even for the later, it provides only a heuristic for ranking of performance.
    &lt;ul&gt;
      &lt;li&gt;Consider two ASR systems, ASR-1 that replaces 2 wrong words for every word it listens too, and another ASR system that replaces 1 wrong word for every word it listens to. Both communicate zero information, but the WER for ASR-1 is 2 and the WER for ASR-1 is 1. This 50% difference in WER is not reflective of performance, since both the systems communicate no correct information whatsoever.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;debunking-conventional-wisdom&quot;&gt;Debunking Conventional Wisdom&lt;/h2&gt;

&lt;p&gt;One might assume that better ASR’s improve the performance of all downstream SLU systems. A paper in 2003 [7] found this was not always the case.  Their model had a 17% better slot accuracy despite a 46% worse WER performance. They have this gain by using a SLU model as the language model for speech recognition. Therefore it is not necessary that an oracular ASR will solve downstream inaccuracies. It is important to be aware that there might be non-linear correlations in metrics for the ASR vs the downstream task.&lt;/p&gt;

&lt;h2 id=&quot;wer-variants&quot;&gt;WER Variants:&lt;/h2&gt;

&lt;p&gt;A few WER variants are discussed below.&lt;/p&gt;

&lt;p&gt;Metrics that look at a different granularity than that of words :
&lt;em&gt;**&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sentence Error Rate (SER)&lt;/strong&gt; : The percentage of sentences with at least one word error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Character Error Rate (CER) :&lt;/strong&gt; Similar as WER with the smallest units as characters and not words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next two error rates are variations of WER, are are bounded between [0,1].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Match Error Rate (MER)&lt;/strong&gt; [3] : Measures the probability of a given match being incorrect.&lt;/li&gt;
&lt;/ul&gt;

\[MER = \frac{I+S+D}{I+S+D+H} = 1 - \frac{H}{I+S+D+H}\]

&lt;p&gt;where,&lt;br /&gt;
H = number of hits
N = total number of words in the actual transcript&lt;/p&gt;

&lt;p&gt;MER is always &amp;lt;= WER.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Word Information Lost&lt;/strong&gt; &lt;strong&gt;(WIL)&lt;/strong&gt; [3] : Information theoretic measure based on entropy. For more details look at the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example values, comparing WER, MER and WIL are shown below:&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/evaluating-asr/mer.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Taken from [3].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note : For implementations of WER, WIL, and MER, have a look at &lt;a href=&quot;https://pypi.org/project/jiwer/&quot;&gt;Jiwer&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;additional-error-analysis&quot;&gt;Additional Error Analysis&lt;/h2&gt;

&lt;p&gt;Apart from looking at just single metrics for evaluating an ASR there are few additional metrics that one should use to evaluate a goal oriented ASR deployed for a given application:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which speaker demographic is most often misrecognised ?&lt;/li&gt;
  &lt;li&gt;What (context-dependent) phones are least well recognised ?&lt;/li&gt;
  &lt;li&gt;Which words are most confused ? Generate a confusion matrix of confused words.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These often help prioritise improvements in terms of what might benefit the goal the most.&lt;/p&gt;

&lt;h1 id=&quot;semantics-based-metrics&quot;&gt;Semantics based Metrics&lt;/h1&gt;

&lt;p&gt;WER doesn’t look at the meaning of what has been transcribed despite the fact that is the semantics that are most relevant in an ASR that is a part of a spoken dialogue system.&lt;/p&gt;

&lt;h2 id=&quot;concept-accuracy&quot;&gt;Concept accuracy&lt;/h2&gt;

&lt;p&gt;It is a simple metric that looks at the accuracy of the concepts that are of relevance in the transcripts.&lt;/p&gt;

&lt;p&gt;Example:&lt;br /&gt;
Reference - I want to go from Boston to Baltimore on September 29
Hypothesis - Go from Boston to Baltimore on December 29&lt;/p&gt;

&lt;p&gt;The WER is \(45\%\). However if one looks at concept accuracy, it is 2/3. Out of the 3 concepts : “Boston”, “Baltimore” and “September 29” it gets 2 of them right.&lt;/p&gt;

&lt;h2 id=&quot;wer-with-embeddings&quot;&gt;WER with embeddings&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;this paper&lt;/a&gt; [9] they look at augmenting the WER metric for an ASR that is used for a Spoken Language Translation (SLT) task.&lt;/p&gt;

&lt;p&gt;They argue that some morphological operations, like adding a plural doesn’t impact the translation task and that such substitution errors should be penalised differently. To decide which ones to penalise they use word embeddings. They call their new metric, WER-E i.e. WER with embeddings. The only change in this metric is that the substitution cost in WER is replaced by the cosine distance between the two words, so near identical words get assigned a very low cost.&lt;/p&gt;

&lt;h2 id=&quot;other-metrics&quot;&gt;Other Metrics&lt;/h2&gt;

&lt;p&gt;There are lots of different papers that augment the WER metric or introduce a new metric specific to a downstream task. Mentioning a few of them below :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In [10] a new measure called Automatic Transcription Evaluation for Named Entity (ATENE) is introduced for the NER downstream task&lt;/li&gt;
  &lt;li&gt;3 new evaluation metrics are introduced in [12], where the downstream application is Information Retrieval&lt;/li&gt;
  &lt;li&gt;SemDist metric is introduced in [13] for downstream SLU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Downstream applications aside, it is quite pertinent to evaluate and benchmarks ASR across different social and demographic groups to evaluate bias and fairness. Systems don’t exist in isolation from the society in which they are deployed in and it is important that ML Engineers pay head to such metrics while deploying ASRs.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://arxiv.org/abs/2106.06519&quot;&gt;N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing (3rd ed. draft)&lt;/a&gt; by &lt;a href=&quot;http://web.stanford.edu/people/jurafsky/&quot;&gt;Dan Jurafsky&lt;/a&gt; and &lt;a href=&quot;http://www.cs.colorado.edu/~martin/&quot;&gt;James H. Martin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.researchgate.net/profile/Phil-Green-4/publication/221478089_From_WER_and_RIL_to_MER_and_WIL_improved_evaluation_measures_for_connected_speech_recognition/links/00b4951f95799284d9000000/From-WER-and-RIL-to-MER-and-WIL-improved-evaluation-measures-for-connected-speech-recognition.pdf&quot;&gt;From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition&lt;/a&gt; (2004)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.4023&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Automatic Human Utility Evaluation of ASR Systems: Does WER Really Predict Performance&lt;/a&gt;? (2013)&lt;/p&gt;

&lt;p&gt;[5] : http://www.cs.columbia.edu/~julia/courses/CS4706/asreval.pdf&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3447548.3467372&quot;&gt;Meaning Error Rate: ASR domain-specific metric framework&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.424&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Is word error rate a good indicator for spoken language understanding accuracy&lt;/a&gt; (2003)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1877050918302187&quot;&gt;Automatic speech recognition errors detection and correction: A review&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01350102/file/metrics_correlation_asr-smt.pdf&quot;&gt;Better Evaluation of ASR in Speech Translation Context Using Word Embeddings&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1289.pdf&quot;&gt;How to Evaluate ASR Output for Named Entity Recognition ?&lt;/a&gt; (2015)&lt;/p&gt;

&lt;p&gt;[11] : &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/5947637&quot;&gt;Why word error rate is not a good metric for speech recognizer training for the speech translation task?&lt;/a&gt; (2011)&lt;/p&gt;

&lt;p&gt;[12] : &lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/44281782/Evaluating_ASR_Output_for_Information_Re20160331-31804-1u2wdv8.pdf?1459484689=&amp;amp;response-content-disposition=inline%3B+filename%3DEvaluating_ASR_Output_for_Information_Re.pdf&amp;amp;Expires=1641991659&amp;amp;Signature=DMkXBKRcISYtsJw2M6l9P4Lth-0ZEF6plQyHD08TWaIhRZSaZdFCmtTagKLx7YMLGX~ZxvQtgiQe4EHJcZ-aGL5DiUh3Vfztn7feWDMZF~bEgFMluedYI6Jq39t0BBd2mVJjuUVCVtx1-S--pH89PQ9aFcfpbSH4W88uytgZHwXyZyeR9tIVzM45lblIVeFtvVNwREc6jmm1ijW4ir0lGbGlfI2DoLXwyYFM6pltQHngtoVtAfBrBF3XOMB2AwXA0hQkSDlO0v~iwletUK1o3xBdmb6MrK47A7nt8TlO9xFB33RA8hrN-KnafvJGdhI-Or8Ic2HN4cWlXvr~2uetNg__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=19&quot;&gt;Evaluating ASR Output for Information Retrieval&lt;/a&gt; (2007)&lt;/p&gt;

&lt;p&gt;[13] : &lt;a href=&quot;https://arxiv.org/pdf/2104.02138.pdf&quot;&gt;Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[14] : &lt;a href=&quot;https://aclanthology.org/W13-4064.pdf&quot;&gt;Which ASR should I choose for my dialogue system?&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[15] : &lt;a href=&quot;https://arxiv.org/pdf/2010.11745.pdf&quot;&gt;Rethinking evaluation in ASR : Are out models robust enough ?&lt;/a&gt; (2021)&lt;/p&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="WER" /><category term="sticky" /><summary type="html">An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/evaluating-asr.jpg" /><media:content medium="image" url="/assets/images/evaluating-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Complexity of Conversations - I</title><link href="/complexity-of-conversations/" rel="alternate" type="text/html" title="Complexity of Conversations - I" /><published>2022-01-18T00:00:00+00:00</published><updated>2022-01-18T00:00:00+00:00</updated><id>/complexity-of-conversations</id><content type="html" xml:base="/complexity-of-conversations/">&lt;p&gt;Consider a restaurant booking voice bot built using a frames and slots approach.
While this can easily &lt;em&gt;solve the problem&lt;/em&gt; of booking with high automation
accuracy, such slot-filling framework can’t carry on a meaningful conversation
in a debate unless you over-engineer the frames and slots to monstrous
complexity. Booking a restaurant is a form of conversation that’s innately
simpler than arguing with someone in a debate competition. We can roughly say
that these two conversations lie in different complexity classes. In this first
post of a series, we will lay down a few factors that will help us define a map
of conversations arranged according to their complexities.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;At Skit we build many kinds of task-oriented dialog systems for call center
automation. A very crude categorization of such systems, for us, is based on the
interaction with a sibling call handling system&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and the direction of
intention, user or agent initiation.&lt;/p&gt;

&lt;p&gt;While we have used multiple approaches to measure difficulty of conversations
for our product delivery purposes, it’s interesting to see if a purer framework
could be built around this. Similar to computational complexity, this can tell
us which problems are tractable under an algorithm. It can also help in
identifying the path towards the next generation of human machine conversational
systems.&lt;/p&gt;

&lt;p&gt;We will cover a few thoughts around a few core constructs of the framework next.
First is the definition of success in a conversation, second around the
difficulty of doing so, and third about the algorithms and their complexities.&lt;/p&gt;

&lt;h2 id=&quot;success&quot;&gt;Success&lt;/h2&gt;

&lt;p&gt;The definition of success of a conversation depends on alignment between goals
of the involved parties.&lt;/p&gt;

&lt;p&gt;A regular goal oriented conversation with user initiation has a simple success
definition. For example, a call with user asking for temperature of a place can
be called successful if the temperature is provided. The metric here could be
something like the following:&lt;/p&gt;

\[\text{Resolution%} = \frac{\text{Calls where user goals were met}}{\text{Total calls}}\]

&lt;p&gt;This simple formulation becomes tricky as the alignment between user and bot
goals becomes inexact. For example when the &lt;em&gt;bot is calling&lt;/em&gt; the user for
payment reminders, it might not just want to remind and collect next reminder
time, but also want to persuade the users to pay as early as possible. In such
cases, you might want to use another rate for &lt;em&gt;favorable outcomes&lt;/em&gt;:&lt;/p&gt;

\[\text{Favorable%} = \frac{\text{Calls with favorable outcomes}}{\text{Resolved calls}}\]

&lt;p&gt;Another example where this works is in &lt;em&gt;argumentative&lt;/em&gt; conversations where
holding a reasonable conversation and reaching conclusion is important
(&lt;em&gt;resolution&lt;/em&gt;), but winning the argument (the favorable outcome) is what defines
success.&lt;/p&gt;

&lt;h2 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h2&gt;

&lt;p&gt;We can look at difficulty of conversations from multiple levels. For the
smallest unit of dialog, a turn&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, parsing and generating every utterance in a
conversation can be rated for difficulty. Here are a few factors that drive
difficulty for a turn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Knowledge needed for understanding an entity. This could be general or
specific to a situation, involving connection with a dynamic or static
knowledge source.&lt;/li&gt;
  &lt;li&gt;Speech Acts. Simpler acts like &lt;em&gt;greeting&lt;/em&gt; are easier to handle, while
something like &lt;em&gt;pleading&lt;/em&gt; is hard.&lt;/li&gt;
  &lt;li&gt;Expression complexity, intentional or unintentional. For speech systems, this
is even more varied because of the richness of acoustic signals that adds to
the underlying text. For example sarcasm could be expressed by changing the
tone of speech and not just via textual constructs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But these are not sufficient since higher order behaviors across multiple turns
also make conversations difficult. As an example, consider negotiation for the
price in a market. In this situation, you need to use the conversational context
across turns to decide your next steps in a way that’s harder than situations
where context dependency is lesser.&lt;/p&gt;

&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;

&lt;p&gt;The frameworks of developing, and running, voice bots are the last pieces that
will help us to map out the tractability of problems. A common method in the
industry is the frame-filling model that roughly needs learning &lt;em&gt;intents&lt;/em&gt; and
&lt;em&gt;entities&lt;/em&gt; for each utterance.&lt;/p&gt;

&lt;p&gt;These frameworks, or algorithms, can be measured on their resource consumption.
We can start with sample complexity of conversations as the resource and create
statements like the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Under framework \(f\), you need an order of \(N\) data points to supervise a
voice bot of class \(k\) to achieve a success rate of \(R(N)\)&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be mapped to the statistical learning problem and abstractions can be
translated from there.&lt;/p&gt;

&lt;p&gt;Having set the groundwork here, we will tackle the more interesting problem of
complexity class definitions in a later post.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The sibling system could be non-existent, or backend human agents who can
take over the more co plex conversations, or complex parts of running
conversations. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We can cover backchannel events also in a kind of &lt;em&gt;background&lt;/em&gt; turn. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Including the other factors around PAC learning. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><summary type="html">Consider a restaurant booking voice bot built using a frames and slots approach. While this can easily solve the problem of booking with high automation accuracy, such slot-filling framework can’t carry on a meaningful conversation in a debate unless you over-engineer the frames and slots to monstrous complexity. Booking a restaurant is a form of conversation that’s innately simpler than arguing with someone in a debate competition. We can roughly say that these two conversations lie in different complexity classes. In this first post of a series, we will lay down a few factors that will help us define a map of conversations arranged according to their complexities.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On using ASR Alternatives for a Better SLU</title><link href="/on-using-asr-alternatives-for-a-better-slu/" rel="alternate" type="text/html" title="On using ASR Alternatives for a Better SLU" /><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>/on-using-asr-alternatives-for-a-better-slu</id><content type="html" xml:base="/on-using-asr-alternatives-for-a-better-slu/">&lt;p&gt;This blog discusses some concepts from the recently published &lt;a href=&quot;https://arxiv.org/pdf/2106.06519.pdf&quot;&gt;paper&lt;/a&gt; by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in &lt;a href=&quot;https://2021.aclweb.org/&quot;&gt;ACL-IJCNLP’21&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Voice bots in the industry heavily rely on the use of Automatic Speech Recognition (ASR) transcripts to understand the user and capture intents &amp;amp; entities which are then used to resolve the customer’s problem. ASR’s however are far from perfect, especially on noisy real world data and on instances with acoustic confusion. The downstream Spoken Language Understanding (SLU) components would benefit greatly if they take the ASR’s confusion into account.&lt;/p&gt;

&lt;p&gt;Example use-case with confounding ASR transcripts (over voice):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Bot: When would you like to make the reservation ?
User (actually said): right now
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ASR alternatives for given user’s speech:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- like, now
- right now
- write no
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Often the downstream SLU services which act on ASR transcripts use only the most probable alternative (also called 1-best alternative), thereby leaving out a lot of other information that exists in the form of alternative probabilities. This paper presents a simple way of using the information that exists in the alternatives to get SOTA performance on a standard benchmark for a SLU system.&lt;/p&gt;

&lt;h1 id=&quot;types-of-asr-outputs&quot;&gt;Types of ASR Outputs&lt;/h1&gt;

&lt;p&gt;Before we get into how to best use ASR’s confusion to increase the performance of the SLU, we discuss the different ways an ASR outputs the probable word sequence probabilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N-best alternatives : this is a list of the top N alternative sentences for the given spoken utterance. These are usually ranked based on probability of occurrence.&lt;/li&gt;
  &lt;li&gt;Word lattices : As shown in the below figure, they have no a-priori structure per se. Every path from the start node to the end node represents a possible hypothesis transcript. Every transition adds a word to the hypothesis.&lt;/li&gt;
  &lt;li&gt;Word confusion networks : They are a more compact and normalised topology for word lattices. They enforce certain constaints such that competing words should be in the same group and there by also ensure aligment of words that occur at the same time interval. Though these graphs capture lesser number of possibilities, this topology can be used to get as high recognition accuracy as using word lattices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The transitions probabilities of both, word lattices and word confusion networks are weighted by the acoustic and language model probabilities.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/word-lattices.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 1: The structure of a word lattice as contrasted with a word confusion network.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;dataset&quot;&gt;Dataset&lt;/h1&gt;

&lt;p&gt;The task that is used to compare the modelling approaches is the &lt;a href=&quot;https://aclanthology.org/W14-4337.pdf&quot;&gt;DSTC - 2 challenge&lt;/a&gt; where one is required to predict intent act-slot-value triplets. Pairs of sentences i.e. a sentence whose intent is required to be predicted along with its context is given. For each sentence the top 10 best ASR alternatives are also provided.&lt;/p&gt;

&lt;h1 id=&quot;modelling-approach&quot;&gt;Modelling Approach&lt;/h1&gt;

&lt;p&gt;The central idea of the paper is to leverage pre-trained transformer models BERT and XLMRoBERTa and fine-tune them with a simple input representation shown below. The input consists of the ASR alternatives seperated by a seperator token concatenated with the context. A segment id (not shown below) is also used, which is used to contrast the context (in green) with the alternatives (in purple).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/example-input.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Input representation used to fine-tune transformer models.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;On top of the transformer model a semantic tuple classifier (STC) is applied to predict the act-slot-value triplets. Using this approach, we achieve a performance equivalent to the prior state-of-the-art model on DSTC-2 dataset. We get comparable F1 and SOTA accuracy. The previous SOTA model, WCN-BERT uses word confusion networks.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/model-architecture.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Model Architecture, The input representation is encoded by a transformer model which forms an
input for a Semantic Tuple Classifier (STC). STC uses binary classifiers to predict the presence of act-slot pairs,
followed by a multi-class classifier that predicts the value for each act-slot pair.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Here, using a simple ASR output such an N-best alternatives we get a comparable performance to the SOTA model that uses a much more informative probability graph such as a word confusion network.&lt;/p&gt;

&lt;h1 id=&quot;ablation-experiments&quot;&gt;Ablation Experiments&lt;/h1&gt;

&lt;p&gt;Two ablation experiments are performed. One on low data regimes and another to check the impact of the context on performance.&lt;/p&gt;

&lt;h2 id=&quot;low-data-regime&quot;&gt;Low Data Regime&lt;/h2&gt;

&lt;p&gt;Here, the baseline models are compared with our approach using 5%, 10%, 20% and 50% of the training data respectively. In all these situations our approach beats SOTA by a considerable margin, proving that our training approach effectively transfer learns. We hypothesise, that this is due to the structural similarity between the input representations of the initial training of these open sourced models and the fine-turning that is done on DSTC-2 dataset. It also demonstrates that n-best alternatives are a more natural representation to fine-tune transformer models compared to word lattices or word confusion networks.&lt;/p&gt;

&lt;h2 id=&quot;context-dependency&quot;&gt;Context Dependency&lt;/h2&gt;

&lt;p&gt;In this experiment we wanted to test the impact of adding context (last turn) to performance and to check if it is relevant at all. An improvement of around ~1.5% F1 score is obtained using context. An example situation where context is relevance is shown below. Dialog context can help in resolving ambiguities in parses and reducing the impact of ASR noise.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/n-best-asr/context-dependence.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Example to demonstrate context dependence.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Lastly, this methodology can be used by users of third-party ASR APIs which do not provide word-lattice information and thereby is more accessible.&lt;/p&gt;

&lt;h1 id=&quot;implementation-and-citation&quot;&gt;Implementation and Citation&lt;/h1&gt;

&lt;p&gt;The code for the project can be found &lt;a href=&quot;https://github.com/skit-ai/N-Best-ASR-Transformer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you use our work, please cite using the following BibTex Citation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{ganesan-etal-2021-n,
    title = &quot;N-Best {ASR} Transformer: Enhancing {SLU} Performance using Multiple {ASR} Hypotheses&quot;,
    author = &quot;Ganesan, Karthik  and
      Bamdev, Pakhi  and
      B, Jaivarsan  and
      Venugopal, Amresh  and
      Tushar, Abhinav&quot;,
    booktitle = &quot;Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)&quot;,
    month = aug,
    year = &quot;2021&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2021.acl-short.14&quot;,
    doi = &quot;10.18653/v1/2021.acl-short.14&quot;,
    pages = &quot;93--98&quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Tür, G., Deoras, A., &amp;amp; Hakkani-Tür, D. (2013, September). Semantic parsing using word confusion networks with conditional random fields. In &lt;em&gt;INTERSPEECH&lt;/em&gt; (pp. 2579-2583).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="ASR" /><category term="SLU" /><summary type="html">This blog discusses some concepts from the recently published paper by members of the ML team at Skit (formerly Vernacular.ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in ACL-IJCNLP’21.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/n-best-asr.jpg" /><media:content medium="image" url="/assets/images/n-best-asr.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Authentication in gRPC</title><link href="/authentication-in-grpc/" rel="alternate" type="text/html" title="Authentication in gRPC" /><published>2021-10-31T00:00:00+00:00</published><updated>2021-10-31T00:00:00+00:00</updated><id>/authentication-in-grpc</id><content type="html" xml:base="/authentication-in-grpc/">&lt;p&gt;In gRPC, there are a number of ways you can add authentication between client
and server. It is handled via Credentials Objects.&lt;/p&gt;

&lt;p&gt;There are two types of credential objects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Channel Credentials&lt;/strong&gt;: These are handled on the channel level, i.e when
the connection is established and a channel is created.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Call Credentials&lt;/strong&gt;: These are handled on per request level, i.e for every
RPC call that is made. These Credential objects can also be combined to
create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CompositeChannelCredentials&lt;/code&gt; with one Channel Credential and one
Call Credential object.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let us see how we can use these credential objects.&lt;/p&gt;

&lt;h2 id=&quot;client-side-tlsssl-authentication&quot;&gt;Client-Side TLS/SSL Authentication&lt;/h2&gt;
&lt;p&gt;gRPC provides a way to establish a connection without any secure connection i.e just like HTTP.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithInsecure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For secure communication, we will create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransportCredentials&lt;/code&gt; which is a type
of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ChannelCredential&lt;/code&gt; object.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewClientTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:5000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// server.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;localhost:50051&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServerTLSFromFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Serve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about generating own ssl certificates &lt;a href=&quot;https://www.linuxjournal.com/content/understanding-public-key-infrastructure-and-x509-certificates&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the case where you don’t own the client, it means you are creating a gRPC
API for public use, you cannot give your certificate to everyone using your
client. In that case, we rely on well known &lt;a href=&quot;https://en.wikipedia.org/wiki/Certificate_authority&quot;&gt;Certificate
Authority&lt;/a&gt; like LetsEncrypt, Amazon, etc. to generate a
certificate. So let us change our client code a little.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;InsecureSkipVerify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithTransportCredentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewTLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;// server code remains the same&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case what happens is that grpc loads the certificates of well-known
Certificate Authorities from the OS and sends it to the server, hence no need
to manually provide a certificate.&lt;/p&gt;

&lt;h2 id=&quot;token-based-authentication--oauth2&quot;&gt;Token-Based Authentication / OAuth2&lt;/h2&gt;

&lt;p&gt;Many a time we want to differentiate a client by issuing them different tokens.
TLS Authentication is a good way to secure your connection but it does not tell
us from which client the request is coming from. We will send the token in
request metadata just like HTTP Headers.&lt;/p&gt;

&lt;p&gt;gRPC has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google.golang.org/grpc/metadata&lt;/code&gt; package to send and receive metadata
with a RPC request.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// client.go&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// add metadata to request context&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Bearer xxx-xxx-xxx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// use this context to call rpc&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CallRPC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requestObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnaryInterceptor&lt;/code&gt; on the server which acts as middleware and
checks for the token for all the requests.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// create a middleware&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryServerInfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FromContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;missing context metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  
    &lt;span class=&quot;c&quot;&gt;// Take care: grpc internally reduce key values to lowercase&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;authorization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xxx-xxx-xxx&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Errorf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unauthenticated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;invalid token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// pass this when creating server&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnaryInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AuthInterceptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But above code only works for non-streaming RPCs. For Streaming RPCs you can
implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StreamInterceptor&lt;/code&gt;. Instead of implementing it again you can use
this package. I hope this article helps you with authentication in gRPC.&lt;/p&gt;

&lt;p&gt;If you are interested in working with cutting-edge technologies, come work with
&lt;a href=&quot;https://skit.ai&quot;&gt;Skit&lt;/a&gt;. Apply &lt;a href=&quot;https://skit.recruiterbox.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Deepankar Agrawal</name></author><category term="Engineering" /><category term="authentication" /><category term="gRPC" /><summary type="html">In gRPC, there are a number of ways you can add authentication between client and server. It is handled via Credentials Objects.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/thumbnail-authentication-in-grpc.svg" /><media:content medium="image" url="/assets/images/thumbnail-authentication-in-grpc.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>