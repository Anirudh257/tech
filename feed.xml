<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-10-20T15:06:40+00:00</updated><id>/feed.xml</id><title type="html">Skit Tech</title><subtitle>Speech Technology from Skit</subtitle><entry><title type="html">Seminar - Code Mixing in NLP and Speech</title><link href="/Code-Mixing-Seminar/" rel="alternate" type="text/html" title="Seminar - Code Mixing in NLP and Speech" /><published>2021-08-24T00:00:00+00:00</published><updated>2021-08-24T00:00:00+00:00</updated><id>/Code-Mixing-Seminar</id><content type="html" xml:base="/Code-Mixing-Seminar/">&lt;p&gt;Below are some pointers and insights from the papers that we covered in the recently concluded &lt;a href=&quot;https://skit-ai.github.io/seminars/topics/&quot;&gt;seminar on Code-mixing in NLP and Speech&lt;/a&gt;. During the seminar we covered 6 papers, two each from NLP, Speech Synthesis and Speech Recognition.&lt;/p&gt;

&lt;h1 id=&quot;session-1-nlp&quot;&gt;Session-1 (NLP)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2004.12376.pdf&quot;&gt;GLUECoS : An Evaluation Benchmark for Code-Switched NLP&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shashank Shailabh&lt;/p&gt;

&lt;p&gt;The main idea of GLUECoS is to introduce a benchmark evaluation task set for code-switched NLP. It is inspired from GLUE (Generalized Language Evaluation Benchmark) and covers tasks such as Language Identification (LID),  Part-Of-Speech (POS) Tagging, Named Entity Recognition (NER), Question &amp;amp; Answering (QA), Natural Language Inference (NLI) and sentiment analysis (SENT).  and also introduces benchmark code-mixed datasets. The language pairs that this benchmark covers in English-Spanish and English-Hindi code-mixed datasets. The paper also introduces several Metrics, most of which are covered in the previous blog on Metrics for Code-switching.&lt;/p&gt;

&lt;p&gt;To arrive at a baseline performance, several cross-lingual word embeddings models such as MUSE (supervised and unsupervised embeddings), BiCVM, Biskip and GCM embeddings are used. For supervised cross-lingual embeddings one requires a parallel mono-lingual corpus of the two languages such that each sentence in one language maps as close as possible to the parallel sentence in the other language. In the unsupervised setting an adversarial loss is used to train cross-lingual embeddings. The other model that is used for the baseline is multilingual models (mBERT). This model is simply trained on monolingual data from each language, and has no explicit loss to map the embedding of the same words from both languages to the same point. Another fine-tuned version of mBert is used which is fine-tuned on synthetically generated code-switched data. Interestingly the mBert model performs better as compared the cross-lingual embeddings and fine-tuning on synthetic data further improves performance.&lt;/p&gt;

&lt;h1 id=&quot;session-2-speech-synthesis&quot;&gt;Session-2 (Speech Synthesis)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2008.00768v1.pdf&quot;&gt;Building Multilingual End-to-End Speech Synthesisers for Indian Languages&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Swaraj&lt;/p&gt;

&lt;p&gt;The goal of the paper is to build a Multilingual TTS for Indic languages in a low data resource setting. The paper discusses ways of assimilating inputs from different indian scripts both at the level of characters and phonemes, into a common input set. This leverages the similarities in phonemes and the structure of Indian languages which fall in the Abjad class of languages where vowels are diacritics.&lt;/p&gt;

&lt;p&gt;For phonemes, it looks at 2 approaches : a transliteration approach using the unified parser and the common label set; and another approach that considers a 1:1 phoneme map between the different languages. At character level they present the MLCM character set that is 68 characters in total and combines 8 indic scripts. Since vowels are a major component in indic languages, they consider approaches where the vowel and vowel modifier are mapped to the same and different input token. For modelling they use tacotron-2 and compare the impact of various input tokens on performance for monolingual and multilingual settings. The primary take away from the discussion were the different ways of combining and varying the input representations for training an Indic language TTS.&lt;/p&gt;

&lt;h1 id=&quot;session---3-nlp&quot;&gt;Session - 3 (NLP)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://aclanthology.org/2021.adaptnlp-1.12.pdf&quot;&gt;BERTologiCoMix, How does Code-Mixing interact with Multilingual BERT?&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Jaivarsan B&lt;/p&gt;

&lt;p&gt;This paper addresses the impact (on code-mixed tasks) of fine-tuning with different types of code-mixed data and outlines the changes to mBERT’s attention heads during such fine-tuning. Code-mixing can be treated as a domain adaptation problem, but in this particular context it is to adapt at the grammatical level, not purely at the vocabulary level or style level. For sake of empirical study they evaluate two different language pairs, English-Spanish (enes), English-Hindi (enhi) with the extra three different varieties on them. Those three varieties are namely: randomly-ordered CM (l-CM), grammatically appropriate CM (g-cm) and real-world CM (r-CM), so in total we have 2 lang pairs \(\times\) 3 varieties = 6 possible fine-tuning datasets on mBERT against the plain mBERT.&lt;/p&gt;

&lt;p&gt;They evaluate these 6 combinations of CM datasets on GLUECoS benchmark. They empirically find out that naturally occurring code-mixed data (r-CM) brings in the best performance improvement after fine-tuning, against synthetic fine-tunings (l-CM, g-CM) and even plain mBERT. Other task specific observations include: fine-tuning with r-CM data helps with SENT, NER, LID and QA CM tasks. General trend observed was easier tasks like LID, POS are solved in earlier layers and as complexity of tasks increases, the effective layer moves deeper. This paper’s other significant contribution is to visualize the differences to these mBERT attention heads post-CM-fine-tuning. They use three methods (including one of their own contribution), which try to answer questions related to: Has anything changed within the models due to pre-training with CM datasets? Attention patterns of which heads have changed? How do attention heads respond to code-mixed probes?. There are changes to these models, which show these attention heads are receptive to CM specific data at different layers of mBERT for different tasks and different language pairs.&lt;/p&gt;

&lt;p&gt;The most important finding from these probing experiments is that there are discernible changes introduced in the models due to exposure to CM data, of which a particularly interesting observation is that this exposure increases the overall responsivity of the attention heads to CM.&lt;/p&gt;

&lt;h1 id=&quot;session---4-speech-recognition&quot;&gt;Session - 4 (Speech Recognition)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://www.researchgate.net/profile/Karan-Taneja/publication/335829565_Exploiting_Monolingual_Speech_Corpora_for_Code-Mixed_Speech_Recognition/links/602571e3299bf1cc26bcbce9/Exploiting-Monolingual-Speech-Corpora-for-Code-Mixed-Speech-Recognition.pdf&quot;&gt;Exploiting Monolingual Speech Corpora for Code-mixed Speech Recognition&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kriti Anandan&lt;/p&gt;

&lt;p&gt;This paper aims to mitigate the problem of scarcity in code-mixed datasets for training ASR models robust to code-mixed data. It introduces two algorithms to synthetically generate code mixed data by using annotated monolingual data sets that are available in large quantities and a small amount of annotated real code-mixed data. Both these algorithms make use of probability distributions derived from real code-mixed data to imitate their characteristics. The code-mixed sentences are framed from sentence fragments extracted from the monolingual corpus. In the paper, first algorithm is a naïve approach that uses the code-mixed language span distributions to weave fragments of two different languages in an alternate fashion where fragment lengths are picked by sampling the distribution. The second algorithm uses two distributions that try to model phone transitions across fragments and within fragments.&lt;/p&gt;

&lt;p&gt;The authors also demonstrate the usefulness of these synthetically generated datasets by adding them to the existing monolingual dataset for training the acoustic model of an ASR. Results from the best acoustic model show a ~ 6.85 point drop in WER score. They also conduct some language modelling experiments by using code-mixed transcripts to train the language model of the ASR. The best system uses the best acoustic model trained from the previous experiment and the language model trained on the synthetic text generated by all the algorithms presented, as well as the real code-mixed text. The results via this method show a ~ 8.17 point drop in the WER score.&lt;/p&gt;

&lt;h1 id=&quot;session---5-speech-recognition&quot;&gt;Session - 5 (Speech Recognition)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/pdf/2006.00782.pdf&quot;&gt;Learning to recognize code switched speech without forgetting monolingual speech recognition&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shahid Nagra&lt;/p&gt;

&lt;p&gt;The goal of the paper is to fine tune ASR models on code switched speech without affecting the performance on the same models on monolingual speech. They propose the use of a learning without forgetting algorithm. This approach can also be used when one doesn’t have access to the monolingual data that the model was trained on, which is often the case for open-source ASR models.&lt;/p&gt;

&lt;p&gt;They conducted 5 experiments where training and fine-tuning is varied across the 3 datasets, monolingual and code-mixed and pooled. The best model performance is got when fine-turning a model on pooled data (code-mixed + monolingual) where only 25% of code-mixed data is used. An extra KL Diverge loss between the fine-tuned and the pre-trained model is added to prevent the increase in WER on the monolingual data. A metric poWER is also introduced which measures the Levenstein distance between the phonemic representation of utterances which takes care of measuring WER in the code-mixed setting.&lt;/p&gt;

&lt;h1 id=&quot;session---6-speech-synthesis&quot;&gt;Session - 6 (Speech Synthesis)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://www1.se.cuhk.edu.hk/~hccl/publications/pub/Icassp20_cstts_camera_ready.pdf&quot;&gt;Code-Switched Speech Synthesis Using Bilingual Phonetic Posteriorgram with Only Monolingual Corpora&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Shangeth Rajaa&lt;/p&gt;

&lt;p&gt;This paper aims to synthesize fluent code-switched speech using only monolingual speech corpora and explores the usage of Bilingual Phonetic Posteriorgram (PPG) as the speech representation. PPGs are the posterior probabilities of each phonetic class for a specific frame of one utterance, stacking up the PPG for two languages forms the bilingual PPG. As obtaining a bilingual/code-switched speech corpus for TTS training can be expensive, this paper solves the problem, by training a tacotron2 based code-switched TTS system with only the monolingual corpus of both the languages (English and Chinese) where each language is spoken by a different speaker. The trained model is then evaluated by generating speech for a code-switched text corpus with Mean Opinion Score (MOS) score.&lt;/p&gt;

&lt;p&gt;The paper introduces the use of Bilingual PPG as a representation of speech as PPGs are speaker-independent and can capture the context of the speech better, PPG features are computed with a pretrained ASR model for each language. They replaced the traditional acoustic model which predicts the acoustic features(Mel Spectrogram) directly from text, to predict the PPG from the text, followed by predicting the Mel Spectrogram from the predicted PPG, speaker embedding and language embedding. The proposed method also includes a residual encoder to encode the speech into a latent space in a Variational AudioEncoder (VAE) setting which helps to model the prosodic features of the audio. The proposed method was compared with a tacotron2 based text to acoustic feature (Mel spectrogram) model baseline. As the proposed method uses a PPG feature, the model was able to disentangle the context information of the speech signal across different speakers and synthesize better code-mixed speech by combining the PPG feature, speaker embedding, language embedding, and prosody features.&lt;/p&gt;

&lt;p&gt;From the experimental results, we can observe that the proposed method can synthesize high intelligible code-switched speech with a cross-lingual speaker (eg: Chinese speaker for English/code-switched text) with only training on a monolingual corpus. But the proposed method failed to beat the baseline in speech fidelity MOS score. The authors stated that the Bilingual PPG extracted was not accurate enough and needs to be further studied in future work.&lt;/p&gt;

&lt;p&gt;Do check out the blog on &lt;a href=&quot;https://tech.skit.ai/Code-Mixing-Metrics/&quot;&gt;Code-Mixing metrics&lt;/a&gt; if you haven’t already. Please write to us @ machine-learning@skit.ai in case you want to join any of our future seminars.&lt;/p&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="asr" /><category term="tts" /><category term="code-mixing" /><summary type="html">Below are some pointers and insights from the papers that we covered in the recently concluded seminar on Code-mixing in NLP and Speech. During the seminar we covered 6 papers, two each from NLP, Speech Synthesis and Speech Recognition.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/tile-work.png" /><media:content medium="image" url="/assets/images/tile-work.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Code Mixing Metrics</title><link href="/Code-Mixing-Metrics/" rel="alternate" type="text/html" title="Code Mixing Metrics" /><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>/Code-Mixing-Metrics</id><content type="html" xml:base="/Code-Mixing-Metrics/">&lt;p&gt;We at skit, recently concluded a seminar series on code-mixing, where we covered research papers that looked at approaches to deal with code-mixed data across ASR, TTS and SLU related services. Code-mixing is a common phenomena in the Indian subcontinent. Code-mixing refers to the situation when two or more languages are present in an utterance or a corpus. It is important to formulate, discuss and deliberate on metrics because, it is improvement on metrics across benchmark datasets that spur continuous research in a field.&lt;/p&gt;

&lt;h1 id=&quot;terminologies&quot;&gt;Terminologies&lt;/h1&gt;

&lt;p&gt;Because how can one study a phenomena without having a requisite vocabulary.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Matrix Language :&lt;/strong&gt; The Matrix Language-Frame (MLF) model is one of the dominant models to analyse code-switching. In this framework there is a Matrix Language and an Embedded Language. The embedding language is inserted into the mono-syntactic frame of the Matrix language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inter-sentential switching :&lt;/strong&gt;  Occurs &lt;em&gt;outside&lt;/em&gt; the sentence or the clause level&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intra-sentential switching&lt;/strong&gt; : Occurs &lt;em&gt;within&lt;/em&gt; a sentence or a clause&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intra-word switching&lt;/strong&gt; : Occurs &lt;em&gt;within&lt;/em&gt; a word itself, sometimes at phoneme boundaries example : “I’m chalaaoing a car” → “i am driving a car”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Language Span:&lt;/strong&gt; The number of monolingual words between 2 consecutive switch points in the corpus. The language span distribution is the aggregate of all such language spans into a discrete pdf.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The performance of models on code-mixed corpora, would depend on the level of code-mixing. A minuscule level of code-mixing could be treated as noise/OOV and ignored. However, as the level of code-mixing increases, the performance of the model will vary. Therefore it is important to quantify the extent of code-mixing in a corpora; “how much” and
“how often”. In the first &lt;a href=&quot;https://arxiv.org/pdf/2004.12376.pdf&quot;&gt;paper&lt;/a&gt; covered in the code-mixing seminar, there were several metrics presented, however they were not discussed in detail. We discuss them below.&lt;/p&gt;

&lt;h1 id=&quot;measuring-the-amount-of-code-mixing&quot;&gt;Measuring the amount of Code-Mixing&lt;/h1&gt;

&lt;h2 id=&quot;word-frequency-based-metrics&quot;&gt;Word-frequency based metrics&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.727.9087&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;&lt;strong&gt;Code-Mixing Index (CMI)&lt;/strong&gt;&lt;/a&gt; : An utterance level metric that measures the fraction of tokens(words) that are not from the matrix language.&lt;/p&gt;

\[CMI = 100*(1 - \frac{max(w_i)}{n-u}) \text{ if } n&amp;gt;u\]

&lt;p&gt;where,&lt;br /&gt;
n = total number of words, irrespective of language&lt;br /&gt;
u = language independent words&lt;br /&gt;
\(w_i\) = number of words in language i&lt;br /&gt;
\(max(w_i)\) measures the number of words in the matrix language&lt;/p&gt;

&lt;p&gt;If \(n=u\) i.e. if the utterance contains only language independent words then CMI = 0.&lt;/p&gt;

&lt;p&gt;One way calculate CMI for the entire corpus is to just calculate the above at a corpus level rather than an utterance level. However, this method doesn’t take into account the switching frequency. Another way is to combine the utterance level CMI as discussed &lt;a href=&quot;https://www.aclweb.org/anthology/L16-1292.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Multilingual Index (M-index)&lt;/strong&gt;&lt;/a&gt; : Quantifies the ratio of languages in the corpora based on the Gini coefficient to measure the inequality distribution of languages in the corpus.&lt;/p&gt;

\[M-index = \frac{1-\sum p_j^2}{(k-1).\sum p_j^2}\]

&lt;p&gt;where,&lt;br /&gt;
k = number of languages&lt;br /&gt;
\(p_j\) = number of words in language j divided by total number of words&lt;/p&gt;

&lt;p&gt;The M-index = 0 when the corpus is monolingual and = 1 when there is equal distribution of token across all the languages i.e. \(p_j\) for all \(j = 1/k\).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Language Entropy (LE)&lt;/strong&gt;&lt;/a&gt; :  An information theoretic alternative to the M-index. Measures the number of bits required to describe the distribution of language tags.&lt;/p&gt;

\[LE = -\sum_{1...k} p_i*log_2(p_i)\]

&lt;p&gt;where,&lt;br /&gt;
k = number of languages&lt;br /&gt;
\(p_i\) = number of words in language j divided by total number of words&lt;/p&gt;

&lt;p&gt;This metric is 0 for a monolingual corpus and is bounded by \(log_2(k)\) for equally distributed k languages. Both LE and M-index can be derived from one another.&lt;/p&gt;

&lt;p&gt;However, just specifying the frequency of words belonging to another language doesn’t provide enough information. A corpus with a higher frequency of language switches per utterance is more complex, therefore we need some measures on the frequency and distribution of code-switching points.&lt;/p&gt;

&lt;h2 id=&quot;measuring-code-switching&quot;&gt;Measuring code-switching&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Probability of Switching (I-index)&lt;/strong&gt;&lt;/a&gt; : The average number of switch points in the corpus.&lt;/p&gt;

\[f_n = P/(N-1)\]

&lt;p&gt;where,&lt;br /&gt;
\(P\) = number of code-switching points&lt;br /&gt;
\(N-1\) = possible switching points in a corpus that contains n tokens&lt;/p&gt;

&lt;p&gt;A token is considered a switch point, if the preceding token is from another language.&lt;/p&gt;

&lt;h2 id=&quot;time-course-metrics&quot;&gt;Time-course Metrics&lt;/h2&gt;

&lt;p&gt;Since, a corpus is a sequential document, it is informative to have a time-series metrics that quantifies the temporal distribution of code-switching across the corpora.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Burstiness&lt;/strong&gt;&lt;/a&gt; : Measures whether code switching occurs in bursts or is periodic is nature. It compares the code-switching behaviour in the corpus to a poisson behaviour where code-switching occurs at random.&lt;/p&gt;

&lt;p&gt;Let, \(\mu_t\) = mean language span and \(\sigma_t\) = s.t. of language span, then&lt;/p&gt;

\[Burstiness = \frac{\sigma_t-\mu_t}{\sigma_t+\mu_t}\]

&lt;p&gt;Burstiness is bounded by \([-1,1]\). For, corpus that have a periodicity in code-switching points, this value is closer to -1 and for corpuses that have less predictable periodicity have a value closer to 1.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Span Entropy (SE)&lt;/strong&gt;&lt;/a&gt; : An information theoretic measure of the language span distribution i.e. the number of bits needed to describe the probability distribution.&lt;/p&gt;

\[SE = -\sum_{1...M} p_l*log_2(p_l)\]

&lt;p&gt;where,&lt;br /&gt;
\(p_l\) is the sample probability of a language span of length l&lt;br /&gt;
M is the maximum language span&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/a&gt; : The above two metrics don’t make any claims about the time ordering of the language spans i.e. say if a long span occurs below or after the current span, then there would be no difference in the above metric values. Corpus’ with the same burstiness can have very different properties. This metric measures the extent to which the \(i^{th}\) span length is influenced by span lengths occurring before it. This metric is bounded by \([-1,1]\). When it is closer to -1, then the length of consecutive spans in negative correlated i.e. long spans followed by short ones.&lt;/p&gt;

&lt;h1 id=&quot;evaluating-code-mixed-asr-and-tts&quot;&gt;Evaluating Code-Mixed ASR and TTS&lt;/h1&gt;

&lt;h2 id=&quot;asr&quot;&gt;ASR&lt;/h2&gt;

&lt;p&gt;Conventional word error rate is not sufficient for measuring the performance of code-mixed models due to cross-transcription, misspellings and borrowing of words.&lt;/p&gt;

&lt;p&gt;One of the issues for transcription of code-mixed speech is transliteration. For example, say the actual transcripts contain first two words of a sentence in Devanagari and the last 2 in Roman but the transcripts from ASR contain only Roman script. Then unless we have transliteration, we can’t get accurate WER scores. This issue is further compounded by non-uniform transliteration standards, across both the training data and the eval data especially since languages often borrow words from other languages.&lt;/p&gt;

&lt;p&gt;However, there are no standard metrics that exist as of now. Below two metrics that measure different aspects are discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cse.iitb.ac.in/~pjyothi/files/IS19.pdf&quot;&gt;&lt;strong&gt;CM-WER&lt;/strong&gt;&lt;/a&gt; : If there are M words on both sides of switch points across all reference transcriptions and N edits in the ASR hypotheses corresponding to words surrounding the switch points in the references, then \(\text{CM-WER} = \frac{N}{M}\) . This metric provides an estimate of how accurately the system predicts words at switch points.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Brij-Srivastava-2/publication/327388676_Homophone_Identification_and_Merging_for_Code-switched_Speech_Recognition/links/5be04751299bf1124fbbf419/Homophone-Identification-and-Merging-for-Code-switched-Speech-Recognition.pdf&quot;&gt;&lt;strong&gt;poWER&lt;/strong&gt;&lt;/a&gt; : Prononciation Optimised WER. It is defined as the Levenshtein distance between the pronunciation optimized hypothesis (H) and reference (R) sentence, normalised by the number of words in the reference sentence.&lt;/p&gt;

\[\text{poWER} = \text{LevDist}(f(H), f(R))/N\]

&lt;p&gt;Here, \(f\) = grapheme-to-phoneme (g2p) conversion of each word in the sentence&lt;/p&gt;

&lt;p&gt;So, for example, the following below sentences would have a poWER of zero.&lt;/p&gt;

&lt;p&gt;HYP : रूम service आपको कै सी लग&lt;br /&gt;
REF : room service आपको कै सी लग&lt;/p&gt;

&lt;h2 id=&quot;tts&quot;&gt;TTS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Degradation MOS :&lt;/strong&gt; Annotators listen to a sample audio which is considered to have no degradation. The next audio is rated in comparison to that, in terms of degradation, out of 5. A score of 5 means no degradation, a score of 1 means the highest degradation. While testing a code-mixed TTS, it one can use degradation score of monolingual sentences generated from Multi-lingual TTS as compared to Mono-lingual TTS.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The above measures can be used to specify different patterns of code switching which have been termed as “&lt;a href=&quot;http://grlmc.wdfiles.com/local--files/slsp-2013/Day2Session2_03Heike_CS-attitude_dependent_LM.pdf&quot;&gt;code-switching attitudes&lt;/a&gt;”. These can be thought of as aspects of personality and are correlated with regional identities and other trait properties.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have any questions please reach out to &lt;em&gt;swaraj@skit.ai&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/L16-1292.pdf&quot;&gt;Comparing the Level of Code-Switching in Corpora&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.727.9087&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;On measuring the complexity of code-mixing&lt;/a&gt;
&lt;a href=&quot;http://amitavadas.com/Social_India/SOCIAL_INDIA_2014_PROCEEDING.pdf#page=6&quot;&gt;&lt;/a&gt;- &lt;a href=&quot;https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF&quot;&gt;Metrics for modeling code-switching across corpora&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/W16-58.pdf#page=24&quot;&gt;Simple tools for exploring variation in code-switching for linguists&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/physics/0610233.pdf&quot;&gt;Burstiness and memory in complex systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="code-mixing" /><category term="tts" /><category term="asr" /><summary type="html">We at skit, recently concluded a seminar series on code-mixing, where we covered research papers that looked at approaches to deal with code-mixed data across ASR, TTS and SLU related services. Code-mixing is a common phenomena in the Indian subcontinent. Code-mixing refers to the situation when two or more languages are present in an utterance or a corpus. It is important to formulate, discuss and deliberate on metrics because, it is improvement on metrics across benchmark datasets that spur continuous research in a field.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/ornament.jpg" /><media:content medium="image" url="/assets/images/ornament.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Normalizing Flows - Part 2</title><link href="/normalizing-flows-part-2/" rel="alternate" type="text/html" title="Normalizing Flows - Part 2" /><published>2021-05-08T00:00:00+00:00</published><updated>2021-05-08T00:00:00+00:00</updated><id>/normalizing-flows-part-2</id><content type="html" xml:base="/normalizing-flows-part-2/">&lt;p&gt;In &lt;a href=&quot;https://tech.skit.ai/normalizing-flows/&quot;&gt;Part-1&lt;/a&gt;, we introduced the concept of normalizing flows. Here, we discuss the different types of normalizing flows. In most blogs that discuss Normalizing Flows, several concepts related to autoregressive flows and residual flows aren’t discussed very clearly. We hope to simplify the explanations of a relatively theoretical topic and make them accessible.&lt;/p&gt;

&lt;h1 id=&quot;topics-covered&quot;&gt;Topics Covered:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Element-wise Flows&lt;/li&gt;
  &lt;li&gt;Linear Flows&lt;/li&gt;
  &lt;li&gt;Planar and Radial Flows&lt;/li&gt;
  &lt;li&gt;Coupling Flows&lt;/li&gt;
  &lt;li&gt;Autoregressive Flows
    &lt;ul&gt;
      &lt;li&gt;Masked Autoregressive Flows&lt;/li&gt;
      &lt;li&gt;Inverse Autoregressive Flows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Residual Flows&lt;/li&gt;
  &lt;li&gt;Convolutions and NFs&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;element-wise-flows&quot;&gt;Element-wise Flows&lt;/h1&gt;

&lt;p&gt;This is the simplest normalizing flow. In this case, the bijective function \(T\) can be broken into individual components i.e.  \(T(x_1, x_2...x_d) = (h_1(x_1), h_2(x_2)...h_d(x_d))\) s.t. every \(h_i\) is a transform from \(R \rightarrow R\). Each dimension has its own bijective function. However, these simple flows don’t capture any dependency between dimensions. Some activation functions like Parametric Leaky RELU (bijective) can be considered element-wise NFs.&lt;/p&gt;

&lt;h1 id=&quot;linear-flows&quot;&gt;Linear Flows&lt;/h1&gt;

&lt;p&gt;Linear flows are able to capture correlations between dimensions and are of the form : \(T(x) = Ax+b\) , where \(A \in R^{D*D}\) and \(b \in R^D\) where \(D\) is the dimensionality of the input. For the transform T to be invertible, only the matrix A needs to be invertible. When matrix A is a non-diagonal matrix, it captures correlation between different dimensions.&lt;/p&gt;

&lt;p&gt;Linear flows are limited in their expressive power. Starting out with a Gaussian \(z \sim N(\mu, \sigma)\), we simply arrive at another Gaussian \(z' = N(A*\mu+b, A^T*\sigma*A)\), similarly for other distributions.&lt;/p&gt;

&lt;p&gt;However, they serve as important building blocks for NFs. In terms of computational complexity, computing the determinant and inverse are both of order \(O(D^3)\) where \(D\) is the dimensionality of matrix A. To make the calculation more efficient, there need to be some restrictions on A.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Possible restrictions on A:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A is a diagonal matrix: this reduces computation time for the calculation of the det. and inverse to \(O(D)\). However, this is equivalent to element-wise bijections.&lt;/li&gt;
  &lt;li&gt;A is a triangular matrix: It is more expressive than a diagonal matrix. The computation time for calculation of the determinant is \(O(D)\) and for the inverse is \(O(D^2)\).&lt;/li&gt;
  &lt;li&gt;A is an orthogonal matrix: Inverse of an orthogonal matrix is simply its transpose and the determinant is either \(\pm 1\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some other methods that use &lt;a href=&quot;https://papers.nips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html&quot;&gt;LU factorisations&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1901.11137&quot;&gt;QR decomposition&lt;/a&gt; which essentially exploit matrix properties to make the computation of inverse and determinant efficient and at the same time allow A to be more expressive.&lt;/p&gt;

&lt;h1 id=&quot;planar-and-radial-flows&quot;&gt;Planar and Radial Flows&lt;/h1&gt;

&lt;p&gt;They were first introduced in &lt;a href=&quot;https://arxiv.org/abs/1505.05770&quot;&gt;Rezende and Mohamed [2015]&lt;/a&gt;, but aren’t widely used in practice and therefore not covered in details here. Planar flows expand/contract the distribution around a certain direction (specified by a plane) and radial flows modify the distribution around specific points. The effect of the flows can be seen below. The left half shows the effect of planar NFs on an initial distribution of a Gaussian and uniform distribution, and the right half shows the effect of a radial flow.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/planar_radial_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1 : Examples of planar and radial flows on initial distributions like Gaussian and uniform distributions.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;coupling-flows&quot;&gt;Coupling Flows&lt;/h1&gt;

&lt;p&gt;Coupling flows are highly expressive and widely used flow architectures. They can either have linear or non-linear invertible transformations.&lt;/p&gt;

&lt;p&gt;Coupling flows are defined by two sets of mappings, an identity map and a coupling function \(h\). Consider a disjoint partition of the input \(x \in \mathbb{R}\) into two subspaces: \((x^A, x^B)\) \(\in\) \(\mathbb{R}^d \times \mathbb{R}^{D-d}\).  A bijective differentiable coupling function is defined as, \(h(\cdot ;\theta)\) : \(\mathbb{R}^d \rightarrow \mathbb{R}^{D}\) that is parameterized by \(\theta\).  The equations for both the mappings are shown below.&lt;/p&gt;

&lt;p&gt;\(y^A\) =  \(h(x^A, \theta(x^B))\)   &lt;br /&gt;
\(y^{B}\) = \(x^B\)&lt;/p&gt;

&lt;p&gt;The bijection \(h\) is called a &lt;strong&gt;coupling function&lt;/strong&gt;, and the resulting function \(g\) that consists of both the mappings is called a &lt;strong&gt;coupling flow&lt;/strong&gt;. Coupling flow is invertible if and only if \(h\) is invertible and has an inverse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let’s consider an example :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let us consider an input with 4 dimensions \((x1,x2,x3,x4)\). Let \(x^A = (x1,x2)\) and \(x^B = (x3,x4)\). Let \(\theta(x^B) = x3+x4\) and let the coupling function \(h(x^A, \theta(x^B)) = \theta(x^B) * x^A\).&lt;/p&gt;

&lt;p&gt;Given this example, the coupling function \(h\) is invertible but only if \(\theta\) isn’t zero. Therefore, the invertibility requirement of \(h\) introduces a limitation on \(\theta\) as well. In this case, it can’t be defined as a simple additive function. The restriction for non-zero values needs to be taken into account.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/coupling_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2 : Example coupling flow architectures.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The figure above shows two examples of coupling flow architectures. In figure (a), \(x^{A}\) and \(x^{B}\) are the two input subspaces. The coupling function \(h\) is applied to \(x^{A}\) directly while it is parameterized on \(x^{B}\). In figure (b), there are two subsequent flows, where the size of input subspace on which the coupling function \(h\) is applied, gradually increases with each flow.&lt;/p&gt;

&lt;p&gt;There are various types of coupling functions. We discuss some of these briefly,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Additive coupling
    &lt;ul&gt;
      &lt;li&gt;This is one of the simplest form of coupling functions defined by,
  \(h(x;\theta) =\)  \(x + \theta\),     where \(\theta\) is a constant \(\in \mathbb{R}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Affine coupling
    &lt;ul&gt;
      &lt;li&gt;\(h(x;\theta) =\)  \(\theta_1x + \theta_2\),     \(\theta_1 \neq 0\),  \(\theta_2 \in \mathbb{R}\)
  Both additive and affine coupling were introduced in &lt;a href=&quot;https://arxiv.org/pdf/1410.8516.pdf&quot;&gt;NICE&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural auto-regressive flows
    &lt;ul&gt;
      &lt;li&gt;This was first introduced by &lt;a href=&quot;https://arxiv.org/pdf/1804.00779.pdf&quot;&gt;Huang et al [2018]&lt;/a&gt;. The coupling function \(h(\cdot;\theta)\) is modelled as a neural network. Every neural network can be considered as a parameterized function, the parameters being the weights. In this case, the weights of the neural network are defined by \(\theta\). To ascertain that the neural network follows bijectivity, the following proposition is applied :
        &lt;ul&gt;
          &lt;li&gt;If \(NN(\cdot)\) :  \(\mathbb{R} \rightarrow \mathbb{R}\) is a multilayer perceptron, such that all weights are positive and all activation functions are strictly monotone, then \(NN({\cdot})\) is a strictly monotone function.
  So, since the neural network is monotone, it is also invertible and hence a normalizing flow.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More types of coupling flows can be found in this &lt;a href=&quot;https://arxiv.org/pdf/1908.09257.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;autoregressive-flows&quot;&gt;Autoregressive Flows&lt;/h1&gt;

&lt;p&gt;An autoregressive flow is a type of normalizing flow where the transformations use autoregressive functions. The term &lt;strong&gt;autoregressive&lt;/strong&gt; originates from time-series models where the predictions at the current time-step are dependent on the observations from the previous time-steps.&lt;/p&gt;

&lt;p&gt;The probability distribution of an autoregressive model is given by, the equation below where the output at time-step \(i\) is conditioned on all the previous outputs.&lt;/p&gt;

\[p(x) = \prod_{i=1..D} p(x_i | x_{1:i-1})\]

&lt;p&gt;An autoregressive function can be represented as a coupling flow:&lt;/p&gt;

&lt;p&gt;Let the coupling function \(h(\cdot ;\theta)\) : \(\mathbb{R} \rightarrow \mathbb{R}\) be a bijection parameterized by \(\theta\), and let \(x_{1:t}\) be the set of inputs Then, an autoregressive model is a function \(g : \mathbb{R}^D \rightarrow \mathbb{R}^D\), in which every entry of the output \(y = g(x)\) is conditioned on the previous entries of the input:&lt;/p&gt;

\[y_t = h(x_t ; \theta_t(x_{1:t-1})\]

&lt;p&gt;The functions \(\theta_t(\cdot)\) are called &lt;strong&gt;conditioners&lt;/strong&gt;. \(\theta_1\) is a constant whereas \(\theta_2\), \(\theta_3\) … , \(\theta_t\) are arbitrary functions mapping \(\mathbb{R}_{t-1}\) to the set of all parameters. The conditioners need to be arbitrarily complex for effective transformations and hence, are usually modelled as neural networks.&lt;/p&gt;

&lt;p&gt;Since each output depends only on the previous inputs, the Jacobian matrix of an autoregressive transformation \(g\) is triangular (refer to &lt;a href=&quot;https://paper.dropbox.com/doc/Part-1-Introducing-Normalizing-Flows--BHah7N6t5K91Tds2Cz_e8oeWAQ-SMbUFvz9GWqRqcTsYW0Vm&quot;&gt;P&lt;/a&gt;&lt;a href=&quot;https://paper.dropbox.com/doc/Part-1-Introducing-Normalizing-Flows--BHah7N6t5K91Tds2Cz_e8oeWAQ-SMbUFvz9GWqRqcTsYW0Vm&quot;&gt;art 1&lt;/a&gt; for explanation). The determinant of a triangular matrix is simply a product of its diagonal entries.&lt;/p&gt;

\[\det{(Dg)} = \prod_{t=1..k} |\frac{\partial y_t}{\partial x_t}|\]

&lt;h2 id=&quot;masked-autoregressive-flows-mafs&quot;&gt;Masked Autoregressive Flows (MAFs)&lt;/h2&gt;

&lt;p&gt;The time taken to train autoregressive flow models is very high because of the need to sequentially generate outputs. Using MAFs one can parallelize the training process.&lt;/p&gt;

&lt;p&gt;MAFs were inspired by the observation that on stacking several autoregressive models, where each model had unimodal conditionals, the normalizing flow can learn multi-modal conditionals (&lt;a href=&quot;https://arxiv.org/pdf/1705.07057.pdf&quot;&gt;MAF [2018]&lt;/a&gt;). The architecture consists of &lt;strong&gt;stacked MADEs&lt;/strong&gt; &lt;strong&gt;with Gaussian conditionals&lt;/strong&gt; which are explained below.&lt;/p&gt;

&lt;p&gt;What are &lt;strong&gt;MADEs&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MADE&lt;/strong&gt; stands for Masked Auto-Encoder for Distribution Estimation (&lt;a href=&quot;https://arxiv.org/pdf/1502.03509.pdf&quot;&gt;MADE [2015]&lt;/a&gt;) are essentially auto-encoders with some modifications like masks. In a MADE, the autoregressive property is enforced by multiplying the weight matrices of the hidden layers of the auto-encoder with a binary mask. These masks ensure that the training of the sequential auto-regressive model can be done in parallel.&lt;/p&gt;

&lt;p&gt;The masking is done such that forward passes with mask are equivalent to conditioning the output only on the earlier sequences of input. This is shown in the below image. On the left, a typical auto-encoder with 2 hidden layers is shown. If one wants to represent the output as an autoregressive sequence specified by the equation below, then masks are multiplied to the weights of the auto-encoder.&lt;/p&gt;

\[p(x) = p(x_2) * p(x_3 | x_2) * p(x_1 | x_2, x_3)\]

&lt;p&gt;The order of the inputs are \(x_2, x_3, x_1\) The image on the right shows the network weights after masking. For example \(p(x_2)\) doesn’t depend on any of the other inputs since it is the first in the sequence. \(p(x_3)\) is dependent on only the nodes that depend on \(x_2\) and similarly for \(p(x_1)\) The general form for masking is discussed in the MADE paper. The MADE architecture parallelizes the sequential autoregressive computation.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/MADEs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3 : This image is taken from the MADE paper an explains the idea of masking, so as to parallelizes the sequential autoregressive computation.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;density-estimation-and-sampling&quot;&gt;Density Estimation and Sampling&lt;/h2&gt;

&lt;p&gt;There are two important concepts when studying flow architectures : density estimation and sampling. Both these passes have different computation complexity and for auto-regressive models, the computation time of one is inversely related to the other.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/MAFs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4 : Forward pass of MAFs (left) vs. Inverse pass of MAFs (right).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The diagrams above demonstrate a forward pass and an inverse pass in a MAF.
The output \(x_i's\) depends upon the input \(z_{i}\) and the scalars \(\alpha_{i}\), \(\mu_{i}\) which are computed using \(x_{1:t-1}\).  It is these scalars that define the density parameters of the distribution. This is also called a scale and shift transform. The reason why it is designed this way is that inverting \(f(x)\) does not require us to invert the scalar functions \(\alpha_{i}\) and \(\mu_{i}\).&lt;/p&gt;

&lt;p&gt;\(f^{-1}(x_{i}) = z_{i} = \frac{x_{i} - \mu_{i}}{exp(\alpha_{i})}\).&lt;/p&gt;

&lt;p&gt;MAFs can compute the density \(p(x)\) very efficiently. During the training process, all the \(x_i's\) are known. One can therefore parallelize this step using masks. Sampling on the other hand (predicting \(x_{D}\)) is slower as it requires it requires performing \(D\) sequential passes (where \(D\) is the dimensionality of \(x\)) to calculate the previous samples (\(x_{1}, x_{2} ... x_{D-1}\)) before computing \(x_{D}\).&lt;/p&gt;

&lt;h2 id=&quot;inverse-autoregressive-flows-iafs&quot;&gt;Inverse Autoregressive Flows (IAFs)&lt;/h2&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/IAFs.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5 : Inverse pass of MAF (left) vs. Forward pass of IAF (right).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Since MAFs are better at training by exploiting parallel computation but slower during sampling, we modify the functions to get another flow architecture that is faster at sampling. This gives us Inverse Autoregressive flows (IAFs).&lt;/p&gt;

&lt;p&gt;The figures above show a comparison between the inverse pass of a MAF and the forward pass in an IAF.  The only difference between both the architectures is that for IAF’s the autoregression is based on the latent variables and not the predicted distribution. The scale(\(\alpha_{i}\)) and shift(\(\mu_{i}\)) quantities are computed using previous data points from the base distribution instead of the transformed distribution.&lt;/p&gt;

&lt;p&gt;IAF can generate samples efficiently with one pass through the model since all the \(z_i's\)  are known. However, the training process  is slow, since estimating the \(z_i\) requires i sequential passes to calculate all the required input variables \(z_{1}\), \(z_{2}\), .. \(z_{i-1}\).&lt;/p&gt;

&lt;p&gt;Aside : According to our understanding, during training, both the weights of the network parameters(\(\alpha, \mu\)) and \(z_i's\) are estimated i.e. the ith latent variable is itself treated as a parameter that is to be learned.&lt;/p&gt;

&lt;p&gt;One should use IAFs if fast sampling is needed, and MAFs if fast density estimation is desirable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.10433&quot;&gt;Parallel WaveNet&lt;/a&gt; [2017] which was once the state-of-art model for speech synthesis made use of a MAF and an IAF. A fully trained “teacher” network that used MAF architecture was used to train a smaller and parallel “student” network that used IAF architecture. Since the teacher used MAF architecture it could be trained fast. Once the student network, an IAF model, was trained, it could then generate all the audio samples in parallel without depending on the previously generated audio samples and with no loss in audio quality.&lt;/p&gt;

&lt;h1 id=&quot;residual-flows&quot;&gt;Residual Flows&lt;/h1&gt;

&lt;p&gt;During training, deep networks find it difficult to reach a convergence point due to the vanishing/exploding gradient. In such cases, adding more layers to the network only results in a higher training error. Residual Networks or &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;ResNets&lt;/a&gt; were introduced to solve this problem. ResNets consist of skip-connection blocks.&lt;/p&gt;

&lt;p&gt;A residual block is shown below. The output of the residual block is represented by the equation below, where \(F(x)\) represented the pass through the neural layers.&lt;/p&gt;

\[M(x) = F(x) + x\]

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/residual_flows.png&quot; /&gt;
  &lt;figcaption&gt;Fig 6 : A residual block.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;How does this help?
Skip connections in a deep neural network allow the back-propagation signal to reach the initial layers of the neural network thereby solving the vanishing gradient problem.
Instead of treating the number of layers as an important hyper-parameter to tune, by adding skip connections to our network, we are allowing the network to skip training for the layers that are not useful and do not add value to overall accuracy. In a way, skip connections make our neural networks dynamic, so that they may optimally tune the number of layers during training.&lt;/p&gt;

&lt;p&gt;Residual flows in their general form aren’t invertible and can only be used in flow architectures after applying some constraints(first introduced in &lt;a href=&quot;https://arxiv.org/pdf/1906.02735.pdf&quot;&gt;Chen et al. [2019]&lt;/a&gt;). This is discussed next.&lt;/p&gt;

&lt;p&gt;A Lipschitz condition is necessary to enforce invertibility in residual flows.&lt;/p&gt;

&lt;p&gt;Lipchitz condition :
A real valued function : \(f : \mathbb{R} \rightarrow \mathbb{R}\), is Lipchitz continuous if there exists a positive real constant \(K\) such that, for all real \(x_1\) and \(x_2\), \(|f(x_1) - f(x_2)| \leq  K| x_1 - x_2 |\)&lt;/p&gt;

&lt;p&gt;If \(K = 1\) the function is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Short_map&quot;&gt;&lt;strong&gt;short map&lt;/strong&gt;&lt;/a&gt;, and if \(0 \leq K &amp;lt; 1\) the function is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Contraction_mapping&quot;&gt;&lt;strong&gt;contraction&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invertible Residual Flows:&lt;/strong&gt;
Given a residual flow of the form, \(F(x)=x+g(x)\), if the mapping \(g(x)\) has a lipschitz constant \(K\), \(0 \leq K &amp;lt; 1\) then the residual flow is invertible. We discuss a short proof of this next.&lt;/p&gt;

&lt;p&gt;For the residual flow to be invertible, proving that \(F(x)\) is monotonically increasing is sufficient i.e. we need to prove \(F(x+\delta) - F(x) &amp;gt;= 0\)&lt;/p&gt;

\[F(x+\delta) - F(x) = x+\delta + g(x+\delta) - (x - g(x)) = \delta + g(x+\delta) - g(x)\]

&lt;p&gt;From the Lipschitz condition for \(g\), we have \(|g(x+\delta) - g(x)| \le |\delta|\), since \(0 \leq K &amp;lt; 1\).
Therefore, \(F(x+\delta) - F(x)\) is always &amp;gt;= 0 and hence monotonically increasing.&lt;/p&gt;

&lt;p&gt;The addition of invertible residual blocks greatly enhances the class of neural network flow models and also ensures we can train deeper models without the fear of vanishing gradient.&lt;/p&gt;

&lt;h1 id=&quot;convolutions-and-nfs&quot;&gt;Convolutions and NFs&lt;/h1&gt;

&lt;p&gt;Convolutions are one of the most common operations used in the design of DNNs, however, the computation of their inverse and determinant is non-obvious. Since they aren’t invertible they can’t be used in the family of NF flows. However, recently [Hoogeboom et al. &lt;a href=&quot;https://arxiv.org/abs/1901.11137&quot;&gt;2019&lt;/a&gt;] defined invertible \(d*d\) convolutions and designed an architecture using stacked masked autoregressive convolutions. This is explained next.&lt;/p&gt;

&lt;p&gt;In the figure below, on the right, the input x is shown along with the kernel matrix w. The convolution operation between these two can be represented as a matrix multiplication of the form \(A*x\)  shown on the right.  The matrix A is non-invertible here, however, if we consider \(f = g = h = i = 0\), then, we get a triangular matrix that can represent a normalizing flow.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/convolutions_nfs_1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 7 : A convolution represented as a matrix multiplication.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;One of the issues of using a filter where \(f = g = h = i = 0\) is that the receptive field is narrow. However, one can rotate the filter and combine two filters to get different receptive fields as shown on the left in the image below. These can then which can be stacked to capture more expressive combinations as shown on the right. These are called “expressive convolutions” in the paper. The grey and orange filters are rotations of the same filter that result in a triangular matrix. They can be stacked to get different receptive fields shown in blue. These blue receptive fields can be stacked further to arrive at more expressive transforms that have the desired receptive field.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/bigger_kernels.png&quot; /&gt;
  &lt;figcaption&gt;Fig 8 : The part in grey is an example of an individual kernel that would result in a triangular matrix. The grey and orange matrices are all rotations of the standard matrix and the blue matrix shows the receptive field when the grey and orange filters are combined..&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;This concludes our discussion of normalizing flow architectures. Incase of any doubts/questions please reach out to us at &lt;em&gt;kriti@skit.ai&lt;/em&gt; or &lt;em&gt;swaraj@skit.ai&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.09257.pdf&quot;&gt;Normalizing Flows IEEE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.07057.pdf&quot;&gt;MAFs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://akosiorek.github.io/ml/2018/04/03/norm_flows.html&quot;&gt;Adam Kosiorek’s Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&quot;&gt;Lilian Weng’s Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs236.stanford.edu/assets/slides/cs236_lecture8.pdf&quot;&gt;Stanford Lecture&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="normalizing-flows" /><category term="tts" /><summary type="html">In Part-1, we introduced the concept of normalizing flows. Here, we discuss the different types of normalizing flows. In most blogs that discuss Normalizing Flows, several concepts related to autoregressive flows and residual flows aren’t discussed very clearly. We hope to simplify the explanations of a relatively theoretical topic and make them accessible.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/normalising_flow.jpg" /><media:content medium="image" url="/assets/images/normalising_flow.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What’s New in Kaldi-Serve 1.0</title><link href="/whats-new-kaldi-serve-10/" rel="alternate" type="text/html" title="What’s New in Kaldi-Serve 1.0" /><published>2021-03-25T00:00:00+00:00</published><updated>2021-03-25T00:00:00+00:00</updated><id>/whats-new-kaldi-serve-10</id><content type="html" xml:base="/whats-new-kaldi-serve-10/">&lt;p&gt;&lt;a href=&quot;https://github.com/skit-ai/kaldi-serve&quot;&gt;Kaldi-Serve&lt;/a&gt; is our open source high performance Speech Recognition server framework capable of serving &lt;a href=&quot;https://github.com/kaldi-asr/kaldi&quot;&gt;Kaldi ASR&lt;/a&gt; models in production environments for real-time inference, and it’s got an upgrade!&lt;/p&gt;

&lt;h2 id=&quot;whats-changed&quot;&gt;What’s Changed?&lt;/h2&gt;

&lt;p&gt;Originally designed as a standalone application, kaldi-serve had some issues mostly pertaining to it’s usability or extensibility for custom use cases thus greatly reducing the core’s potential to just a handful of rigid situations and dependencies.&lt;/p&gt;

&lt;p&gt;After one too many requests for changes in the architecture to better fit the various production needs of our customers, we realised it’s time for a rewrite of the framework that allows us to better utilise the core’s scope and extend it’s capabilities in the form of general API consumable in most use cases.&lt;/p&gt;

&lt;h3 id=&quot;library&quot;&gt;Library&lt;/h3&gt;

&lt;p&gt;Kaldi-Serve is now a general extensible library with all the functionality necessary to serve Kaldi ASR models in production with any server framework of your choice, for ex. gRPC, Open CGI, HTTP, etc. or even import it in your own custom offline application.&lt;/p&gt;

&lt;p&gt;The earlier standalone gRPC server is now and application that extends the core kaldi-serve library and only contains the frontend gRPC methods that call the general API. We’ll talk about how you can extend the library in your own applications below.&lt;/p&gt;

&lt;h3 id=&quot;python-port&quot;&gt;Python Port&lt;/h3&gt;

&lt;p&gt;We also made it easier to use kaldi-serve by porting the core library to python which can be easily installed as a package via pip. The transcription interface is much simpler and faster to get up and running.&lt;/p&gt;

&lt;p&gt;Below is sample code snippet for transcribing a wav file with your Kaldi Chain model using the kaldiserve python package:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;kaldiserve&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ChainModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse_model_specs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_decoding&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# chain model contains all const components to be shared across multiple threads
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ChainModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_model_specs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model-spec.toml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initialize a decoder that references the chain model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# read audio file as bytes
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sample.wav&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;audio_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getvalue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_decoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# decode the audio
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode_wav_audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;audio_bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# get the n-best alternatives
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;alts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_decoded_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;More Features Coming Soon&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Ontology&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU Inference&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MACE Integration&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Prabhsimran Singh</name></author><category term="Machine Learning" /><category term="speech recognition" /><category term="framework" /><category term="new release" /><summary type="html">Kaldi-Serve is our open source high performance Speech Recognition server framework capable of serving Kaldi ASR models in production environments for real-time inference, and it’s got an upgrade!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our new Tech blog</title><link href="/new-blog/" rel="alternate" type="text/html" title="Our new Tech blog" /><published>2021-02-28T00:00:00+00:00</published><updated>2021-02-28T00:00:00+00:00</updated><id>/new-blog</id><content type="html" xml:base="/new-blog/">&lt;p&gt;We are merging past webpages of our
&lt;a href=&quot;https://skit-ai.github.io/engineering/&quot;&gt;Engineering&lt;/a&gt; and
&lt;a href=&quot;https://skit-ai.github.io/ml/&quot;&gt;ML&lt;/a&gt; team in this new, central, Skit
Tech page. From here on, this is going to be the main source of updates on
speech technology work from &lt;a href=&quot;https://skit.ai&quot;&gt;Skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stay tuned to our &lt;a href=&quot;/feed.xml&quot;&gt;rss feed&lt;/a&gt; for updates.&lt;/p&gt;</content><author><name>Abhinav Tushar</name></author><category term="Engineering" /><category term="Machine Learning" /><category term="sticky" /><summary type="html">We are merging past webpages of our Engineering and ML team in this new, central, Skit Tech page. From here on, this is going to be the main source of updates on speech technology work from Skit.ai.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EMNLP 2020</title><link href="/emnlp/" rel="alternate" type="text/html" title="EMNLP 2020" /><published>2020-12-21T00:00:00+00:00</published><updated>2020-12-21T00:00:00+00:00</updated><id>/emnlp</id><content type="html" xml:base="/emnlp/">&lt;p&gt;Individual summary notes from &lt;a href=&quot;https://2020.emnlp.org/&quot;&gt;EMNLP 2020&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ltbringer.github.io/blog&quot;&gt;Amresh&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I have recently been following the branch of model explainability for practical
purposes. I was delighted to know EMNLP was experiencing a surge of similar
research. I encountered Minumum Description Length (MDL) as a measure to
understand a model’s ability to capture linguistic properties. The core idea
being changing the probing task of identifying labels to transmitting data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/karthik19967829&quot;&gt;Karthik&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Dialogue and Interactive systems track in EMNLP was very interesting and
introduced novel techniques and approaches to some of the ML problems that I am
currently working on for example, MAD-X: a framework for effective transfer
learning to new languages using adapter base architecture , TOD-BERT:
Pre-training Transformer LMs on open-source dialogue data-sets for better
few-short learning capability this could be helpful to mitigate cold-start
problem of SLU module. Also the generative approach based papers for dialogue
state tracking were an interesting approach that might have few-shot learning
capability as compared to discriminative ones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lohith-anandan&quot;&gt;Lohith&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Productising” ML Models has become one of the primary answers for most of the
complex problem we face in day to day life, and explaining how they work, has
become a bigger concern. I was happy to see a lot of papers and demos in that
direction, like the LIT tool, which helps both the developer and the user to
understand the underlying workings of an ML model and how it looks at each input
and how they help in the decision making process. I was glad to see a lot of
papers working with various NLP techniques for better and efficient Health Care
for people, which we can all say is more important than ever, given what we have
seen this year.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/janaab11&quot;&gt;Manas&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Insights from Negative Results in NLP, Workshop was a highlight for me. It
revolved around ideas of what an interesting question is and how to connect
ideas that didn’t work out. These questions feel central to our research efforts
in-team, and the keynotes offered a rewarding and scientific route to exploring
open problems in speech tech.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/swarajdalmia&quot;&gt;Swaraj&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The papers i liked from this year’s EMNLP were : “Incremental Processing in the
Age of Non-Incremental Encoders” by Madureira, B., &amp;amp; Schlangen, D (2020), which
was similar to how humans incrementally process natural language and provided a
way to probe the workings of transformer encoders; and “Digital Voicing of
Silent Speech” by Gaddy, D., &amp;amp; Klein, D. (2020) which was presented very well
and i felt they did a good job modelling the EMG features and got good
improvements on silent speech. There were a lot of papers and workshops on
interpretability of models which is essential considering the direction where
the field is heading and the gather sessions did a good job of ensuring one
doesn’t miss out on the social aspects of attending a conference.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Kumarmanas Nethil</name></author><category term="Machine Learning" /><category term="conference" /><summary type="html">Individual summary notes from EMNLP 2020.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Normalizing Flows - Part 1</title><link href="/normalizing-flows/" rel="alternate" type="text/html" title="Normalizing Flows - Part 1" /><published>2020-12-19T00:00:00+00:00</published><updated>2020-12-19T00:00:00+00:00</updated><id>/normalizing-flows</id><content type="html" xml:base="/normalizing-flows/">&lt;p&gt;Normalizing flows, popularized by &lt;a href=&quot;https://arxiv.org/abs/1505.05770&quot;&gt;(Rezende, &amp;amp; Mohamed,
2015)&lt;/a&gt;, are techniques used in machine
learning to transform simple probability distribution functions into
complicated ones. One of the popular use cases is in generative modelling - an
unsupervised learning method - where the goal is to model a probability
distribution given samples drawn from that distribution.&lt;/p&gt;

&lt;h2 id=&quot;why-bother-about-normalizing-flows&quot;&gt;Why bother about normalizing flows?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;They have been used in many TTS (text-to-speech) models, memorably in the
&lt;a href=&quot;http://proceedings.mlr.press/v80/oord18a/oord18a.pdf&quot;&gt;Parallel WaveNet
model&lt;/a&gt; (2017) where a
clever application of normalizing flows resulted in a 1000 times faster
generation of audio samples in comparison to the original
&lt;a href=&quot;https://arxiv.org/abs/1609.03499&quot;&gt;WaveNet&lt;/a&gt; model. The Parallel WaveNet model
was also deployed on Google assistant for real-time generation of audio.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Normally a back-propagation pass requires the activation value for each
neuron to be stored in memory. This places a restriction on training deeper,
wider models on single GPU’s(since GPU’s have limited memory) and forces one
to use small batch sizes during training. In flow based networks, one does
not need to store the activations at all, as they can be &lt;a href=&quot;https://ameroyer.github.io/reading-notes/architectures/2019/05/07/the_reversible_residual_network.html&quot;&gt;reconstructed
online&lt;/a&gt;
during the back-propagation. This property was leveraged in the &lt;a href=&quot;https://arxiv.org/abs/1707.04585&quot;&gt;RevNets
paper&lt;/a&gt; (2017) which uses invertible
residual blocks. Reducing the memory cost of storing activations
significantly improve the ability to efficiently train wider and deeper
networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.05957.pdf&quot;&gt;Flowtron&lt;/a&gt; (2020), an autoregressive
flow based TTS model does a kind of representation learning using normalizing
flows to learn an invertible mapping from a data space to a latent space
which can be manipulated to control many aspects of speech synthesis (pitch,
tone, speech rate, cadence, accent). Flowtron matches state-of-the-art TTS
models in terms of speech quality and is able to transfer speech
characteristics from a source speaker to a target speaker, making the target
speaker sound more expressive.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’ve ever thought about reversible networks, Normalizing flows do
precisely that. Reversibility of flows also means that one can trivially
encode images into the latent space for editing. They also have cool
mathematical applications, for example their use in &lt;a href=&quot;https://arxiv.org/pdf/1806.07366.pdf&quot;&gt;Neural ODE
solvers&lt;/a&gt; (2019.) which use continuous
normalizing flows.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;brief-introduction&quot;&gt;Brief Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Definition&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;:&lt;/em&gt; A Normalizing Flow is a transformation of a simple
probability distribution into a more complex distribution by a sequence of
invertible and differentiable mappings.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;: The above formalism is a simplification, for a more precise
definition one can consult [5]. The formalism allows piecewise continuous
functions to be used in the construction of the flow which the above
definition restricts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Normalizing&lt;/strong&gt; since the transformed distribution needs to be normalized by
the change of variables formula (discussed below). &lt;strong&gt;Flow&lt;/strong&gt; refers to the
series of invertible transformations which are composed with each other to
create more complex invertible transformations.&lt;/p&gt;

&lt;p&gt;When applied as density estimators, some NFs provide a general way of
constructing &lt;strong&gt;flexible&lt;/strong&gt; probability distributions over continuous random
variables starting from a simple probability distribution. By constraining the
transformations to be invertible, Flow-based models provide a tractable method
to calculate the exact likelihood for a wide variety of generative modeling
problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Efficient inference and efficient synthesis:&lt;/strong&gt; Autoregressive models, such as
the &lt;a href=&quot;https://arxiv.org/pdf/1606.05328.pdf&quot;&gt;PixelCNN&lt;/a&gt;, are also reversible,
however synthesis from such models is difficult to parallelize, and typically
inefficient on parallel hardware. Flow-based generative models like
&lt;a href=&quot;https://arxiv.org/abs/1807.03039&quot;&gt;Glow&lt;/a&gt; (and RealNVP) are efficient to
parallelize for both training and synthesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exact latent-variable inference:&lt;/strong&gt; Within the class of exact likelihood
models, normalizing flows provide two key advantages: model flexibility and
generation speed. Flows have been explored both to increase the flexibility of
the variational posterior in the context of variational autoencoders (VAEs),
and directly as a generative model.  With VAEs, one is able to infer only
approximately the value of the latent variables that correspond to a datapoint.
GAN’s have no encoder at all to infer the latents. In flow based generative
models, this can be done exactly without approximation. Not only does this lead
to accurate inference, it also enables optimization of the exact log-likelihood
of the data, instead of a lower bound of it.&lt;/p&gt;

&lt;h2 id=&quot;mathematical-framework&quot;&gt;Mathematical Framework&lt;/h2&gt;

&lt;p&gt;Let, \(z_0\) be a continuous random variable belonging to a simple probability distribution \(p_\theta(z_0)\) . Let it be a Gaussian with parameters \((\mu, \sigma) = (0,1)\).&lt;/p&gt;

\[z_0 \sim p_\theta (z_0) = N(z_0;0,1)\]

&lt;p&gt;Normalizing flows transforms the simple distribution, into a desired output probability distribution with random variable \(x\), with a sequence of invertible transformations, \(f_i's\).&lt;/p&gt;

\[z_k = f_\theta (z_0) = f_k...f_2.f_1(z_0) \text{ s.t. each $f_i$ is invertible (bijective)}\]

&lt;p&gt;The composition of all the individual flows is represented by \(f_\theta\). Since each \(f_i\) is bijective, so is \(f_\theta\). The new density \(p_\theta (z_k)\) is called a &lt;em&gt;push forward&lt;/em&gt; of the initial density \(p_\theta(z_0)\) by the function \(f_\theta.\)&lt;/p&gt;

&lt;p&gt;An example of a transformation obtained by a normalizing flow is shown below, which transforms a base gaussian distribution into a target multi-modal distribution with the help of a bijective function.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/normalizing_flow.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 1: The transformation of a base distribution into a target distribution using a bijective function f.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The constrains of a distribution being a probability distribution is that \(\int p_\theta (z_0) =1\). However, this doesn’t hold after applying a bijective function (for intuition consider \(f_1 : z \rightarrow z^3\)).&lt;/p&gt;

&lt;h2 id=&quot;change-of-variables-formula&quot;&gt;Change of Variables Formula&lt;/h2&gt;

&lt;p&gt;Consider the normalizing flow \(f_1 : Z_0 \rightarrow Z_1\).  If we want the probability distribution of the random variable \(z_1 \sim Z_1\), we need to consider the &lt;em&gt;change of variables formula&lt;/em&gt; derived below.&lt;/p&gt;

&lt;p&gt;Consider the event \(z_0 \sim Z\), mapped to \(z_1 \sim Z_1\) s.t. \(f_1(z_0) = z_1\). Since, the mapping is bijective, the probabilities of the events are the same. Therefore,&lt;/p&gt;

\[p_\theta(z_1)*\partial z_1 = p_\theta(z_0)*\partial z_0\]

\[p_\theta(z_1) = p_\theta(z_0)* \frac{\partial z_0}{\partial z_1}\]

\[p_\theta(z_1) = p_\theta(z_0)* |\frac{\partial z_0}{\partial z_1}|\]

&lt;p&gt;(since probabilities are always &amp;gt; 0)&lt;/p&gt;

\[p_\theta(z_1) = p_\theta(z_0)* |\frac{\partial z_0}{\partial f_1(z_0)}|\]

&lt;p&gt;(\(z_1 = f_1(z_0)\))&lt;/p&gt;

\[p_\theta(z_1) = p_\theta(z_0)* |\frac{\partial f_1(z_0)}{\partial z_0}|^{-1}\]

&lt;p&gt;In the multivariate case (\(R^D \rightarrow R^D\)) this generalises to:&lt;/p&gt;

\[p_\theta(z_1) = p_\theta(z_0)* |\det(\frac{\partial f_1(z_0)}{\partial z_0})|^{-1}\]

&lt;p&gt;Considering the sequence of compositional transformations \(f_i's\), one obtains:&lt;/p&gt;

\[p_\theta(z_k) = p_\theta(z_0)* \prod_{i=1..k}|\det(\frac{\partial f_i}{\partial z_{i-1}})|^{-1}\]

&lt;p&gt;The term on the right i.e. determinant of the Jacobian accounts for the change
of \(\delta\) volume induced by the transformation. It serves to normalize the
transformed distribution locally, after each flow through a transformation,
hence the name, Normalizing Flows.&lt;/p&gt;

&lt;p&gt;Some Questions at this stage:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The sequence of flows need to be invertible and differentiable. What sort of
constraints does it introduce in terms of the output distributions that can
be reached? Are there families of distributions that we can’t reach starting
from a Gaussian?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling&quot;&gt;Sampling&lt;/h2&gt;

&lt;p&gt;In this case, the bijective functions and the initial distribution are given. Sampling points from the output distribution requires calculating the forward pass, i.e. an efficient calculation of the functions \(f_i's\).&lt;/p&gt;

&lt;h2 id=&quot;density-estimation-using-maximum-likelihood&quot;&gt;Density Estimation using Maximum Likelihood&lt;/h2&gt;

&lt;p&gt;In this case a dataset \(\{a_1, a_2,....,a_n\}\) is provided and the objective is to learn the probability density function \(p_\theta(A)\) to which the points belong. An initial density \(p_\theta(z_0)\) is chosen.&lt;/p&gt;

&lt;p&gt;For each \(a_i\) we have:&lt;/p&gt;

\[p_\theta(a_i) = p_\theta(z_i)* |\det(\frac{\partial f(z_i)}{\partial z_i})|^{-1}\]

&lt;p&gt;where, \(a_i = f(z_i)\)&lt;/p&gt;

\[\prod_{i=1}^n p_\theta(a_i) =\prod_{i=1}^n [p_\theta(z_i)* |\det(\frac{\partial f(z_i)}{\partial z_i})|^{-1}]\]

&lt;p&gt;We need to maximize the above over all possible flows \(f\) to find the flow \(\hat{f}\) that maximizes the probability.&lt;/p&gt;

\[\hat{f} =\text{arg max}_f \text{ }\prod_{i=1}^n [p_\theta(z_i)* |\det(\frac{\partial f(z_i)}{\partial z_i})|^{-1}]\]

&lt;p&gt;Using log likelihood maximization we arrive at:&lt;/p&gt;

\[\hat{f} =\text{arg max}_f \text{ }\sum_{i=1}^n [log(p_\theta(z_i)) - log(|\det(\frac{\partial f(z_i)}{\partial z_i})|)]\]

&lt;p&gt;The equation shown above is used during training for density estimation. However, since we only know \(a_i's\) the only way to find \(z_i's\) which are used, is to find the inverse mapping i.e. \(z_i = f^{-1}(a_i)\) .  So for density estimation and training, the calculation of both the inverse and determinant of the Jacobian are required.&lt;/p&gt;

&lt;p&gt;However, calculating the inverse and the determinant of the Jacobian of a sequence of high dimensional transformations can be very time consuming (for dimensionality \(d\) matrix, both are of complexity \(O(d^3)\)). There are various tricks that are used to reduce the complexity of these two operations, one of the popular ones being the use of triangular maps.&lt;/p&gt;

&lt;h2 id=&quot;triangular-maps&quot;&gt;Triangular maps:&lt;/h2&gt;

&lt;p&gt;Let \(T\) be a normalizing flow, \(T: z \rightarrow x\) where \(x = (x_1, x_2 .... x_d)\) and \(z = (z_1, z_2 .... z_d)\). More generally, one can decompose \(T\) into \(T_1, T_2...T_d\) such that \(x_i = T_i(z_1,z_2.....z_d)\).&lt;/p&gt;

&lt;p&gt;Now if we want to introduce an additional constraint on \(T\) i.e. for \(T\) to be a triangular map, each \(T_j\) should be a function of \((z_1,z_2.....z_j)\) i.e. the first \(j\) elements and not all the \(d\) elements.&lt;/p&gt;

&lt;p&gt;For triangular maps/matrices, both the inverse and the determinant of the jacobian is easy to compute. The jacobian for a triangular map is shown below. The determinant is simply the product of the diagonals and has a complexity of \(O(d)\) instead of \(O(d^3)\). The complexity for the calculation of the inverse is \(O(d^2)\) instead of \(O(d^3)\).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/normalizing-flows/jacobian.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 2: The jacobian for a triangular map. This is taken from here.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note: For an increasing triangular map, \(\frac {\partial T_i}{\partial z_i} &amp;gt; 0\). This will be useful in Part - 2 where the different types/families of Normalizing flows will be considered.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.09257.pdf&quot;&gt;Normalizing Flows: An Introduction and Review of Current Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#vae--flows&quot;&gt;Flow-based Deep Generative Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=3KUvxIOJD0k&amp;amp;ab_channel=PascalPoupart&quot;&gt;Lecture on NFs by Priyank Jaini&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="normalizing-flows" /><category term="tts" /><summary type="html">Normalizing flows, popularized by (Rezende, &amp;amp; Mohamed, 2015), are techniques used in machine learning to transform simple probability distribution functions into complicated ones. One of the popular use cases is in generative modelling - an unsupervised learning method - where the goal is to model a probability distribution given samples drawn from that distribution.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/normalising_flow.jpg" /><media:content medium="image" url="/assets/images/normalising_flow.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Interspeech 2020</title><link href="/interspeech/" rel="alternate" type="text/html" title="Interspeech 2020" /><published>2020-12-01T00:00:00+00:00</published><updated>2020-12-01T00:00:00+00:00</updated><id>/interspeech</id><content type="html" xml:base="/interspeech/">&lt;p&gt;We recently attended the all remote &lt;a href=&quot;http://www.interspeech2020.org/&quot;&gt;Interspeech
2020&lt;/a&gt;. Each of us made notes on what they did
overall. But instead of posting those or going with a
what-we-think-about-the-conference style post, we thought to just ask the team
what interested them the most in a few sentences. Here are the individual
responses:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lepisma.xyz/&quot;&gt;Abhinav&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I really liked the &lt;a href=&quot;https://zerospeech.com/&quot;&gt;ZeroSpeech&lt;/a&gt; challenge. As usual,
they had few really interesting unsupervised problems and solutions. I find
their tracks really ambitious as evident by this statement on their website
“… infants learn to speak their native language, spontaneously, from raw
sensory input, without supervision from text or linguists. It should be
possible to do the same in machines”.&lt;/p&gt;

  &lt;p&gt;Next year, 2021, the &lt;a href=&quot;https://zerospeech.com/2021/news.html&quot;&gt;target is Spoken Language
Modeling&lt;/a&gt;. Looking forward to that too.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://ltbringer.github.io/blog/&quot;&gt;Amresh&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Meta Learning Tutorial on day 1 was a detailed session on the topic. The
promise of performing well on a set of task(s) with less amount of data had my
attention. The authors take care of introduction, utility and comparison of
this approach and its impact on tasks like speaker verification, keyword
spotting, Emotion Recognition and my special interest conversational AI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/janaab11/&quot;&gt;Manas&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A new thing here was Computational Paralinguistics, that covers the
non-content parts of speech. Given my interest in the stylistic parts of
speech, this was particularly interesting. Papers presented many ideas
relevant to building a better voicebot, like - uncertainty aware methods for
multiple labels, Autism Quotient as a perception feature, and predicting CSAT
scores from sentiment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pskrunner14&quot;&gt;Prabhsimran&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Interspeech had some great sessions, from discussions on more fundamental
concepts related to Speech Processing in the Brain, Phonetics and Phonology to
novel ideas on Training Strategies for ASR like Semantic Word Masking,
Efficient Vocoder implementations for faster Neural Waveform Synthesis and
Automatic Prosody Analysis for Non-Semantic Speech Representations. It helped
me connect alot of dots and exposed me to some great ideas we should be
exploring in our work.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/kaustavtamuly/&quot;&gt;Kaustav&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I really liked attending the Speech Emotion Recognition tracks. The tracks
covered a multitude of topics including self-supervised learning methods,
non-semantic representations, etc. It was overall, a very balanced track with
a lot of interaction among the attendees and the presenters. The speech signal
representation track was pretty fun too with some really interesting papers on
voice casting, universal non-semantic representations.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><category term="featured" /><category term="conference" /><summary type="html">We recently attended the all remote Interspeech 2020. Each of us made notes on what they did overall. But instead of posting those or going with a what-we-think-about-the-conference style post, we thought to just ask the team what interested them the most in a few sentences. Here are the individual responses:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reading Sessions</title><link href="/reading-sessions/" rel="alternate" type="text/html" title="Reading Sessions" /><published>2020-11-30T00:00:00+00:00</published><updated>2020-11-30T00:00:00+00:00</updated><id>/reading-sessions</id><content type="html" xml:base="/reading-sessions/">&lt;p&gt;Studying researches and building on top of them is an important part of what a
team of ML Engineers do on a regular basis. Usually, teams do this by organizing
periodic, often weekly, paper reading sessions. Here is a snippet from an
internal work memo by &lt;a href=&quot;https://github.com/janaab11/&quot;&gt;Manas&lt;/a&gt; explaining how we
look at these sessions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lets start with the basic motivation behind these sessions - we want
to read more papers. But beyond this individual goal, there is also
the simpler driving force of enthusiasm - we read something we like,
and we want to share it. It is the same instinct that drives us to
talk about books we read, movies we watch, and podcasts we listen to.
…&lt;/p&gt;

  &lt;p&gt;There are also secondary benefits, like knowledge transfer - both
speaker and audience will understand the topic better after a good
presentation - and discovering shared interests within a larger group,
etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But it’s not that easy to bring this in practice. Specially in a startup, where
processes and structures are constantly in flux. As the team size keeps growing,
different kinds of diversities start interfering. Diversities in interests,
reading styles, and even bandwidth.&lt;/p&gt;

&lt;p&gt;This post covers how we organize reading sessions in the ML team at
&lt;a href=&quot;https://skit.ai/&quot;&gt;Skit.ai&lt;/a&gt;. It might be helpful if you are trying
to do the same in your group.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The very first thing that we did was to start asking people for research
papers that they like, weekly. After voting on one, the proposer of the
paper presented it on a predetermined day. This lost steam away after a
while because of various reasons. One of them being bandwidth crunch for
everyone that time. We were just 3 people.&lt;/p&gt;

&lt;p&gt;We revived paper reading after a while. This time everyone picked and
presented a paper of their own liking. This wasn’t supposed to scale up
with team size, but we went with this for a decent while. While we were
free to choose and read whatever we wanted, lack of continuity in
readings, practical disconnects and difference in interests started to
reduce the overall engagement.&lt;/p&gt;

&lt;p&gt;After lockdown, the engagement level dropped further. On video calls,
you have to upgrade the quality of meetings if you want to maintain the
same level of participation. The missing modalities hurt significantly.
We spent inordinate amount of time trying to get to one single view on
how to go about these sessions. We tried experimenting with various
aspects like paper selection, accountability, presentation
accessibility, etc.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Rather than going in those experiments in chronological order, it makes
sense to think about the problems from two angles based on what we know
now. You can say that the &lt;em&gt;sessions&lt;/em&gt; are having certain issues, or
alternatively you can say that the &lt;em&gt;people&lt;/em&gt; are having issues with the
sessions. While both feed on each other and are cyclic, it helps to look
at them separately.&lt;/p&gt;

&lt;h2 id=&quot;sessions&quot;&gt;Sessions&lt;/h2&gt;

&lt;p&gt;We can break down sessions temporarily in the following three acts.&lt;/p&gt;

&lt;h3 id=&quot;1-pre-session&quot;&gt;1. Pre Session&lt;/h3&gt;

&lt;p&gt;Here it’s known that a certain session is supposed to happen. You can
do the following in preparation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set clear &lt;em&gt;expectations&lt;/em&gt;. What is supposed to be covered? How it’s
supposed to be covered? Who should come? What will be the outcome?
etc.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Excite&lt;/em&gt; the potential audience. If the audience is not really aware
of the topic, some amount of pre-work needs to be done to pull them
in.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-session&quot;&gt;2. Session&lt;/h3&gt;

&lt;p&gt;During the session, you want to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make the presentation &lt;em&gt;stimulating and engaging&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Keep the presentation &lt;em&gt;accessible&lt;/em&gt; while not being superficial.&lt;/li&gt;
  &lt;li&gt;Develop &lt;em&gt;practical connections&lt;/em&gt; between the audience and content.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-post-session&quot;&gt;3. Post Session&lt;/h3&gt;

&lt;p&gt;Since you want the next sessions to be successful too, you would like
to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make sure people are going away with a healthy amount of &lt;em&gt;thought
food&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Nudge towards the &lt;em&gt;utilitarian aspects&lt;/em&gt; of concepts discussed so that
audience have a few threads of experimentation to follow in their day
to day work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;people&quot;&gt;People&lt;/h2&gt;

&lt;p&gt;For people, we can think along the following lines&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Resourcefulness.&lt;/li&gt;
  &lt;li&gt;Motivation.&lt;/li&gt;
  &lt;li&gt;Interests, their depths, and varieties.&lt;/li&gt;
  &lt;li&gt;Style and method of working with new knowledge.&lt;/li&gt;
  &lt;li&gt;Level of comfort with group sessions. Both for presentation and
discussion.&lt;/li&gt;
  &lt;li&gt;Bandwidth. Specially considering industrial settings like ours.&lt;/li&gt;
  &lt;li&gt;Structural assistance and pushes.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Acting on all these factors to deliver a single style of session that
works for everyone is impossible. Not all factors might be important for
a team at a given moment of time, but even a reasonably small set is
sufficiently diverse. The key idea is accept a pluralistic view on the
issue. There is no &lt;em&gt;single&lt;/em&gt; fundamentally correct way of doing these
sessions, and it’s better to pick a digestible subset and solve for that.&lt;/p&gt;

&lt;p&gt;Going ahead with this realization, we started doing &lt;em&gt;seminars&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;seminars&quot;&gt;Seminars&lt;/h2&gt;

&lt;p&gt;Reading Seminars are very similar to seminar courses in Universities.
From another internal memo:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;These [Seminars] exist to complement the world of &lt;em&gt;paper readings&lt;/em&gt;.
While &lt;em&gt;Paper Reading&lt;/em&gt; sessions are about reading more papers and
sharing what we like, &lt;em&gt;Reading Seminars&lt;/em&gt; are about learning something
specific. These are much more structured and pointed towards a goal.
The idea is to have deeper discussions, over longer periods of time,
about topics that might interest you. Either directly or indirectly,
this will lead to a better output (from the speaker) and experience
(for the audience) in the &lt;em&gt;Paper Reading&lt;/em&gt; sessions that follow.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Seminars cover many of the issues nicely and they clearly don’t touch
a few others. For example you can’t just bring in any new paper and
discuss that in a session without setting up a seminar for that field.
And that’s okay. There are other ways of handling that case.&lt;/p&gt;

&lt;p&gt;At the moment, we have the following parallel seminars running:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-Style TTS&lt;/li&gt;
  &lt;li&gt;End-to-End ASR&lt;/li&gt;
  &lt;li&gt;Speech Representation Learning&lt;/li&gt;
  &lt;li&gt;Dialog State Trackers&lt;/li&gt;
  &lt;li&gt;Computational Paralinguistics&lt;/li&gt;
  &lt;li&gt;Learning Theory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of these has a list of papers or topics to be discussed over a period of
1-2 months. While not perfect, these are turning out to be decent reading
roadmaps for these topics. Something we would like to open out after a couple of
months, similar to the old style
&lt;a href=&quot;https://backyard.vernacular.ai/paper-reading/&quot;&gt;paper reading&lt;/a&gt; sessions.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Many are derived from an internal note by
&lt;a href=&quot;https://github.com/janaab11/&quot;&gt;Manas&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><category term="work" /><summary type="html">Studying researches and building on top of them is an important part of what a team of ML Engineers do on a regular basis. Usually, teams do this by organizing periodic, often weekly, paper reading sessions. Here is a snippet from an internal work memo by Manas explaining how we look at these sessions:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bad Audio Detection</title><link href="/bad-audio-detection/" rel="alternate" type="text/html" title="Bad Audio Detection" /><published>2020-07-29T00:00:00+00:00</published><updated>2020-07-29T00:00:00+00:00</updated><id>/bad-audio-detection</id><content type="html" xml:base="/bad-audio-detection/">&lt;p&gt;This blog will be a short one, where we’ll talk about our approach on filtering
out inscrutable audios from &lt;a href=&quot;https://skit.ai/vasr&quot;&gt;&lt;strong&gt;VASR&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are situations in Call Center Automation (CCA) pipeline where user
utterances are bad. &lt;strong&gt;Bad&lt;/strong&gt; here is defined by things like noise, static,
silences or background murmur etc. rendering the downstream SLU systems
helpless. We started with a proposal and prepare a dataset for making an ML
system learn to reject these audios.&lt;/p&gt;

&lt;h2 id=&quot;benefits&quot;&gt;Benefits&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;No more misfires from SLU side which ultimately leads to a better user
experience.&lt;/li&gt;
  &lt;li&gt;Save compute and time by skipping bad audios.&lt;/li&gt;
  &lt;li&gt;The whole system can be used for all our audio based tasks to predict and
filter out the poor ones, hence avoiding sample noise for these tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;We prepared a dataset of intent tagged conversations with specially marked
intents which tell us that these utterances are bad and them going further in
SLU will result in errors. Also we have a sampling of non-bad utterances
(tagged with regular intents) to make this a classification problem.&lt;/p&gt;

&lt;p&gt;There are total 9928 samples of audios labelled as bad and 20000 samples
labeled as good.&lt;/p&gt;

&lt;p&gt;All the raw labels were not very useful, hence we clean and preprocess the data
to finally create 2 broad categories with sub-classes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-bad&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-noisy&lt;/code&gt;: Noisy audio.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-silent&lt;/code&gt;: Silent audio.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-talking&lt;/code&gt;: Background talking.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hold-phone&lt;/code&gt;: Music from keeping on hold.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-good&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;p&gt;We needed to understand the class imbalance and hence we plot a histogram
representing number of samples for each class.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/bad-audio-detection/Class_Distribution.png&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/bad-audio-detection/EDA.png&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;We also plot the frequency vs duration histogram plot to understand the general
distribution of audio durations in the dataset.  Based on the mean duration of
5.26 seconds and the peaks in the histogram, we decided to threshold our audios
to 6.5 seconds.  Anything less than that will be padded to 6.5 seconds and
anything greater than that duration will be truncated to 6.5 seconds.&lt;/p&gt;

&lt;p&gt;Even though we have sub-classes for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio-bad&lt;/code&gt; spanning different areas of
what “bad” could be, we decided to focus only on the &lt;strong&gt;noisy audios&lt;/strong&gt;. Silent
Audios can be treated separately, since they do not actually require something
as complex as an ML model to classify them. We can simply use the age-old
powerful signal processing methods to filter those out with some good
confidence.&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;If we are going to reject these bad audios then we need to do so with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;High Precision&lt;/strong&gt;: We should not be rejecting good audios which are
perfectly interpretable and understandable.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Low Latency:&lt;/strong&gt; This system should have little to no latency, otherwise it
will just slow down our whole VASR flow after being deployed and integrated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Online:&lt;/strong&gt; The model should be capable of running in an online setting where
continuous chunks of audios are fed into the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We used a standard audio classification pipeline to train our binary
classification task.&lt;/p&gt;

&lt;p&gt;This involves generating log-mel spectrograms and then running a Convolution
Neural Network (CNN) based feature extractor on top of this fixed size
spectrogram image.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/bad-audio-detection/model_architecture.png&quot; /&gt;
  &lt;figcaption&gt;Binary Audio Classification based on Log-Mel Spectrograms&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;While these features (spectrograms) can be generated once after processing all
the audios in the dataset, this feature generation needs to be done on the fly
to make a model that can be used in deployment i.e given raw audios as input,
it should be able to predict the class, that was easily incorporated through a
few transforms done within the model using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch-audio&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Even though this architecture is simple, it got us an &lt;strong&gt;accuracy of about
87%&lt;/strong&gt;. But it is not the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accuracy&lt;/code&gt; we need to see, our choice of metric to
measure the performance is &lt;strong&gt;precision&lt;/strong&gt; as explained earlier. We are still in
the process improving these initial baseline numbers of the model. One simple
approach for increasing the precision is to increase the threshold, trading-off
some coverage in the form of support.&lt;/p&gt;

&lt;h2 id=&quot;misclassification-analysis&quot;&gt;Misclassification Analysis&lt;/h2&gt;

&lt;p&gt;We also do a post prediction analysis on the misclassified audios, which
revealed an interesting pattern in the dataset and in the kind of audios that
the model was finding hard to make predictions on.&lt;/p&gt;

&lt;p&gt;Briefly these errors followed 3 major types which now helps to understand the
places where we can make improvements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Type 1 (Very Short Utterance)&lt;/strong&gt; : say 0.2 seconds in audio of 6 seconds.
Due to noise in most part of such short utterance audios, our model predicts
it to be noisy and not good in some occasions. This can probably be fixed
with VAD which can trim the non speech segments in such short utterance
audios.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Type 2 (Long audios)&lt;/strong&gt; : Audio duration is longer than 6.5 seconds with the
speaker in latter half. Since we chose to threshold our features (log-mels)
at 6.5 seconds, the latter part of the audio is basically truncated and hence
such errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Type 3 (Ambiguous / Wrongly Labeled)&lt;/strong&gt; : There are samples in the dataset
which are not perfectly labelled. One may say these audios are debatablem,
some may find them to be bad others may think that they are ok. This type of
label noise is something which needs to be tackled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Needless to say, there are places where we can improve these results, but
having a solid baseline model initially is important for incremental
improvements over time and after a few iterations we finally see these models
in our production systems.&lt;/p&gt;

&lt;p&gt;That’s all for now. Stay tuned to our &lt;a href=&quot;/feed.xml&quot;&gt;rss feed&lt;/a&gt; for updates and more.&lt;/p&gt;</content><author><name>Anirudh Dagar</name></author><category term="Machine Learning" /><category term="classification" /><summary type="html">This blog will be a short one, where we’ll talk about our approach on filtering out inscrutable audios from VASR.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>